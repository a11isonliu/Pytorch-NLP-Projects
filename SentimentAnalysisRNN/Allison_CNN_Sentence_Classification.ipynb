{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allison CNN Sentence Classification\n",
    "## Applied Deep Learning II\n",
    "## Biweekly Report 1 - Jan 26 2022\n",
    "In this notebook, I implement the model presented in the paper titled 'Convolutional Neural Networks for Sentence Classification' (Yoon Kim 2014) and then evaluate it on a movie review dataset from rotten tomatoes found here: https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz. I wanted to understand this model because it has become a baseline for new text classification architectures due to good classification performance across text classification tasks. \n",
    "\n",
    "\n",
    "I follow the tutorial presented on https://opendatascience.com/implementing-a-cnn-for-text-classification-in-tensorflow/ which implements Yoon Kim's original model (done in python 2.7) using tensorflow. TensorFlow was my choice of library, because while I have experimented with PyTorch, I have never touched tensorflow and I want to gain a better baseline understanding of it this semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "1.16.4\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from cs.cornell website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-26 01:49:25--  https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 487770 (476K) [application/x-gzip]\n",
      "Saving to: ‘./data/rt-polaritydata.tar.gz’\n",
      "\n",
      "rt-polaritydata.tar 100%[===================>] 476.34K  1.49MB/s    in 0.3s    \n",
      "\n",
      "2022-01-26 01:49:25 (1.49 MB/s) - ‘./data/rt-polaritydata.tar.gz’ saved [487770/487770]\n",
      "\n",
      "x rt-polaritydata.README.1.0.txt\n",
      "x rt-polaritydata/rt-polarity.neg\n",
      "x rt-polaritydata/rt-polarity.pos\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "!wget -P './data' $url\n",
    "!tar xvzf 'data/rt-polaritydata.tar.gz' -C 'data/'\n",
    "\n",
    "dataset_dir = os.path.join('./data/rt-polaritydata')\n",
    "!rm -r ./data/rt-polaritydata.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt-polarity.neg', 'rt-polarity.pos']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and print some examples. We see that there are 5331 positive examples in this dataset and 5331 negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s '\n",
      " 'going to make a splash even greater than arnold schwarzenegger , jean-claud '\n",
      " 'van damme or steven segal .',\n",
      " 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy '\n",
      " 'is so huge that a column of words cannot adequately describe '\n",
      " \"co-writer/director peter jackson's expanded vision of j . r . r . tolkien's \"\n",
      " 'middle-earth .',\n",
      " 'effective but too-tepid biopic',\n",
      " 'if you sometimes like to go to the movies to have fun , wasabi is a good '\n",
      " 'place to start .',\n",
      " \"emerges as something rare , an issue movie that's so honest and keenly \"\n",
      " \"observed that it doesn't feel like one .\"]\n",
      "pos statements:  5331\n",
      "----------------------------------\n",
      "['simplistic , silly and tedious .',\n",
      " \"it's so laddish and juvenile , only teenage boys could possibly find it \"\n",
      " 'funny .',\n",
      " 'exploitative and largely devoid of the depth or sophistication that would '\n",
      " 'make watching such a graphic treatment of the crimes bearable .',\n",
      " '[garbus] discards the potential for pathological study , exhuming instead , '\n",
      " 'the skewed melodrama of the circumstantial situation .',\n",
      " 'a visually flashy but narratively opaque and emotionally vapid exercise in '\n",
      " 'style and mystification .']\n",
      "neg statements:  5331\n"
     ]
    }
   ],
   "source": [
    "pos_examples = list(open(os.path.join(dataset_dir, 'rt-polarity.pos'), encoding='unicode_escape').readlines())\n",
    "pos_examples = [review.strip(' \\n ') for review in pos_examples]\n",
    "pprint(pos_examples[:5])\n",
    "print(\"pos statements: \", len(pos_examples))\n",
    "print(\"----------------------------------\")\n",
    "neg_examples = list(open(os.path.join(dataset_dir, 'rt-polarity.neg').lower().strip(), encoding = 'unicode_escape').readlines())\n",
    "neg_examples = [review.strip(' \\n ') for review in neg_examples]\n",
    "pprint(neg_examples[:5])\n",
    "print(\"neg statements: \", len(neg_examples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(pos_examples, neg_examples):\n",
    "    x_text = pos_examples+neg_examples\n",
    "    y = np.array([[0, 1]]*len(pos_examples)+[[1, 0]]*len(neg_examples))\n",
    "    return[x_text, y]\n",
    "\n",
    "x_text, y = label_data(pos_examples, neg_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary\n",
    "First, tokenize the words using Tokenizer, which vectorizes a text corpus by turning each text into a sequence of integers (with each integer being the index of a token in a dictionary). Punctuation is removed by default. Then, create a vocabulary dictionary and see that there are 19,498 vocabulary words in the corpus. The longest review has a length of 59 words, so all the word vectors have a length of 59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_doc_len = max([len(x.split(\" \")) for x in x_text])\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(x_text)\n",
    "X = tokenizer.texts_to_sequences(x_text)\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_doc_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Lengths: 59\n",
      "Vocabulary Size: 19498\n",
      "Train/Dev split: 9062/1600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size = 0.15, random_state=8)\n",
    "VOCAB_SIZE = len(tokenizer.word_index)\n",
    "\n",
    "print(f'Vector Lengths: {max_doc_len}')\n",
    "print(f'Vocabulary Size: {VOCAB_SIZE}')\n",
    "print(f\"Train/Dev split: {len(y_train)}/{len(y_dev)}\")\n",
    "\n",
    "# Write out vocabulary dictionary\n",
    "vocabulary = tokenizer.index_word\n",
    "vocabulary.values()\n",
    "with open('./data/vocab.txt', 'w') as f:\n",
    "    f.write(str(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text CNN Model\n",
    "* First layers: embed words into low-dimensional vectors\n",
    "* Next layers: convolve over the embedded word vectors using filter sizes of 3, 4, and 5\n",
    "* Max-pool the result of the convolutional later into a long feature vector\n",
    "* Add dropout regularization\n",
    "* Classify the results using a softmax \n",
    "\n",
    "Note that pre-trained word2vec vectors are not used for word embeddings and are learned from scratch.\n",
    "\n",
    "Put into a TextCNN class to allow for various hyperparameter configurations.\n",
    "\n",
    "Dropout is a method to regularize CNNs. The dropout layer stochastically \"disables\" a fraction of neurons, which forces them to learn individually useful features. \"dropout_keep_prob\" is defined as the fraction of neurons we keep enabled. During evaluation dropout is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.compat.v1.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.compat.v1.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.compat.v1.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random.uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                # W is the embedding matrix learned during training. Initialize as random unif dist\n",
    "                W = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool2d(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.compat.v1.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch iteration function to shuffle training batches. I think there is a tensorflow module for this, \n",
    "# but need to look into it further.\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Iterate over batches of data, call train_step for each batch, occasionally evaluate and checkpoint model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "# Data loading params\n",
    "DEV_SAMPLE_PERCENTAGE = 0.1\n",
    "POSITIVE_DATA_FILE = \"./data/rt-polaritydata/rt-polarity.pos\"\n",
    "NEGATIVE_DATA_FILE = \"./data/rt-polaritydata/rt-polarity.neg\"\n",
    "\n",
    "# Model Hyperparameters\n",
    "EMBEDDING_DIM = 128\n",
    "FILTER_SIZES = \"3, 4, 5\"\n",
    "NUM_FILTERS = 128\n",
    "DROPOUT_KEEP_PROB = 0.5\n",
    "L2_REG_LAMBDA = 0.0\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 200\n",
    "EVALUATE_EVERY = 100\n",
    "CHECKPOINT_EVERY = 100\n",
    "NUM_CHECKPOINTS =- 5\n",
    "ALLOW_SOFT_PLACEMENT = True\n",
    "LOG_DEVICE_PLACEMENT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.compat.v1.ConfigProto(\n",
    "            allow_soft_placement=ALLOW_SOFT_PLACEMENT,\n",
    "            log_device_placement=LOG_DEVICE_PLACEMENT\n",
    "        )\n",
    "        sess = tf.compat.v1.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=VOCAB_SIZE+1,\n",
    "                embedding_size=EMBEDDING_DIM,\n",
    "                filter_sizes=list(map(int, FILTER_SIZES.split(\",\"))),\n",
    "                num_filters=NUM_FILTERS,\n",
    "                l2_reg_lambda=L2_REG_LAMBDA)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.compat.v1.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.compat.v1.summary.histogram(\"{}/grad/hist\".format(v.name.replace(\":\", \"_\")), g)\n",
    "                    sparsity_summary = tf.compat.v1.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(\":\", \"_\")), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.compat.v1.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            print(f\"run: {timestamp}\")\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.compat.v1.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.compat.v1.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.compat.v1.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.compat.v1.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.compat.v1.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.compat.v1.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables(), max_to_keep=NUM_CHECKPOINTS)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: DROPOUT_KEEP_PROB\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train)), BATCH_SIZE, NUM_EPOCHS)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.compat.v1.train.global_step(sess, global_step)\n",
    "                if current_step % EVALUATE_EVERY == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % CHECKPOINT_EVERY == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/33/49tykqg9645_cd5gw3v25dfh0000gn/T/ipykernel_21279/2205772446.py:59: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 01:49:28.827094: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-26 01:49:28.827358: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n",
      "W0126 01:49:28.907726 4454043136 deprecation.py:506] From /var/folders/33/49tykqg9645_cd5gw3v25dfh0000gn/T/ipykernel_21279/2205772446.py:59: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/33/49tykqg9645_cd5gw3v25dfh0000gn/T/ipykernel_21279/2205772446.py:63: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0126 01:49:28.929696 4454043136 deprecation_wrapper.py:119] From /var/folders/33/49tykqg9645_cd5gw3v25dfh0000gn/T/ipykernel_21279/2205772446.py:63: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0126 01:49:30.438992 4454043136 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 1643186971\n",
      "Writing to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/33/49tykqg9645_cd5gw3v25dfh0000gn/T/ipykernel_21279/2874133935.py:65: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0126 01:49:31.527490 4454043136 deprecation_wrapper.py:119] From /var/folders/33/49tykqg9645_cd5gw3v25dfh0000gn/T/ipykernel_21279/2874133935.py:65: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-26T01:49:34.356472: step 1, loss 3.83197, acc 0.46875\n",
      "2022-01-26T01:49:34.587374: step 2, loss 1.82117, acc 0.59375\n",
      "2022-01-26T01:49:34.867896: step 3, loss 2.93784, acc 0.40625\n",
      "2022-01-26T01:49:35.074784: step 4, loss 2.03286, acc 0.4375\n",
      "2022-01-26T01:49:35.279941: step 5, loss 2.02314, acc 0.515625\n",
      "2022-01-26T01:49:35.542995: step 6, loss 3.29338, acc 0.484375\n",
      "2022-01-26T01:49:35.778023: step 7, loss 1.71257, acc 0.546875\n",
      "2022-01-26T01:49:35.972541: step 8, loss 2.43016, acc 0.546875\n",
      "2022-01-26T01:49:36.162962: step 9, loss 2.09332, acc 0.546875\n",
      "2022-01-26T01:49:36.337489: step 10, loss 2.91537, acc 0.484375\n",
      "2022-01-26T01:49:36.520613: step 11, loss 2.37934, acc 0.515625\n",
      "2022-01-26T01:49:36.701093: step 12, loss 2.66288, acc 0.390625\n",
      "2022-01-26T01:49:36.888448: step 13, loss 1.84955, acc 0.53125\n",
      "2022-01-26T01:49:37.090768: step 14, loss 2.22834, acc 0.53125\n",
      "2022-01-26T01:49:37.291097: step 15, loss 1.54301, acc 0.546875\n",
      "2022-01-26T01:49:37.461153: step 16, loss 2.12384, acc 0.4375\n",
      "2022-01-26T01:49:37.667437: step 17, loss 2.16676, acc 0.453125\n",
      "2022-01-26T01:49:37.852147: step 18, loss 2.61093, acc 0.453125\n",
      "2022-01-26T01:49:38.039739: step 19, loss 2.18067, acc 0.5\n",
      "2022-01-26T01:49:38.272891: step 20, loss 1.85728, acc 0.59375\n",
      "2022-01-26T01:49:38.482012: step 21, loss 1.937, acc 0.40625\n",
      "2022-01-26T01:49:38.667623: step 22, loss 1.97855, acc 0.515625\n",
      "2022-01-26T01:49:38.855249: step 23, loss 1.66903, acc 0.515625\n",
      "2022-01-26T01:49:39.030584: step 24, loss 1.6061, acc 0.625\n",
      "2022-01-26T01:49:39.227195: step 25, loss 1.7405, acc 0.5\n",
      "2022-01-26T01:49:39.455139: step 26, loss 2.33805, acc 0.4375\n",
      "2022-01-26T01:49:39.670001: step 27, loss 1.68105, acc 0.515625\n",
      "2022-01-26T01:49:39.873880: step 28, loss 1.79732, acc 0.5\n",
      "2022-01-26T01:49:40.063950: step 29, loss 1.78024, acc 0.484375\n",
      "2022-01-26T01:49:40.247117: step 30, loss 1.60786, acc 0.546875\n",
      "2022-01-26T01:49:40.449566: step 31, loss 1.83737, acc 0.5\n",
      "2022-01-26T01:49:40.644532: step 32, loss 1.81329, acc 0.5\n",
      "2022-01-26T01:49:40.835175: step 33, loss 1.86129, acc 0.5\n",
      "2022-01-26T01:49:41.048922: step 34, loss 1.71417, acc 0.53125\n",
      "2022-01-26T01:49:41.291659: step 35, loss 1.72192, acc 0.578125\n",
      "2022-01-26T01:49:41.490019: step 36, loss 1.56747, acc 0.578125\n",
      "2022-01-26T01:49:41.695319: step 37, loss 1.65292, acc 0.578125\n",
      "2022-01-26T01:49:41.878779: step 38, loss 1.79042, acc 0.5625\n",
      "2022-01-26T01:49:42.109559: step 39, loss 1.95887, acc 0.515625\n",
      "2022-01-26T01:49:42.298197: step 40, loss 2.12129, acc 0.453125\n",
      "2022-01-26T01:49:42.502864: step 41, loss 1.55593, acc 0.515625\n",
      "2022-01-26T01:49:42.694660: step 42, loss 1.90389, acc 0.59375\n",
      "2022-01-26T01:49:42.873605: step 43, loss 1.95487, acc 0.40625\n",
      "2022-01-26T01:49:43.051814: step 44, loss 1.7827, acc 0.46875\n",
      "2022-01-26T01:49:43.227870: step 45, loss 1.94403, acc 0.5\n",
      "2022-01-26T01:49:43.413062: step 46, loss 1.41744, acc 0.53125\n",
      "2022-01-26T01:49:43.602278: step 47, loss 1.21364, acc 0.59375\n",
      "2022-01-26T01:49:43.778964: step 48, loss 1.45167, acc 0.546875\n",
      "2022-01-26T01:49:43.947245: step 49, loss 1.20938, acc 0.625\n",
      "2022-01-26T01:49:44.117410: step 50, loss 1.54701, acc 0.46875\n",
      "2022-01-26T01:49:44.309800: step 51, loss 1.68921, acc 0.453125\n",
      "2022-01-26T01:49:44.494123: step 52, loss 1.69472, acc 0.53125\n",
      "2022-01-26T01:49:44.694415: step 53, loss 2.08965, acc 0.484375\n",
      "2022-01-26T01:49:44.870066: step 54, loss 1.4919, acc 0.53125\n",
      "2022-01-26T01:49:45.062068: step 55, loss 1.57952, acc 0.484375\n",
      "2022-01-26T01:49:45.235402: step 56, loss 2.08217, acc 0.484375\n",
      "2022-01-26T01:49:45.419794: step 57, loss 1.50045, acc 0.515625\n",
      "2022-01-26T01:49:45.631621: step 58, loss 1.58831, acc 0.515625\n",
      "2022-01-26T01:49:45.829354: step 59, loss 1.72052, acc 0.5\n",
      "2022-01-26T01:49:46.024241: step 60, loss 2.09572, acc 0.421875\n",
      "2022-01-26T01:49:46.228554: step 61, loss 1.55595, acc 0.578125\n",
      "2022-01-26T01:49:46.425874: step 62, loss 1.82068, acc 0.5\n",
      "2022-01-26T01:49:46.622726: step 63, loss 1.73432, acc 0.578125\n",
      "2022-01-26T01:49:46.829581: step 64, loss 1.29818, acc 0.5625\n",
      "2022-01-26T01:49:47.033816: step 65, loss 1.42869, acc 0.515625\n",
      "2022-01-26T01:49:47.247577: step 66, loss 1.76883, acc 0.390625\n",
      "2022-01-26T01:49:47.412783: step 67, loss 1.7933, acc 0.53125\n",
      "2022-01-26T01:49:47.578684: step 68, loss 1.81766, acc 0.453125\n",
      "2022-01-26T01:49:47.745254: step 69, loss 1.68328, acc 0.484375\n",
      "2022-01-26T01:49:47.918218: step 70, loss 1.78332, acc 0.515625\n",
      "2022-01-26T01:49:48.081040: step 71, loss 1.68294, acc 0.578125\n",
      "2022-01-26T01:49:48.249732: step 72, loss 1.97674, acc 0.453125\n",
      "2022-01-26T01:49:48.429887: step 73, loss 1.79672, acc 0.546875\n",
      "2022-01-26T01:49:48.609503: step 74, loss 1.81596, acc 0.53125\n",
      "2022-01-26T01:49:48.776274: step 75, loss 1.32432, acc 0.53125\n",
      "2022-01-26T01:49:48.944436: step 76, loss 1.59312, acc 0.578125\n",
      "2022-01-26T01:49:49.104977: step 77, loss 1.92244, acc 0.515625\n",
      "2022-01-26T01:49:49.284385: step 78, loss 1.54834, acc 0.5\n",
      "2022-01-26T01:49:49.448245: step 79, loss 1.01548, acc 0.65625\n",
      "2022-01-26T01:49:49.618205: step 80, loss 1.40797, acc 0.578125\n",
      "2022-01-26T01:49:49.789886: step 81, loss 1.68626, acc 0.5\n",
      "2022-01-26T01:49:49.958309: step 82, loss 1.84933, acc 0.453125\n",
      "2022-01-26T01:49:50.118094: step 83, loss 1.46104, acc 0.5625\n",
      "2022-01-26T01:49:50.286091: step 84, loss 1.35308, acc 0.515625\n",
      "2022-01-26T01:49:50.451172: step 85, loss 1.0395, acc 0.609375\n",
      "2022-01-26T01:49:50.621768: step 86, loss 1.63178, acc 0.578125\n",
      "2022-01-26T01:49:50.786335: step 87, loss 1.68659, acc 0.53125\n",
      "2022-01-26T01:49:50.954521: step 88, loss 1.14956, acc 0.59375\n",
      "2022-01-26T01:49:51.113793: step 89, loss 1.56664, acc 0.53125\n",
      "2022-01-26T01:49:51.280446: step 90, loss 1.20706, acc 0.546875\n",
      "2022-01-26T01:49:51.442205: step 91, loss 1.59834, acc 0.515625\n",
      "2022-01-26T01:49:51.609398: step 92, loss 1.61563, acc 0.515625\n",
      "2022-01-26T01:49:51.770887: step 93, loss 1.96343, acc 0.453125\n",
      "2022-01-26T01:49:51.949615: step 94, loss 1.26561, acc 0.5\n",
      "2022-01-26T01:49:52.111194: step 95, loss 1.10498, acc 0.640625\n",
      "2022-01-26T01:49:52.281185: step 96, loss 1.41701, acc 0.484375\n",
      "2022-01-26T01:49:52.450690: step 97, loss 1.40804, acc 0.515625\n",
      "2022-01-26T01:49:52.617189: step 98, loss 2.01654, acc 0.421875\n",
      "2022-01-26T01:49:52.784523: step 99, loss 1.29424, acc 0.5625\n",
      "2022-01-26T01:49:52.954819: step 100, loss 1.30015, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:49:53.780589: step 100, loss 0.712846, acc 0.5875\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-100 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:49:54.021190 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-100 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/allisonliu/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0126 01:49:54.024179 4454043136 deprecation.py:323] From /Users/allisonliu/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-100\n",
      "\n",
      "2022-01-26T01:49:54.268231: step 101, loss 1.38999, acc 0.4375\n",
      "2022-01-26T01:49:54.474319: step 102, loss 1.24082, acc 0.59375\n",
      "2022-01-26T01:49:54.747160: step 103, loss 1.28752, acc 0.625\n",
      "2022-01-26T01:49:54.992575: step 104, loss 1.61845, acc 0.578125\n",
      "2022-01-26T01:49:55.207179: step 105, loss 1.57953, acc 0.53125\n",
      "2022-01-26T01:49:55.484427: step 106, loss 1.71203, acc 0.4375\n",
      "2022-01-26T01:49:55.709673: step 107, loss 1.41408, acc 0.515625\n",
      "2022-01-26T01:49:55.912467: step 108, loss 1.54607, acc 0.546875\n",
      "2022-01-26T01:49:56.152926: step 109, loss 1.21853, acc 0.515625\n",
      "2022-01-26T01:49:56.388904: step 110, loss 1.16841, acc 0.53125\n",
      "2022-01-26T01:49:56.669829: step 111, loss 1.89506, acc 0.515625\n",
      "2022-01-26T01:49:56.866823: step 112, loss 1.77993, acc 0.453125\n",
      "2022-01-26T01:49:57.087124: step 113, loss 1.45578, acc 0.5\n",
      "2022-01-26T01:49:57.278739: step 114, loss 1.2211, acc 0.546875\n",
      "2022-01-26T01:49:57.471903: step 115, loss 1.56208, acc 0.546875\n",
      "2022-01-26T01:49:57.664621: step 116, loss 1.40856, acc 0.421875\n",
      "2022-01-26T01:49:57.864153: step 117, loss 1.78055, acc 0.484375\n",
      "2022-01-26T01:49:58.061524: step 118, loss 1.584, acc 0.53125\n",
      "2022-01-26T01:49:58.249842: step 119, loss 2.39734, acc 0.28125\n",
      "2022-01-26T01:49:58.420165: step 120, loss 1.33667, acc 0.578125\n",
      "2022-01-26T01:49:58.593271: step 121, loss 1.69684, acc 0.390625\n",
      "2022-01-26T01:49:58.759992: step 122, loss 0.982132, acc 0.609375\n",
      "2022-01-26T01:49:58.933937: step 123, loss 1.31036, acc 0.5\n",
      "2022-01-26T01:49:59.109451: step 124, loss 1.26008, acc 0.578125\n",
      "2022-01-26T01:49:59.296150: step 125, loss 1.56615, acc 0.40625\n",
      "2022-01-26T01:49:59.466056: step 126, loss 1.40452, acc 0.5\n",
      "2022-01-26T01:49:59.662314: step 127, loss 1.10477, acc 0.578125\n",
      "2022-01-26T01:49:59.853680: step 128, loss 2.02145, acc 0.46875\n",
      "2022-01-26T01:50:00.085603: step 129, loss 1.16029, acc 0.59375\n",
      "2022-01-26T01:50:00.294264: step 130, loss 1.87499, acc 0.40625\n",
      "2022-01-26T01:50:00.524670: step 131, loss 1.21232, acc 0.6875\n",
      "2022-01-26T01:50:00.714455: step 132, loss 1.59209, acc 0.453125\n",
      "2022-01-26T01:50:00.901950: step 133, loss 1.58828, acc 0.5\n",
      "2022-01-26T01:50:01.081221: step 134, loss 1.04505, acc 0.609375\n",
      "2022-01-26T01:50:01.259358: step 135, loss 1.30287, acc 0.515625\n",
      "2022-01-26T01:50:01.436962: step 136, loss 1.99371, acc 0.40625\n",
      "2022-01-26T01:50:01.599878: step 137, loss 0.910124, acc 0.640625\n",
      "2022-01-26T01:50:01.761500: step 138, loss 1.4636, acc 0.4375\n",
      "2022-01-26T01:50:01.925858: step 139, loss 1.67292, acc 0.546875\n",
      "2022-01-26T01:50:02.085042: step 140, loss 0.884909, acc 0.671875\n",
      "2022-01-26T01:50:02.252218: step 141, loss 1.25113, acc 0.546875\n",
      "2022-01-26T01:50:02.386846: step 142, loss 1.08894, acc 0.578947\n",
      "2022-01-26T01:50:02.553488: step 143, loss 0.93584, acc 0.578125\n",
      "2022-01-26T01:50:02.740467: step 144, loss 0.847889, acc 0.625\n",
      "2022-01-26T01:50:02.940589: step 145, loss 1.15367, acc 0.546875\n",
      "2022-01-26T01:50:03.130837: step 146, loss 1.07166, acc 0.609375\n",
      "2022-01-26T01:50:03.324613: step 147, loss 0.993799, acc 0.609375\n",
      "2022-01-26T01:50:03.532782: step 148, loss 1.10454, acc 0.609375\n",
      "2022-01-26T01:50:03.723966: step 149, loss 1.28051, acc 0.546875\n",
      "2022-01-26T01:50:03.902282: step 150, loss 1.29353, acc 0.5625\n",
      "2022-01-26T01:50:04.065429: step 151, loss 1.47145, acc 0.5625\n",
      "2022-01-26T01:50:04.231275: step 152, loss 1.26882, acc 0.578125\n",
      "2022-01-26T01:50:04.390357: step 153, loss 0.836834, acc 0.65625\n",
      "2022-01-26T01:50:04.557777: step 154, loss 1.40146, acc 0.515625\n",
      "2022-01-26T01:50:04.716413: step 155, loss 0.974254, acc 0.640625\n",
      "2022-01-26T01:50:04.881898: step 156, loss 1.02632, acc 0.640625\n",
      "2022-01-26T01:50:05.108743: step 157, loss 1.0691, acc 0.609375\n",
      "2022-01-26T01:50:05.288261: step 158, loss 1.02083, acc 0.59375\n",
      "2022-01-26T01:50:05.468437: step 159, loss 1.07771, acc 0.578125\n",
      "2022-01-26T01:50:05.675127: step 160, loss 0.778592, acc 0.640625\n",
      "2022-01-26T01:50:05.878885: step 161, loss 0.699049, acc 0.65625\n",
      "2022-01-26T01:50:06.083726: step 162, loss 1.30282, acc 0.53125\n",
      "2022-01-26T01:50:06.281700: step 163, loss 1.08134, acc 0.625\n",
      "2022-01-26T01:50:06.451593: step 164, loss 1.11164, acc 0.609375\n",
      "2022-01-26T01:50:06.608993: step 165, loss 1.17032, acc 0.625\n",
      "2022-01-26T01:50:06.784128: step 166, loss 0.893954, acc 0.59375\n",
      "2022-01-26T01:50:06.951573: step 167, loss 1.57736, acc 0.453125\n",
      "2022-01-26T01:50:07.124840: step 168, loss 1.18568, acc 0.421875\n",
      "2022-01-26T01:50:07.284105: step 169, loss 0.944794, acc 0.671875\n",
      "2022-01-26T01:50:07.447067: step 170, loss 1.04804, acc 0.609375\n",
      "2022-01-26T01:50:07.607078: step 171, loss 0.958567, acc 0.625\n",
      "2022-01-26T01:50:07.775234: step 172, loss 0.96766, acc 0.625\n",
      "2022-01-26T01:50:07.935478: step 173, loss 0.825555, acc 0.625\n",
      "2022-01-26T01:50:08.110194: step 174, loss 1.05833, acc 0.625\n",
      "2022-01-26T01:50:08.283800: step 175, loss 1.33324, acc 0.5\n",
      "2022-01-26T01:50:08.481447: step 176, loss 1.03725, acc 0.578125\n",
      "2022-01-26T01:50:08.656965: step 177, loss 1.052, acc 0.625\n",
      "2022-01-26T01:50:08.835716: step 178, loss 0.807631, acc 0.609375\n",
      "2022-01-26T01:50:09.009806: step 179, loss 1.10567, acc 0.609375\n",
      "2022-01-26T01:50:09.198096: step 180, loss 1.11152, acc 0.53125\n",
      "2022-01-26T01:50:09.380447: step 181, loss 0.966346, acc 0.609375\n",
      "2022-01-26T01:50:09.557345: step 182, loss 1.15784, acc 0.546875\n",
      "2022-01-26T01:50:09.732148: step 183, loss 0.854775, acc 0.65625\n",
      "2022-01-26T01:50:09.909647: step 184, loss 0.887955, acc 0.640625\n",
      "2022-01-26T01:50:10.206483: step 185, loss 0.971128, acc 0.625\n",
      "2022-01-26T01:50:10.409813: step 186, loss 0.897162, acc 0.625\n",
      "2022-01-26T01:50:10.600191: step 187, loss 0.841623, acc 0.640625\n",
      "2022-01-26T01:50:10.833606: step 188, loss 0.950314, acc 0.609375\n",
      "2022-01-26T01:50:11.030375: step 189, loss 1.44489, acc 0.5\n",
      "2022-01-26T01:50:11.213723: step 190, loss 0.889654, acc 0.59375\n",
      "2022-01-26T01:50:11.379681: step 191, loss 0.854735, acc 0.625\n",
      "2022-01-26T01:50:11.558131: step 192, loss 0.711139, acc 0.625\n",
      "2022-01-26T01:50:11.740353: step 193, loss 1.10382, acc 0.5\n",
      "2022-01-26T01:50:11.913608: step 194, loss 1.06796, acc 0.578125\n",
      "2022-01-26T01:50:12.088640: step 195, loss 0.602274, acc 0.6875\n",
      "2022-01-26T01:50:12.266180: step 196, loss 0.807142, acc 0.609375\n",
      "2022-01-26T01:50:12.445396: step 197, loss 0.871067, acc 0.640625\n",
      "2022-01-26T01:50:12.624815: step 198, loss 0.866779, acc 0.625\n",
      "2022-01-26T01:50:12.791466: step 199, loss 1.11798, acc 0.5625\n",
      "2022-01-26T01:50:12.973643: step 200, loss 1.12082, acc 0.59375\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:50:13.610588: step 200, loss 0.654643, acc 0.62625\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-200 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:50:13.795296 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-200 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-200\n",
      "\n",
      "2022-01-26T01:50:14.039875: step 201, loss 1.11145, acc 0.53125\n",
      "2022-01-26T01:50:14.222097: step 202, loss 0.89921, acc 0.578125\n",
      "2022-01-26T01:50:14.530795: step 203, loss 0.965137, acc 0.578125\n",
      "2022-01-26T01:50:15.001663: step 204, loss 0.842339, acc 0.59375\n",
      "2022-01-26T01:50:15.277497: step 205, loss 1.07818, acc 0.59375\n",
      "2022-01-26T01:50:15.497784: step 206, loss 1.35947, acc 0.546875\n",
      "2022-01-26T01:50:15.685116: step 207, loss 0.86943, acc 0.609375\n",
      "2022-01-26T01:50:15.975428: step 208, loss 0.889237, acc 0.5625\n",
      "2022-01-26T01:50:16.193613: step 209, loss 0.899264, acc 0.65625\n",
      "2022-01-26T01:50:16.412767: step 210, loss 0.836015, acc 0.625\n",
      "2022-01-26T01:50:16.633307: step 211, loss 0.78335, acc 0.65625\n",
      "2022-01-26T01:50:16.851472: step 212, loss 0.831672, acc 0.609375\n",
      "2022-01-26T01:50:17.084198: step 213, loss 0.852024, acc 0.59375\n",
      "2022-01-26T01:50:17.333708: step 214, loss 1.02252, acc 0.609375\n",
      "2022-01-26T01:50:17.634883: step 215, loss 1.0861, acc 0.65625\n",
      "2022-01-26T01:50:17.895058: step 216, loss 0.97051, acc 0.609375\n",
      "2022-01-26T01:50:18.099788: step 217, loss 0.905713, acc 0.578125\n",
      "2022-01-26T01:50:18.307534: step 218, loss 1.18078, acc 0.59375\n",
      "2022-01-26T01:50:18.638176: step 219, loss 0.893559, acc 0.671875\n",
      "2022-01-26T01:50:18.849666: step 220, loss 1.21743, acc 0.578125\n",
      "2022-01-26T01:50:19.081772: step 221, loss 0.859749, acc 0.59375\n",
      "2022-01-26T01:50:19.350522: step 222, loss 0.895806, acc 0.578125\n",
      "2022-01-26T01:50:19.602570: step 223, loss 0.99496, acc 0.453125\n",
      "2022-01-26T01:50:19.942935: step 224, loss 1.03457, acc 0.59375\n",
      "2022-01-26T01:50:20.304155: step 225, loss 1.06871, acc 0.5625\n",
      "2022-01-26T01:50:20.610211: step 226, loss 0.827513, acc 0.65625\n",
      "2022-01-26T01:50:20.909752: step 227, loss 0.666272, acc 0.625\n",
      "2022-01-26T01:50:21.188853: step 228, loss 1.13682, acc 0.609375\n",
      "2022-01-26T01:50:21.403159: step 229, loss 1.1119, acc 0.546875\n",
      "2022-01-26T01:50:21.591071: step 230, loss 0.729357, acc 0.671875\n",
      "2022-01-26T01:50:21.783792: step 231, loss 0.99248, acc 0.59375\n",
      "2022-01-26T01:50:21.975635: step 232, loss 1.1671, acc 0.5625\n",
      "2022-01-26T01:50:22.172924: step 233, loss 0.610721, acc 0.734375\n",
      "2022-01-26T01:50:22.370868: step 234, loss 0.983301, acc 0.578125\n",
      "2022-01-26T01:50:22.566059: step 235, loss 0.943993, acc 0.640625\n",
      "2022-01-26T01:50:22.758791: step 236, loss 0.975453, acc 0.515625\n",
      "2022-01-26T01:50:22.964130: step 237, loss 0.885079, acc 0.625\n",
      "2022-01-26T01:50:23.152202: step 238, loss 0.684488, acc 0.65625\n",
      "2022-01-26T01:50:23.347640: step 239, loss 0.874721, acc 0.609375\n",
      "2022-01-26T01:50:23.536713: step 240, loss 0.987072, acc 0.53125\n",
      "2022-01-26T01:50:23.711557: step 241, loss 1.05311, acc 0.59375\n",
      "2022-01-26T01:50:23.901290: step 242, loss 0.906069, acc 0.609375\n",
      "2022-01-26T01:50:24.082805: step 243, loss 0.933872, acc 0.59375\n",
      "2022-01-26T01:50:24.258895: step 244, loss 0.738066, acc 0.625\n",
      "2022-01-26T01:50:24.598536: step 245, loss 0.758699, acc 0.65625\n",
      "2022-01-26T01:50:24.794886: step 246, loss 0.942039, acc 0.6875\n",
      "2022-01-26T01:50:24.967122: step 247, loss 1.07436, acc 0.484375\n",
      "2022-01-26T01:50:25.165947: step 248, loss 0.868098, acc 0.6875\n",
      "2022-01-26T01:50:25.352241: step 249, loss 0.667144, acc 0.703125\n",
      "2022-01-26T01:50:25.527512: step 250, loss 1.08903, acc 0.546875\n",
      "2022-01-26T01:50:25.722764: step 251, loss 0.860435, acc 0.671875\n",
      "2022-01-26T01:50:25.908354: step 252, loss 0.761474, acc 0.65625\n",
      "2022-01-26T01:50:26.076365: step 253, loss 0.575029, acc 0.71875\n",
      "2022-01-26T01:50:26.245713: step 254, loss 1.02216, acc 0.484375\n",
      "2022-01-26T01:50:26.421468: step 255, loss 0.648569, acc 0.609375\n",
      "2022-01-26T01:50:26.582715: step 256, loss 0.919672, acc 0.625\n",
      "2022-01-26T01:50:26.747443: step 257, loss 0.819774, acc 0.671875\n",
      "2022-01-26T01:50:26.910685: step 258, loss 0.751781, acc 0.625\n",
      "2022-01-26T01:50:27.078658: step 259, loss 1.14754, acc 0.546875\n",
      "2022-01-26T01:50:27.278679: step 260, loss 0.766505, acc 0.6875\n",
      "2022-01-26T01:50:27.465687: step 261, loss 0.620014, acc 0.671875\n",
      "2022-01-26T01:50:27.622694: step 262, loss 0.685916, acc 0.65625\n",
      "2022-01-26T01:50:27.796198: step 263, loss 0.832434, acc 0.640625\n",
      "2022-01-26T01:50:27.993872: step 264, loss 0.923661, acc 0.5625\n",
      "2022-01-26T01:50:28.172264: step 265, loss 0.896974, acc 0.546875\n",
      "2022-01-26T01:50:28.344528: step 266, loss 0.872618, acc 0.671875\n",
      "2022-01-26T01:50:28.518726: step 267, loss 0.769943, acc 0.609375\n",
      "2022-01-26T01:50:28.694926: step 268, loss 0.708326, acc 0.6875\n",
      "2022-01-26T01:50:28.870076: step 269, loss 0.924912, acc 0.578125\n",
      "2022-01-26T01:50:29.043179: step 270, loss 0.899181, acc 0.59375\n",
      "2022-01-26T01:50:29.211153: step 271, loss 0.785913, acc 0.609375\n",
      "2022-01-26T01:50:29.380605: step 272, loss 0.805807, acc 0.625\n",
      "2022-01-26T01:50:29.547269: step 273, loss 0.618421, acc 0.703125\n",
      "2022-01-26T01:50:29.708080: step 274, loss 0.858211, acc 0.625\n",
      "2022-01-26T01:50:29.880184: step 275, loss 0.859275, acc 0.65625\n",
      "2022-01-26T01:50:30.067100: step 276, loss 0.949168, acc 0.65625\n",
      "2022-01-26T01:50:30.227674: step 277, loss 0.800433, acc 0.65625\n",
      "2022-01-26T01:50:30.387466: step 278, loss 1.06459, acc 0.609375\n",
      "2022-01-26T01:50:30.551136: step 279, loss 0.812401, acc 0.640625\n",
      "2022-01-26T01:50:30.723005: step 280, loss 0.890915, acc 0.546875\n",
      "2022-01-26T01:50:30.898015: step 281, loss 0.943979, acc 0.671875\n",
      "2022-01-26T01:50:31.065512: step 282, loss 0.809301, acc 0.578125\n",
      "2022-01-26T01:50:31.232878: step 283, loss 0.78924, acc 0.609375\n",
      "2022-01-26T01:50:31.372800: step 284, loss 0.88015, acc 0.552632\n",
      "2022-01-26T01:50:31.542960: step 285, loss 0.558507, acc 0.734375\n",
      "2022-01-26T01:50:31.810546: step 286, loss 0.751738, acc 0.65625\n",
      "2022-01-26T01:50:31.997592: step 287, loss 0.554363, acc 0.75\n",
      "2022-01-26T01:50:32.169280: step 288, loss 0.521193, acc 0.71875\n",
      "2022-01-26T01:50:32.344568: step 289, loss 0.776704, acc 0.625\n",
      "2022-01-26T01:50:32.530000: step 290, loss 0.687408, acc 0.671875\n",
      "2022-01-26T01:50:32.695170: step 291, loss 0.758152, acc 0.625\n",
      "2022-01-26T01:50:32.872593: step 292, loss 0.759902, acc 0.671875\n",
      "2022-01-26T01:50:33.050425: step 293, loss 0.823518, acc 0.609375\n",
      "2022-01-26T01:50:33.222205: step 294, loss 0.574647, acc 0.6875\n",
      "2022-01-26T01:50:33.384810: step 295, loss 0.883446, acc 0.609375\n",
      "2022-01-26T01:50:33.563584: step 296, loss 0.650409, acc 0.671875\n",
      "2022-01-26T01:50:33.725749: step 297, loss 0.846306, acc 0.640625\n",
      "2022-01-26T01:50:33.884700: step 298, loss 0.754229, acc 0.6875\n",
      "2022-01-26T01:50:34.057348: step 299, loss 0.991684, acc 0.609375\n",
      "2022-01-26T01:50:34.217792: step 300, loss 0.782407, acc 0.5625\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:50:34.823928: step 300, loss 0.627742, acc 0.6425\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-300 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:50:34.985050 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-300 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-300\n",
      "\n",
      "2022-01-26T01:50:35.233048: step 301, loss 0.569527, acc 0.8125\n",
      "2022-01-26T01:50:35.406678: step 302, loss 0.714234, acc 0.671875\n",
      "2022-01-26T01:50:35.633002: step 303, loss 0.845285, acc 0.5625\n",
      "2022-01-26T01:50:36.061324: step 304, loss 0.739751, acc 0.609375\n",
      "2022-01-26T01:50:36.278444: step 305, loss 0.880945, acc 0.46875\n",
      "2022-01-26T01:50:36.527435: step 306, loss 0.455313, acc 0.75\n",
      "2022-01-26T01:50:36.755246: step 307, loss 1.06426, acc 0.53125\n",
      "2022-01-26T01:50:36.939626: step 308, loss 0.685129, acc 0.640625\n",
      "2022-01-26T01:50:37.137302: step 309, loss 0.564714, acc 0.71875\n",
      "2022-01-26T01:50:37.338627: step 310, loss 0.736484, acc 0.734375\n",
      "2022-01-26T01:50:37.533453: step 311, loss 0.671499, acc 0.609375\n",
      "2022-01-26T01:50:37.717100: step 312, loss 0.726462, acc 0.6875\n",
      "2022-01-26T01:50:37.916957: step 313, loss 0.800194, acc 0.578125\n",
      "2022-01-26T01:50:38.104056: step 314, loss 0.755633, acc 0.609375\n",
      "2022-01-26T01:50:38.309120: step 315, loss 0.771545, acc 0.609375\n",
      "2022-01-26T01:50:38.573360: step 316, loss 0.717328, acc 0.640625\n",
      "2022-01-26T01:50:38.820872: step 317, loss 0.596436, acc 0.65625\n",
      "2022-01-26T01:50:39.078744: step 318, loss 0.542341, acc 0.71875\n",
      "2022-01-26T01:50:39.299158: step 319, loss 0.605423, acc 0.640625\n",
      "2022-01-26T01:50:39.504911: step 320, loss 0.673844, acc 0.671875\n",
      "2022-01-26T01:50:39.693123: step 321, loss 0.882138, acc 0.59375\n",
      "2022-01-26T01:50:39.924960: step 322, loss 0.566792, acc 0.71875\n",
      "2022-01-26T01:50:40.128134: step 323, loss 0.573839, acc 0.734375\n",
      "2022-01-26T01:50:40.326212: step 324, loss 0.829578, acc 0.59375\n",
      "2022-01-26T01:50:40.505265: step 325, loss 0.893042, acc 0.578125\n",
      "2022-01-26T01:50:40.688643: step 326, loss 0.741563, acc 0.703125\n",
      "2022-01-26T01:50:40.880585: step 327, loss 0.818398, acc 0.609375\n",
      "2022-01-26T01:50:41.064669: step 328, loss 0.586883, acc 0.71875\n",
      "2022-01-26T01:50:41.246892: step 329, loss 0.630921, acc 0.6875\n",
      "2022-01-26T01:50:41.430480: step 330, loss 0.835315, acc 0.5625\n",
      "2022-01-26T01:50:41.613875: step 331, loss 0.691179, acc 0.6875\n",
      "2022-01-26T01:50:41.830733: step 332, loss 0.834928, acc 0.578125\n",
      "2022-01-26T01:50:42.025690: step 333, loss 0.775493, acc 0.65625\n",
      "2022-01-26T01:50:42.219240: step 334, loss 0.646176, acc 0.734375\n",
      "2022-01-26T01:50:42.429824: step 335, loss 0.632209, acc 0.71875\n",
      "2022-01-26T01:50:42.626803: step 336, loss 0.749141, acc 0.625\n",
      "2022-01-26T01:50:42.987808: step 337, loss 0.686306, acc 0.671875\n",
      "2022-01-26T01:50:43.164214: step 338, loss 0.658058, acc 0.640625\n",
      "2022-01-26T01:50:43.331424: step 339, loss 0.995362, acc 0.546875\n",
      "2022-01-26T01:50:43.498676: step 340, loss 0.716823, acc 0.65625\n",
      "2022-01-26T01:50:43.695359: step 341, loss 0.745898, acc 0.578125\n",
      "2022-01-26T01:50:43.864857: step 342, loss 0.686327, acc 0.65625\n",
      "2022-01-26T01:50:44.030683: step 343, loss 0.683134, acc 0.625\n",
      "2022-01-26T01:50:44.197925: step 344, loss 0.666842, acc 0.640625\n",
      "2022-01-26T01:50:44.379910: step 345, loss 0.674637, acc 0.65625\n",
      "2022-01-26T01:50:44.573103: step 346, loss 0.68186, acc 0.6875\n",
      "2022-01-26T01:50:44.745143: step 347, loss 0.728461, acc 0.609375\n",
      "2022-01-26T01:50:44.907939: step 348, loss 0.711148, acc 0.6875\n",
      "2022-01-26T01:50:45.096946: step 349, loss 0.713013, acc 0.59375\n",
      "2022-01-26T01:50:45.277522: step 350, loss 0.743235, acc 0.703125\n",
      "2022-01-26T01:50:45.525139: step 351, loss 0.813613, acc 0.578125\n",
      "2022-01-26T01:50:45.697615: step 352, loss 0.553787, acc 0.703125\n",
      "2022-01-26T01:50:45.878544: step 353, loss 0.639812, acc 0.703125\n",
      "2022-01-26T01:50:46.042259: step 354, loss 0.64329, acc 0.703125\n",
      "2022-01-26T01:50:46.224687: step 355, loss 0.58338, acc 0.6875\n",
      "2022-01-26T01:50:46.439658: step 356, loss 0.658365, acc 0.6875\n",
      "2022-01-26T01:50:46.611682: step 357, loss 0.737184, acc 0.625\n",
      "2022-01-26T01:50:46.783700: step 358, loss 0.518825, acc 0.71875\n",
      "2022-01-26T01:50:46.957714: step 359, loss 0.610794, acc 0.671875\n",
      "2022-01-26T01:50:47.125583: step 360, loss 0.508026, acc 0.6875\n",
      "2022-01-26T01:50:47.297206: step 361, loss 0.538504, acc 0.765625\n",
      "2022-01-26T01:50:47.515219: step 362, loss 0.744076, acc 0.59375\n",
      "2022-01-26T01:50:47.714514: step 363, loss 0.709326, acc 0.59375\n",
      "2022-01-26T01:50:47.887300: step 364, loss 0.614587, acc 0.671875\n",
      "2022-01-26T01:50:48.047938: step 365, loss 0.649289, acc 0.65625\n",
      "2022-01-26T01:50:48.210547: step 366, loss 0.718228, acc 0.640625\n",
      "2022-01-26T01:50:48.369581: step 367, loss 0.638805, acc 0.734375\n",
      "2022-01-26T01:50:48.555354: step 368, loss 0.889227, acc 0.546875\n",
      "2022-01-26T01:50:48.710785: step 369, loss 0.689736, acc 0.640625\n",
      "2022-01-26T01:50:48.897640: step 370, loss 0.554456, acc 0.734375\n",
      "2022-01-26T01:50:49.055254: step 371, loss 0.892073, acc 0.578125\n",
      "2022-01-26T01:50:49.236150: step 372, loss 0.650263, acc 0.671875\n",
      "2022-01-26T01:50:49.406077: step 373, loss 0.737847, acc 0.625\n",
      "2022-01-26T01:50:49.605334: step 374, loss 0.552399, acc 0.6875\n",
      "2022-01-26T01:50:49.777433: step 375, loss 0.818266, acc 0.59375\n",
      "2022-01-26T01:50:49.947928: step 376, loss 0.572591, acc 0.703125\n",
      "2022-01-26T01:50:50.110922: step 377, loss 0.745486, acc 0.671875\n",
      "2022-01-26T01:50:50.274515: step 378, loss 0.685677, acc 0.546875\n",
      "2022-01-26T01:50:50.451948: step 379, loss 0.785312, acc 0.625\n",
      "2022-01-26T01:50:50.627851: step 380, loss 0.80043, acc 0.578125\n",
      "2022-01-26T01:50:50.796728: step 381, loss 0.68075, acc 0.671875\n",
      "2022-01-26T01:50:50.957293: step 382, loss 0.783333, acc 0.640625\n",
      "2022-01-26T01:50:51.114403: step 383, loss 0.542967, acc 0.734375\n",
      "2022-01-26T01:50:51.273765: step 384, loss 0.614466, acc 0.703125\n",
      "2022-01-26T01:50:51.431880: step 385, loss 0.671265, acc 0.5625\n",
      "2022-01-26T01:50:51.592274: step 386, loss 0.586585, acc 0.765625\n",
      "2022-01-26T01:50:51.750089: step 387, loss 0.619895, acc 0.6875\n",
      "2022-01-26T01:50:51.908336: step 388, loss 0.701596, acc 0.640625\n",
      "2022-01-26T01:50:52.064838: step 389, loss 0.69121, acc 0.625\n",
      "2022-01-26T01:50:52.242390: step 390, loss 0.724639, acc 0.640625\n",
      "2022-01-26T01:50:52.416511: step 391, loss 0.538679, acc 0.71875\n",
      "2022-01-26T01:50:52.595887: step 392, loss 0.653787, acc 0.65625\n",
      "2022-01-26T01:50:52.757344: step 393, loss 0.702578, acc 0.6875\n",
      "2022-01-26T01:50:52.936514: step 394, loss 0.745695, acc 0.578125\n",
      "2022-01-26T01:50:53.105507: step 395, loss 0.76605, acc 0.5625\n",
      "2022-01-26T01:50:53.294142: step 396, loss 0.855549, acc 0.546875\n",
      "2022-01-26T01:50:53.468123: step 397, loss 0.859508, acc 0.5625\n",
      "2022-01-26T01:50:53.627632: step 398, loss 0.655666, acc 0.65625\n",
      "2022-01-26T01:50:53.787054: step 399, loss 0.576529, acc 0.734375\n",
      "2022-01-26T01:50:53.948863: step 400, loss 0.639742, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:50:54.552936: step 400, loss 0.611434, acc 0.661875\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-400 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:50:54.699397 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-400 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-400\n",
      "\n",
      "2022-01-26T01:50:54.927224: step 401, loss 0.697692, acc 0.65625\n",
      "2022-01-26T01:50:55.090500: step 402, loss 0.773479, acc 0.5625\n",
      "2022-01-26T01:50:55.342316: step 403, loss 0.785909, acc 0.625\n",
      "2022-01-26T01:50:55.621992: step 404, loss 0.546194, acc 0.6875\n",
      "2022-01-26T01:50:55.942268: step 405, loss 0.657434, acc 0.609375\n",
      "2022-01-26T01:50:56.189424: step 406, loss 0.574869, acc 0.703125\n",
      "2022-01-26T01:50:56.366985: step 407, loss 0.634658, acc 0.703125\n",
      "2022-01-26T01:50:56.552190: step 408, loss 0.632441, acc 0.6875\n",
      "2022-01-26T01:50:56.747537: step 409, loss 0.638901, acc 0.640625\n",
      "2022-01-26T01:50:56.947196: step 410, loss 0.60515, acc 0.734375\n",
      "2022-01-26T01:50:57.195797: step 411, loss 0.588059, acc 0.671875\n",
      "2022-01-26T01:50:57.371157: step 412, loss 0.656758, acc 0.625\n",
      "2022-01-26T01:50:57.568153: step 413, loss 0.705933, acc 0.671875\n",
      "2022-01-26T01:50:57.746782: step 414, loss 0.628065, acc 0.6875\n",
      "2022-01-26T01:50:57.920034: step 415, loss 0.760503, acc 0.640625\n",
      "2022-01-26T01:50:58.107593: step 416, loss 0.654261, acc 0.671875\n",
      "2022-01-26T01:50:58.311352: step 417, loss 0.571245, acc 0.75\n",
      "2022-01-26T01:50:58.502483: step 418, loss 0.71465, acc 0.671875\n",
      "2022-01-26T01:50:58.677387: step 419, loss 0.700882, acc 0.65625\n",
      "2022-01-26T01:50:58.857546: step 420, loss 0.565054, acc 0.765625\n",
      "2022-01-26T01:50:59.025458: step 421, loss 0.645571, acc 0.65625\n",
      "2022-01-26T01:50:59.210366: step 422, loss 0.548038, acc 0.765625\n",
      "2022-01-26T01:50:59.392819: step 423, loss 0.622128, acc 0.671875\n",
      "2022-01-26T01:50:59.565184: step 424, loss 0.63632, acc 0.625\n",
      "2022-01-26T01:50:59.778963: step 425, loss 0.746235, acc 0.59375\n",
      "2022-01-26T01:50:59.957081: step 426, loss 0.460135, acc 0.789474\n",
      "2022-01-26T01:51:00.185182: step 427, loss 0.621208, acc 0.671875\n",
      "2022-01-26T01:51:00.378898: step 428, loss 0.582473, acc 0.75\n",
      "2022-01-26T01:51:00.575674: step 429, loss 0.73578, acc 0.6875\n",
      "2022-01-26T01:51:00.764598: step 430, loss 0.450185, acc 0.78125\n",
      "2022-01-26T01:51:01.027614: step 431, loss 0.579802, acc 0.75\n",
      "2022-01-26T01:51:01.226118: step 432, loss 0.500798, acc 0.765625\n",
      "2022-01-26T01:51:01.432310: step 433, loss 0.513718, acc 0.78125\n",
      "2022-01-26T01:51:01.602798: step 434, loss 0.51694, acc 0.734375\n",
      "2022-01-26T01:51:01.780918: step 435, loss 0.548455, acc 0.71875\n",
      "2022-01-26T01:51:01.982595: step 436, loss 0.496122, acc 0.734375\n",
      "2022-01-26T01:51:02.158837: step 437, loss 0.64979, acc 0.6875\n",
      "2022-01-26T01:51:02.343011: step 438, loss 0.621571, acc 0.703125\n",
      "2022-01-26T01:51:02.541981: step 439, loss 0.491233, acc 0.796875\n",
      "2022-01-26T01:51:02.728292: step 440, loss 0.586825, acc 0.6875\n",
      "2022-01-26T01:51:02.897949: step 441, loss 0.452756, acc 0.828125\n",
      "2022-01-26T01:51:03.084063: step 442, loss 0.49733, acc 0.734375\n",
      "2022-01-26T01:51:03.267328: step 443, loss 0.599876, acc 0.65625\n",
      "2022-01-26T01:51:03.427381: step 444, loss 0.506357, acc 0.765625\n",
      "2022-01-26T01:51:03.599348: step 445, loss 0.628443, acc 0.6875\n",
      "2022-01-26T01:51:03.757130: step 446, loss 0.4517, acc 0.78125\n",
      "2022-01-26T01:51:03.923849: step 447, loss 0.546629, acc 0.75\n",
      "2022-01-26T01:51:04.081753: step 448, loss 0.474881, acc 0.828125\n",
      "2022-01-26T01:51:04.256002: step 449, loss 0.577836, acc 0.6875\n",
      "2022-01-26T01:51:04.411933: step 450, loss 0.651246, acc 0.65625\n",
      "2022-01-26T01:51:04.572251: step 451, loss 0.585129, acc 0.703125\n",
      "2022-01-26T01:51:04.730693: step 452, loss 0.529693, acc 0.796875\n",
      "2022-01-26T01:51:04.891657: step 453, loss 0.584001, acc 0.75\n",
      "2022-01-26T01:51:05.050717: step 454, loss 0.581182, acc 0.734375\n",
      "2022-01-26T01:51:05.212005: step 455, loss 0.574726, acc 0.703125\n",
      "2022-01-26T01:51:05.381382: step 456, loss 0.660711, acc 0.65625\n",
      "2022-01-26T01:51:05.547131: step 457, loss 0.547465, acc 0.75\n",
      "2022-01-26T01:51:05.708483: step 458, loss 0.421446, acc 0.828125\n",
      "2022-01-26T01:51:05.869434: step 459, loss 0.614544, acc 0.671875\n",
      "2022-01-26T01:51:06.026794: step 460, loss 0.513022, acc 0.765625\n",
      "2022-01-26T01:51:06.191626: step 461, loss 0.495688, acc 0.703125\n",
      "2022-01-26T01:51:06.347049: step 462, loss 0.50438, acc 0.78125\n",
      "2022-01-26T01:51:06.510880: step 463, loss 0.660167, acc 0.65625\n",
      "2022-01-26T01:51:06.664847: step 464, loss 0.731927, acc 0.65625\n",
      "2022-01-26T01:51:06.827022: step 465, loss 0.467087, acc 0.765625\n",
      "2022-01-26T01:51:06.982566: step 466, loss 0.549935, acc 0.71875\n",
      "2022-01-26T01:51:07.145893: step 467, loss 0.638445, acc 0.6875\n",
      "2022-01-26T01:51:07.303931: step 468, loss 0.509425, acc 0.765625\n",
      "2022-01-26T01:51:07.467280: step 469, loss 0.576261, acc 0.734375\n",
      "2022-01-26T01:51:07.629717: step 470, loss 0.522881, acc 0.75\n",
      "2022-01-26T01:51:07.794501: step 471, loss 0.443711, acc 0.78125\n",
      "2022-01-26T01:51:07.951726: step 472, loss 0.556344, acc 0.734375\n",
      "2022-01-26T01:51:08.112218: step 473, loss 0.537452, acc 0.71875\n",
      "2022-01-26T01:51:08.273788: step 474, loss 0.408537, acc 0.84375\n",
      "2022-01-26T01:51:08.436226: step 475, loss 0.561963, acc 0.78125\n",
      "2022-01-26T01:51:08.593927: step 476, loss 0.629084, acc 0.671875\n",
      "2022-01-26T01:51:08.758271: step 477, loss 0.480622, acc 0.765625\n",
      "2022-01-26T01:51:08.917126: step 478, loss 0.572332, acc 0.75\n",
      "2022-01-26T01:51:09.077584: step 479, loss 0.415593, acc 0.765625\n",
      "2022-01-26T01:51:09.234376: step 480, loss 0.577411, acc 0.75\n",
      "2022-01-26T01:51:09.403033: step 481, loss 0.587128, acc 0.703125\n",
      "2022-01-26T01:51:09.559390: step 482, loss 0.625561, acc 0.671875\n",
      "2022-01-26T01:51:09.721449: step 483, loss 0.521504, acc 0.703125\n",
      "2022-01-26T01:51:09.879787: step 484, loss 0.46515, acc 0.78125\n",
      "2022-01-26T01:51:10.043697: step 485, loss 0.545514, acc 0.765625\n",
      "2022-01-26T01:51:10.201635: step 486, loss 0.614199, acc 0.671875\n",
      "2022-01-26T01:51:10.365550: step 487, loss 0.610257, acc 0.6875\n",
      "2022-01-26T01:51:10.534441: step 488, loss 0.463004, acc 0.78125\n",
      "2022-01-26T01:51:10.702174: step 489, loss 0.496311, acc 0.8125\n",
      "2022-01-26T01:51:10.874145: step 490, loss 0.496117, acc 0.75\n",
      "2022-01-26T01:51:11.055798: step 491, loss 0.560913, acc 0.6875\n",
      "2022-01-26T01:51:11.219980: step 492, loss 0.569882, acc 0.703125\n",
      "2022-01-26T01:51:11.397617: step 493, loss 0.596278, acc 0.703125\n",
      "2022-01-26T01:51:11.571626: step 494, loss 0.585307, acc 0.65625\n",
      "2022-01-26T01:51:11.749516: step 495, loss 0.632869, acc 0.65625\n",
      "2022-01-26T01:51:11.922869: step 496, loss 0.477432, acc 0.765625\n",
      "2022-01-26T01:51:12.090301: step 497, loss 0.586134, acc 0.6875\n",
      "2022-01-26T01:51:12.257188: step 498, loss 0.620774, acc 0.703125\n",
      "2022-01-26T01:51:12.431746: step 499, loss 0.5073, acc 0.8125\n",
      "2022-01-26T01:51:12.589321: step 500, loss 0.586508, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:51:13.218597: step 500, loss 0.629767, acc 0.645625\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-500 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:51:13.370247 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-500 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-500\n",
      "\n",
      "2022-01-26T01:51:13.599812: step 501, loss 0.541548, acc 0.671875\n",
      "2022-01-26T01:51:13.776148: step 502, loss 0.759133, acc 0.625\n",
      "2022-01-26T01:51:14.083586: step 503, loss 0.460214, acc 0.796875\n",
      "2022-01-26T01:51:14.343720: step 504, loss 0.51371, acc 0.78125\n",
      "2022-01-26T01:51:14.646418: step 505, loss 0.757776, acc 0.65625\n",
      "2022-01-26T01:51:14.833524: step 506, loss 0.569913, acc 0.71875\n",
      "2022-01-26T01:51:15.030706: step 507, loss 0.515815, acc 0.6875\n",
      "2022-01-26T01:51:15.223559: step 508, loss 0.669799, acc 0.671875\n",
      "2022-01-26T01:51:15.430592: step 509, loss 0.492522, acc 0.75\n",
      "2022-01-26T01:51:15.601588: step 510, loss 0.564679, acc 0.703125\n",
      "2022-01-26T01:51:15.783062: step 511, loss 0.46797, acc 0.78125\n",
      "2022-01-26T01:51:15.953410: step 512, loss 0.543341, acc 0.71875\n",
      "2022-01-26T01:51:16.145047: step 513, loss 0.574027, acc 0.703125\n",
      "2022-01-26T01:51:16.331608: step 514, loss 0.621426, acc 0.65625\n",
      "2022-01-26T01:51:16.540687: step 515, loss 0.730134, acc 0.65625\n",
      "2022-01-26T01:51:16.749286: step 516, loss 0.585086, acc 0.765625\n",
      "2022-01-26T01:51:16.958702: step 517, loss 0.52591, acc 0.71875\n",
      "2022-01-26T01:51:17.155641: step 518, loss 0.560423, acc 0.75\n",
      "2022-01-26T01:51:17.358154: step 519, loss 0.578487, acc 0.71875\n",
      "2022-01-26T01:51:17.544107: step 520, loss 0.614496, acc 0.640625\n",
      "2022-01-26T01:51:17.744832: step 521, loss 0.678546, acc 0.65625\n",
      "2022-01-26T01:51:17.917821: step 522, loss 0.745381, acc 0.625\n",
      "2022-01-26T01:51:18.098050: step 523, loss 0.480747, acc 0.75\n",
      "2022-01-26T01:51:18.270436: step 524, loss 0.60443, acc 0.640625\n",
      "2022-01-26T01:51:18.445127: step 525, loss 0.512199, acc 0.78125\n",
      "2022-01-26T01:51:18.611575: step 526, loss 0.512512, acc 0.765625\n",
      "2022-01-26T01:51:18.821535: step 527, loss 0.563261, acc 0.765625\n",
      "2022-01-26T01:51:18.999380: step 528, loss 0.687696, acc 0.640625\n",
      "2022-01-26T01:51:19.175298: step 529, loss 0.529386, acc 0.734375\n",
      "2022-01-26T01:51:19.346829: step 530, loss 0.617664, acc 0.671875\n",
      "2022-01-26T01:51:19.522244: step 531, loss 0.597699, acc 0.6875\n",
      "2022-01-26T01:51:19.696221: step 532, loss 0.538822, acc 0.6875\n",
      "2022-01-26T01:51:19.879894: step 533, loss 0.520913, acc 0.703125\n",
      "2022-01-26T01:51:20.073413: step 534, loss 0.600067, acc 0.734375\n",
      "2022-01-26T01:51:20.263039: step 535, loss 0.453355, acc 0.765625\n",
      "2022-01-26T01:51:20.447279: step 536, loss 0.599659, acc 0.6875\n",
      "2022-01-26T01:51:20.619155: step 537, loss 0.598961, acc 0.671875\n",
      "2022-01-26T01:51:20.777942: step 538, loss 0.52879, acc 0.71875\n",
      "2022-01-26T01:51:20.945894: step 539, loss 0.675149, acc 0.65625\n",
      "2022-01-26T01:51:21.103801: step 540, loss 0.470315, acc 0.765625\n",
      "2022-01-26T01:51:21.275865: step 541, loss 0.577597, acc 0.71875\n",
      "2022-01-26T01:51:21.444122: step 542, loss 0.621547, acc 0.6875\n",
      "2022-01-26T01:51:21.614563: step 543, loss 0.617811, acc 0.71875\n",
      "2022-01-26T01:51:21.781417: step 544, loss 0.402162, acc 0.796875\n",
      "2022-01-26T01:51:21.950444: step 545, loss 0.582955, acc 0.71875\n",
      "2022-01-26T01:51:22.106794: step 546, loss 0.626342, acc 0.703125\n",
      "2022-01-26T01:51:22.274117: step 547, loss 0.677131, acc 0.609375\n",
      "2022-01-26T01:51:22.440551: step 548, loss 0.553066, acc 0.71875\n",
      "2022-01-26T01:51:22.610211: step 549, loss 0.710353, acc 0.671875\n",
      "2022-01-26T01:51:22.768716: step 550, loss 0.620115, acc 0.71875\n",
      "2022-01-26T01:51:22.949642: step 551, loss 0.586166, acc 0.6875\n",
      "2022-01-26T01:51:23.115138: step 552, loss 0.582334, acc 0.71875\n",
      "2022-01-26T01:51:23.279972: step 553, loss 0.526362, acc 0.765625\n",
      "2022-01-26T01:51:23.446073: step 554, loss 0.611113, acc 0.703125\n",
      "2022-01-26T01:51:23.606945: step 555, loss 0.642002, acc 0.703125\n",
      "2022-01-26T01:51:23.785533: step 556, loss 0.619549, acc 0.65625\n",
      "2022-01-26T01:51:23.960202: step 557, loss 0.504705, acc 0.75\n",
      "2022-01-26T01:51:24.121295: step 558, loss 0.598337, acc 0.6875\n",
      "2022-01-26T01:51:24.283346: step 559, loss 0.56723, acc 0.65625\n",
      "2022-01-26T01:51:24.446237: step 560, loss 0.598642, acc 0.65625\n",
      "2022-01-26T01:51:24.608155: step 561, loss 0.677679, acc 0.65625\n",
      "2022-01-26T01:51:24.764311: step 562, loss 0.460331, acc 0.765625\n",
      "2022-01-26T01:51:24.926368: step 563, loss 0.515262, acc 0.75\n",
      "2022-01-26T01:51:25.082223: step 564, loss 0.704219, acc 0.65625\n",
      "2022-01-26T01:51:25.246994: step 565, loss 0.622741, acc 0.71875\n",
      "2022-01-26T01:51:25.401804: step 566, loss 0.6841, acc 0.625\n",
      "2022-01-26T01:51:25.565124: step 567, loss 0.451692, acc 0.765625\n",
      "2022-01-26T01:51:25.696856: step 568, loss 0.537492, acc 0.842105\n",
      "2022-01-26T01:51:25.858598: step 569, loss 0.661664, acc 0.671875\n",
      "2022-01-26T01:51:26.016252: step 570, loss 0.602667, acc 0.75\n",
      "2022-01-26T01:51:26.175636: step 571, loss 0.584455, acc 0.71875\n",
      "2022-01-26T01:51:26.334810: step 572, loss 0.616912, acc 0.6875\n",
      "2022-01-26T01:51:26.497130: step 573, loss 0.505699, acc 0.765625\n",
      "2022-01-26T01:51:26.654258: step 574, loss 0.572884, acc 0.703125\n",
      "2022-01-26T01:51:26.815843: step 575, loss 0.732915, acc 0.625\n",
      "2022-01-26T01:51:26.975609: step 576, loss 0.517363, acc 0.671875\n",
      "2022-01-26T01:51:27.139982: step 577, loss 0.455067, acc 0.765625\n",
      "2022-01-26T01:51:27.298943: step 578, loss 0.525196, acc 0.75\n",
      "2022-01-26T01:51:27.465083: step 579, loss 0.463977, acc 0.796875\n",
      "2022-01-26T01:51:27.627145: step 580, loss 0.504299, acc 0.8125\n",
      "2022-01-26T01:51:27.790053: step 581, loss 0.560885, acc 0.6875\n",
      "2022-01-26T01:51:27.954410: step 582, loss 0.516021, acc 0.703125\n",
      "2022-01-26T01:51:28.119007: step 583, loss 0.683065, acc 0.640625\n",
      "2022-01-26T01:51:28.277221: step 584, loss 0.525864, acc 0.71875\n",
      "2022-01-26T01:51:28.439381: step 585, loss 0.431118, acc 0.828125\n",
      "2022-01-26T01:51:28.598590: step 586, loss 0.383228, acc 0.8125\n",
      "2022-01-26T01:51:28.762560: step 587, loss 0.418716, acc 0.828125\n",
      "2022-01-26T01:51:28.920227: step 588, loss 0.480053, acc 0.78125\n",
      "2022-01-26T01:51:29.083794: step 589, loss 0.565979, acc 0.65625\n",
      "2022-01-26T01:51:29.242542: step 590, loss 0.510113, acc 0.75\n",
      "2022-01-26T01:51:29.402374: step 591, loss 0.573573, acc 0.671875\n",
      "2022-01-26T01:51:29.558773: step 592, loss 0.447505, acc 0.8125\n",
      "2022-01-26T01:51:29.720139: step 593, loss 0.550342, acc 0.765625\n",
      "2022-01-26T01:51:29.878938: step 594, loss 0.421216, acc 0.78125\n",
      "2022-01-26T01:51:30.042797: step 595, loss 0.549711, acc 0.765625\n",
      "2022-01-26T01:51:30.200997: step 596, loss 0.492313, acc 0.796875\n",
      "2022-01-26T01:51:30.372470: step 597, loss 0.540043, acc 0.75\n",
      "2022-01-26T01:51:30.529119: step 598, loss 0.730262, acc 0.609375\n",
      "2022-01-26T01:51:30.692832: step 599, loss 0.480684, acc 0.734375\n",
      "2022-01-26T01:51:30.861558: step 600, loss 0.363698, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:51:31.473767: step 600, loss 0.589592, acc 0.68375\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-600 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:51:31.625913 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-600 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-600\n",
      "\n",
      "2022-01-26T01:51:31.869058: step 601, loss 0.507961, acc 0.734375\n",
      "2022-01-26T01:51:32.074615: step 602, loss 0.508092, acc 0.71875\n",
      "2022-01-26T01:51:32.385794: step 603, loss 0.491163, acc 0.796875\n",
      "2022-01-26T01:51:32.669718: step 604, loss 0.544359, acc 0.78125\n",
      "2022-01-26T01:51:33.030497: step 605, loss 0.492772, acc 0.75\n",
      "2022-01-26T01:51:33.247475: step 606, loss 0.595411, acc 0.71875\n",
      "2022-01-26T01:51:33.471247: step 607, loss 0.660226, acc 0.6875\n",
      "2022-01-26T01:51:33.694474: step 608, loss 0.424114, acc 0.84375\n",
      "2022-01-26T01:51:33.906075: step 609, loss 0.471659, acc 0.765625\n",
      "2022-01-26T01:51:34.100590: step 610, loss 0.478535, acc 0.8125\n",
      "2022-01-26T01:51:34.292310: step 611, loss 0.605921, acc 0.65625\n",
      "2022-01-26T01:51:34.475184: step 612, loss 0.416189, acc 0.828125\n",
      "2022-01-26T01:51:34.704197: step 613, loss 0.421578, acc 0.859375\n",
      "2022-01-26T01:51:34.906571: step 614, loss 0.501303, acc 0.703125\n",
      "2022-01-26T01:51:35.117862: step 615, loss 0.548678, acc 0.71875\n",
      "2022-01-26T01:51:35.314447: step 616, loss 0.518497, acc 0.78125\n",
      "2022-01-26T01:51:35.503965: step 617, loss 0.643885, acc 0.6875\n",
      "2022-01-26T01:51:35.687735: step 618, loss 0.509219, acc 0.75\n",
      "2022-01-26T01:51:35.886228: step 619, loss 0.569029, acc 0.75\n",
      "2022-01-26T01:51:36.059600: step 620, loss 0.58395, acc 0.703125\n",
      "2022-01-26T01:51:36.241269: step 621, loss 0.48647, acc 0.71875\n",
      "2022-01-26T01:51:36.410684: step 622, loss 0.498815, acc 0.78125\n",
      "2022-01-26T01:51:36.592178: step 623, loss 0.437922, acc 0.828125\n",
      "2022-01-26T01:51:36.777699: step 624, loss 0.56854, acc 0.734375\n",
      "2022-01-26T01:51:36.958373: step 625, loss 0.577413, acc 0.671875\n",
      "2022-01-26T01:51:37.124154: step 626, loss 0.441827, acc 0.796875\n",
      "2022-01-26T01:51:37.313048: step 627, loss 0.569287, acc 0.703125\n",
      "2022-01-26T01:51:37.493915: step 628, loss 0.538911, acc 0.703125\n",
      "2022-01-26T01:51:37.693910: step 629, loss 0.477106, acc 0.796875\n",
      "2022-01-26T01:51:37.868504: step 630, loss 0.367249, acc 0.859375\n",
      "2022-01-26T01:51:38.047676: step 631, loss 0.34248, acc 0.84375\n",
      "2022-01-26T01:51:38.238204: step 632, loss 0.528261, acc 0.75\n",
      "2022-01-26T01:51:38.428517: step 633, loss 0.576249, acc 0.734375\n",
      "2022-01-26T01:51:38.603483: step 634, loss 0.442377, acc 0.765625\n",
      "2022-01-26T01:51:38.789268: step 635, loss 0.522585, acc 0.75\n",
      "2022-01-26T01:51:38.960444: step 636, loss 0.481982, acc 0.765625\n",
      "2022-01-26T01:51:39.125261: step 637, loss 0.477464, acc 0.75\n",
      "2022-01-26T01:51:39.285560: step 638, loss 0.618776, acc 0.6875\n",
      "2022-01-26T01:51:39.448004: step 639, loss 0.37524, acc 0.875\n",
      "2022-01-26T01:51:39.607873: step 640, loss 0.527104, acc 0.75\n",
      "2022-01-26T01:51:39.777012: step 641, loss 0.480002, acc 0.765625\n",
      "2022-01-26T01:51:39.934829: step 642, loss 0.506803, acc 0.796875\n",
      "2022-01-26T01:51:40.100704: step 643, loss 0.345888, acc 0.859375\n",
      "2022-01-26T01:51:40.267033: step 644, loss 0.504518, acc 0.765625\n",
      "2022-01-26T01:51:40.431811: step 645, loss 0.695582, acc 0.65625\n",
      "2022-01-26T01:51:40.590910: step 646, loss 0.333215, acc 0.890625\n",
      "2022-01-26T01:51:40.755070: step 647, loss 0.656471, acc 0.671875\n",
      "2022-01-26T01:51:40.911930: step 648, loss 0.505481, acc 0.734375\n",
      "2022-01-26T01:51:41.076077: step 649, loss 0.519514, acc 0.734375\n",
      "2022-01-26T01:51:41.234377: step 650, loss 0.586136, acc 0.625\n",
      "2022-01-26T01:51:41.401690: step 651, loss 0.450988, acc 0.765625\n",
      "2022-01-26T01:51:41.565746: step 652, loss 0.565923, acc 0.703125\n",
      "2022-01-26T01:51:41.730579: step 653, loss 0.49673, acc 0.78125\n",
      "2022-01-26T01:51:41.897711: step 654, loss 0.54378, acc 0.75\n",
      "2022-01-26T01:51:42.065102: step 655, loss 0.623645, acc 0.703125\n",
      "2022-01-26T01:51:42.226051: step 656, loss 0.48172, acc 0.765625\n",
      "2022-01-26T01:51:42.389878: step 657, loss 0.427435, acc 0.78125\n",
      "2022-01-26T01:51:42.547979: step 658, loss 0.455726, acc 0.796875\n",
      "2022-01-26T01:51:42.719961: step 659, loss 0.442614, acc 0.78125\n",
      "2022-01-26T01:51:42.890599: step 660, loss 0.480074, acc 0.734375\n",
      "2022-01-26T01:51:43.064807: step 661, loss 0.377846, acc 0.78125\n",
      "2022-01-26T01:51:43.223696: step 662, loss 0.381533, acc 0.828125\n",
      "2022-01-26T01:51:43.385340: step 663, loss 0.495414, acc 0.796875\n",
      "2022-01-26T01:51:43.543431: step 664, loss 0.585264, acc 0.765625\n",
      "2022-01-26T01:51:43.706118: step 665, loss 0.454852, acc 0.765625\n",
      "2022-01-26T01:51:43.865354: step 666, loss 0.573825, acc 0.671875\n",
      "2022-01-26T01:51:44.032015: step 667, loss 0.513151, acc 0.78125\n",
      "2022-01-26T01:51:44.191706: step 668, loss 0.432793, acc 0.734375\n",
      "2022-01-26T01:51:44.360108: step 669, loss 0.456277, acc 0.796875\n",
      "2022-01-26T01:51:44.534665: step 670, loss 0.561885, acc 0.703125\n",
      "2022-01-26T01:51:44.697845: step 671, loss 0.439196, acc 0.765625\n",
      "2022-01-26T01:51:44.858850: step 672, loss 0.57279, acc 0.703125\n",
      "2022-01-26T01:51:45.023007: step 673, loss 0.472167, acc 0.78125\n",
      "2022-01-26T01:51:45.180419: step 674, loss 0.466479, acc 0.765625\n",
      "2022-01-26T01:51:45.345005: step 675, loss 0.367729, acc 0.859375\n",
      "2022-01-26T01:51:45.502677: step 676, loss 0.457194, acc 0.75\n",
      "2022-01-26T01:51:45.666835: step 677, loss 0.466277, acc 0.78125\n",
      "2022-01-26T01:51:45.825636: step 678, loss 0.550168, acc 0.703125\n",
      "2022-01-26T01:51:45.990157: step 679, loss 0.447472, acc 0.8125\n",
      "2022-01-26T01:51:46.162097: step 680, loss 0.541636, acc 0.671875\n",
      "2022-01-26T01:51:46.330618: step 681, loss 0.423495, acc 0.78125\n",
      "2022-01-26T01:51:46.506376: step 682, loss 0.509761, acc 0.75\n",
      "2022-01-26T01:51:46.676001: step 683, loss 0.559377, acc 0.703125\n",
      "2022-01-26T01:51:46.833474: step 684, loss 0.436125, acc 0.78125\n",
      "2022-01-26T01:51:46.995706: step 685, loss 0.437397, acc 0.75\n",
      "2022-01-26T01:51:47.158244: step 686, loss 0.453959, acc 0.796875\n",
      "2022-01-26T01:51:47.319439: step 687, loss 0.612602, acc 0.65625\n",
      "2022-01-26T01:51:47.480814: step 688, loss 0.54448, acc 0.6875\n",
      "2022-01-26T01:51:47.644416: step 689, loss 0.544147, acc 0.734375\n",
      "2022-01-26T01:51:47.809822: step 690, loss 0.56888, acc 0.71875\n",
      "2022-01-26T01:51:47.971879: step 691, loss 0.462687, acc 0.8125\n",
      "2022-01-26T01:51:48.131279: step 692, loss 0.515003, acc 0.765625\n",
      "2022-01-26T01:51:48.295482: step 693, loss 0.53846, acc 0.75\n",
      "2022-01-26T01:51:48.455207: step 694, loss 0.471631, acc 0.71875\n",
      "2022-01-26T01:51:48.621321: step 695, loss 0.641194, acc 0.703125\n",
      "2022-01-26T01:51:48.786215: step 696, loss 0.475086, acc 0.6875\n",
      "2022-01-26T01:51:48.947717: step 697, loss 0.47036, acc 0.796875\n",
      "2022-01-26T01:51:49.111030: step 698, loss 0.382329, acc 0.828125\n",
      "2022-01-26T01:51:49.273501: step 699, loss 0.510248, acc 0.765625\n",
      "2022-01-26T01:51:49.431847: step 700, loss 0.524105, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:51:50.038233: step 700, loss 0.577835, acc 0.69875\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-700 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:51:50.190960 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-700 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-700\n",
      "\n",
      "2022-01-26T01:51:50.420886: step 701, loss 0.471295, acc 0.734375\n",
      "2022-01-26T01:51:50.593306: step 702, loss 0.621758, acc 0.640625\n",
      "2022-01-26T01:51:50.862175: step 703, loss 0.611077, acc 0.703125\n",
      "2022-01-26T01:51:51.135255: step 704, loss 0.551595, acc 0.765625\n",
      "2022-01-26T01:51:51.358595: step 705, loss 0.575728, acc 0.734375\n",
      "2022-01-26T01:51:51.605925: step 706, loss 0.480993, acc 0.75\n",
      "2022-01-26T01:51:51.850828: step 707, loss 0.471777, acc 0.796875\n",
      "2022-01-26T01:51:52.068322: step 708, loss 0.476794, acc 0.796875\n",
      "2022-01-26T01:51:52.284022: step 709, loss 0.457562, acc 0.765625\n",
      "2022-01-26T01:51:52.459088: step 710, loss 0.407157, acc 0.815789\n",
      "2022-01-26T01:51:52.665467: step 711, loss 0.430843, acc 0.765625\n",
      "2022-01-26T01:51:52.849126: step 712, loss 0.408107, acc 0.796875\n",
      "2022-01-26T01:51:53.034941: step 713, loss 0.442129, acc 0.796875\n",
      "2022-01-26T01:51:53.276613: step 714, loss 0.390696, acc 0.8125\n",
      "2022-01-26T01:51:53.497451: step 715, loss 0.409456, acc 0.90625\n",
      "2022-01-26T01:51:53.812084: step 716, loss 0.434903, acc 0.8125\n",
      "2022-01-26T01:51:54.153909: step 717, loss 0.434186, acc 0.78125\n",
      "2022-01-26T01:51:54.378843: step 718, loss 0.460372, acc 0.828125\n",
      "2022-01-26T01:51:54.582341: step 719, loss 0.447825, acc 0.78125\n",
      "2022-01-26T01:51:54.793977: step 720, loss 0.4539, acc 0.734375\n",
      "2022-01-26T01:51:54.991176: step 721, loss 0.346746, acc 0.875\n",
      "2022-01-26T01:51:55.219654: step 722, loss 0.403977, acc 0.828125\n",
      "2022-01-26T01:51:55.405379: step 723, loss 0.41417, acc 0.796875\n",
      "2022-01-26T01:51:55.593144: step 724, loss 0.391066, acc 0.828125\n",
      "2022-01-26T01:51:55.789467: step 725, loss 0.441164, acc 0.796875\n",
      "2022-01-26T01:51:55.989263: step 726, loss 0.578378, acc 0.6875\n",
      "2022-01-26T01:51:56.195023: step 727, loss 0.385574, acc 0.859375\n",
      "2022-01-26T01:51:56.391256: step 728, loss 0.407061, acc 0.84375\n",
      "2022-01-26T01:51:56.622141: step 729, loss 0.369851, acc 0.84375\n",
      "2022-01-26T01:51:56.801490: step 730, loss 0.37627, acc 0.84375\n",
      "2022-01-26T01:51:57.002525: step 731, loss 0.533074, acc 0.734375\n",
      "2022-01-26T01:51:57.290562: step 732, loss 0.36215, acc 0.828125\n",
      "2022-01-26T01:51:57.458520: step 733, loss 0.530203, acc 0.734375\n",
      "2022-01-26T01:51:57.631090: step 734, loss 0.492787, acc 0.78125\n",
      "2022-01-26T01:51:57.796334: step 735, loss 0.301767, acc 0.890625\n",
      "2022-01-26T01:51:58.022969: step 736, loss 0.319992, acc 0.859375\n",
      "2022-01-26T01:51:58.211080: step 737, loss 0.564154, acc 0.71875\n",
      "2022-01-26T01:51:58.390108: step 738, loss 0.443956, acc 0.8125\n",
      "2022-01-26T01:51:58.631731: step 739, loss 0.434189, acc 0.78125\n",
      "2022-01-26T01:51:58.818072: step 740, loss 0.556566, acc 0.6875\n",
      "2022-01-26T01:51:58.995803: step 741, loss 0.438175, acc 0.765625\n",
      "2022-01-26T01:51:59.187041: step 742, loss 0.395737, acc 0.8125\n",
      "2022-01-26T01:51:59.360023: step 743, loss 0.403765, acc 0.859375\n",
      "2022-01-26T01:51:59.566499: step 744, loss 0.420169, acc 0.78125\n",
      "2022-01-26T01:51:59.837915: step 745, loss 0.397634, acc 0.796875\n",
      "2022-01-26T01:52:00.016927: step 746, loss 0.564727, acc 0.703125\n",
      "2022-01-26T01:52:00.183307: step 747, loss 0.470829, acc 0.734375\n",
      "2022-01-26T01:52:00.349818: step 748, loss 0.55352, acc 0.703125\n",
      "2022-01-26T01:52:00.541420: step 749, loss 0.370727, acc 0.890625\n",
      "2022-01-26T01:52:00.747767: step 750, loss 0.451194, acc 0.84375\n",
      "2022-01-26T01:52:00.942390: step 751, loss 0.481528, acc 0.78125\n",
      "2022-01-26T01:52:01.128241: step 752, loss 0.41782, acc 0.8125\n",
      "2022-01-26T01:52:01.310693: step 753, loss 0.506437, acc 0.71875\n",
      "2022-01-26T01:52:01.477608: step 754, loss 0.533323, acc 0.75\n",
      "2022-01-26T01:52:01.648515: step 755, loss 0.471456, acc 0.796875\n",
      "2022-01-26T01:52:01.844616: step 756, loss 0.445326, acc 0.796875\n",
      "2022-01-26T01:52:02.006275: step 757, loss 0.426752, acc 0.796875\n",
      "2022-01-26T01:52:02.171169: step 758, loss 0.562026, acc 0.703125\n",
      "2022-01-26T01:52:02.337466: step 759, loss 0.475434, acc 0.703125\n",
      "2022-01-26T01:52:02.511077: step 760, loss 0.48026, acc 0.765625\n",
      "2022-01-26T01:52:02.692942: step 761, loss 0.350014, acc 0.828125\n",
      "2022-01-26T01:52:02.869629: step 762, loss 0.493963, acc 0.765625\n",
      "2022-01-26T01:52:03.035683: step 763, loss 0.674327, acc 0.734375\n",
      "2022-01-26T01:52:03.215853: step 764, loss 0.376419, acc 0.859375\n",
      "2022-01-26T01:52:03.384644: step 765, loss 0.590181, acc 0.671875\n",
      "2022-01-26T01:52:03.597355: step 766, loss 0.414079, acc 0.859375\n",
      "2022-01-26T01:52:03.759048: step 767, loss 0.381968, acc 0.875\n",
      "2022-01-26T01:52:03.919345: step 768, loss 0.45161, acc 0.765625\n",
      "2022-01-26T01:52:04.094075: step 769, loss 0.492007, acc 0.75\n",
      "2022-01-26T01:52:04.287558: step 770, loss 0.407445, acc 0.8125\n",
      "2022-01-26T01:52:04.464670: step 771, loss 0.342346, acc 0.84375\n",
      "2022-01-26T01:52:04.647576: step 772, loss 0.418727, acc 0.84375\n",
      "2022-01-26T01:52:04.880253: step 773, loss 0.504156, acc 0.796875\n",
      "2022-01-26T01:52:05.097677: step 774, loss 0.516911, acc 0.75\n",
      "2022-01-26T01:52:05.322234: step 775, loss 0.383925, acc 0.828125\n",
      "2022-01-26T01:52:05.531416: step 776, loss 0.330727, acc 0.875\n",
      "2022-01-26T01:52:05.697259: step 777, loss 0.496116, acc 0.765625\n",
      "2022-01-26T01:52:05.874921: step 778, loss 0.363776, acc 0.828125\n",
      "2022-01-26T01:52:06.037452: step 779, loss 0.546291, acc 0.734375\n",
      "2022-01-26T01:52:06.213133: step 780, loss 0.414853, acc 0.8125\n",
      "2022-01-26T01:52:06.377708: step 781, loss 0.459985, acc 0.828125\n",
      "2022-01-26T01:52:06.584634: step 782, loss 0.317897, acc 0.890625\n",
      "2022-01-26T01:52:06.746633: step 783, loss 0.46124, acc 0.796875\n",
      "2022-01-26T01:52:06.906511: step 784, loss 0.382335, acc 0.8125\n",
      "2022-01-26T01:52:07.063375: step 785, loss 0.417913, acc 0.75\n",
      "2022-01-26T01:52:07.239112: step 786, loss 0.415496, acc 0.796875\n",
      "2022-01-26T01:52:07.413946: step 787, loss 0.566877, acc 0.78125\n",
      "2022-01-26T01:52:07.598398: step 788, loss 0.462765, acc 0.765625\n",
      "2022-01-26T01:52:07.776238: step 789, loss 0.44228, acc 0.765625\n",
      "2022-01-26T01:52:07.962991: step 790, loss 0.381687, acc 0.8125\n",
      "2022-01-26T01:52:08.128075: step 791, loss 0.389791, acc 0.828125\n",
      "2022-01-26T01:52:08.293669: step 792, loss 0.347165, acc 0.828125\n",
      "2022-01-26T01:52:08.476055: step 793, loss 0.57205, acc 0.6875\n",
      "2022-01-26T01:52:08.645558: step 794, loss 0.379107, acc 0.796875\n",
      "2022-01-26T01:52:08.822980: step 795, loss 0.365065, acc 0.875\n",
      "2022-01-26T01:52:09.004888: step 796, loss 0.469029, acc 0.703125\n",
      "2022-01-26T01:52:09.175115: step 797, loss 0.489949, acc 0.796875\n",
      "2022-01-26T01:52:09.357108: step 798, loss 0.509176, acc 0.734375\n",
      "2022-01-26T01:52:09.522996: step 799, loss 0.363524, acc 0.828125\n",
      "2022-01-26T01:52:09.695298: step 800, loss 0.399825, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:52:10.331275: step 800, loss 0.621954, acc 0.661875\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-800 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:52:10.488728 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-800 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-800\n",
      "\n",
      "2022-01-26T01:52:10.737077: step 801, loss 0.333491, acc 0.890625\n",
      "2022-01-26T01:52:10.910280: step 802, loss 0.47493, acc 0.734375\n",
      "2022-01-26T01:52:11.199752: step 803, loss 0.530176, acc 0.765625\n",
      "2022-01-26T01:52:11.501338: step 804, loss 0.476926, acc 0.78125\n",
      "2022-01-26T01:52:11.836669: step 805, loss 0.324393, acc 0.890625\n",
      "2022-01-26T01:52:12.139751: step 806, loss 0.439591, acc 0.796875\n",
      "2022-01-26T01:52:12.351835: step 807, loss 0.501094, acc 0.734375\n",
      "2022-01-26T01:52:12.559843: step 808, loss 0.470323, acc 0.765625\n",
      "2022-01-26T01:52:12.799668: step 809, loss 0.445361, acc 0.78125\n",
      "2022-01-26T01:52:12.995361: step 810, loss 0.470152, acc 0.765625\n",
      "2022-01-26T01:52:13.214328: step 811, loss 0.404781, acc 0.8125\n",
      "2022-01-26T01:52:13.393644: step 812, loss 0.429103, acc 0.8125\n",
      "2022-01-26T01:52:13.589845: step 813, loss 0.466336, acc 0.8125\n",
      "2022-01-26T01:52:13.787447: step 814, loss 0.445834, acc 0.765625\n",
      "2022-01-26T01:52:13.997367: step 815, loss 0.36161, acc 0.84375\n",
      "2022-01-26T01:52:14.181199: step 816, loss 0.60659, acc 0.75\n",
      "2022-01-26T01:52:14.408196: step 817, loss 0.439038, acc 0.859375\n",
      "2022-01-26T01:52:14.669585: step 818, loss 0.415218, acc 0.8125\n",
      "2022-01-26T01:52:14.923691: step 819, loss 0.539455, acc 0.71875\n",
      "2022-01-26T01:52:15.139688: step 820, loss 0.428943, acc 0.78125\n",
      "2022-01-26T01:52:15.338214: step 821, loss 0.328412, acc 0.859375\n",
      "2022-01-26T01:52:15.536019: step 822, loss 0.52576, acc 0.75\n",
      "2022-01-26T01:52:15.739192: step 823, loss 0.385309, acc 0.8125\n",
      "2022-01-26T01:52:15.941537: step 824, loss 0.446349, acc 0.828125\n",
      "2022-01-26T01:52:16.122958: step 825, loss 0.467929, acc 0.796875\n",
      "2022-01-26T01:52:16.317753: step 826, loss 0.424436, acc 0.796875\n",
      "2022-01-26T01:52:16.499551: step 827, loss 0.404471, acc 0.8125\n",
      "2022-01-26T01:52:16.681763: step 828, loss 0.432403, acc 0.8125\n",
      "2022-01-26T01:52:16.892903: step 829, loss 0.348505, acc 0.859375\n",
      "2022-01-26T01:52:17.079479: step 830, loss 0.444909, acc 0.75\n",
      "2022-01-26T01:52:17.297605: step 831, loss 0.387142, acc 0.859375\n",
      "2022-01-26T01:52:17.490218: step 832, loss 0.461334, acc 0.78125\n",
      "2022-01-26T01:52:17.706694: step 833, loss 0.41513, acc 0.84375\n",
      "2022-01-26T01:52:17.921241: step 834, loss 0.545423, acc 0.8125\n",
      "2022-01-26T01:52:18.101299: step 835, loss 0.375429, acc 0.84375\n",
      "2022-01-26T01:52:18.285696: step 836, loss 0.394349, acc 0.84375\n",
      "2022-01-26T01:52:18.448969: step 837, loss 0.308506, acc 0.90625\n",
      "2022-01-26T01:52:18.627906: step 838, loss 0.388224, acc 0.8125\n",
      "2022-01-26T01:52:18.816597: step 839, loss 0.444399, acc 0.796875\n",
      "2022-01-26T01:52:19.011042: step 840, loss 0.474548, acc 0.75\n",
      "2022-01-26T01:52:19.205130: step 841, loss 0.370858, acc 0.8125\n",
      "2022-01-26T01:52:19.414784: step 842, loss 0.395874, acc 0.828125\n",
      "2022-01-26T01:52:19.588051: step 843, loss 0.354121, acc 0.890625\n",
      "2022-01-26T01:52:19.754978: step 844, loss 0.366822, acc 0.890625\n",
      "2022-01-26T01:52:19.947430: step 845, loss 0.455853, acc 0.75\n",
      "2022-01-26T01:52:20.145300: step 846, loss 0.441932, acc 0.84375\n",
      "2022-01-26T01:52:20.311787: step 847, loss 0.623578, acc 0.71875\n",
      "2022-01-26T01:52:20.492643: step 848, loss 0.420285, acc 0.84375\n",
      "2022-01-26T01:52:20.687126: step 849, loss 0.407153, acc 0.796875\n",
      "2022-01-26T01:52:20.850570: step 850, loss 0.351988, acc 0.8125\n",
      "2022-01-26T01:52:21.007088: step 851, loss 0.462758, acc 0.78125\n",
      "2022-01-26T01:52:21.142851: step 852, loss 0.565446, acc 0.763158\n",
      "2022-01-26T01:52:21.324839: step 853, loss 0.419563, acc 0.78125\n",
      "2022-01-26T01:52:21.499508: step 854, loss 0.310853, acc 0.84375\n",
      "2022-01-26T01:52:21.665971: step 855, loss 0.248491, acc 0.90625\n",
      "2022-01-26T01:52:21.851292: step 856, loss 0.434866, acc 0.796875\n",
      "2022-01-26T01:52:22.052856: step 857, loss 0.297639, acc 0.890625\n",
      "2022-01-26T01:52:22.218142: step 858, loss 0.38499, acc 0.828125\n",
      "2022-01-26T01:52:22.382311: step 859, loss 0.459279, acc 0.765625\n",
      "2022-01-26T01:52:22.543908: step 860, loss 0.198781, acc 0.9375\n",
      "2022-01-26T01:52:22.710546: step 861, loss 0.441357, acc 0.734375\n",
      "2022-01-26T01:52:22.898000: step 862, loss 0.500427, acc 0.71875\n",
      "2022-01-26T01:52:23.095193: step 863, loss 0.357968, acc 0.859375\n",
      "2022-01-26T01:52:23.292115: step 864, loss 0.355504, acc 0.84375\n",
      "2022-01-26T01:52:23.491510: step 865, loss 0.325764, acc 0.859375\n",
      "2022-01-26T01:52:23.682537: step 866, loss 0.284945, acc 0.890625\n",
      "2022-01-26T01:52:23.840764: step 867, loss 0.296552, acc 0.828125\n",
      "2022-01-26T01:52:24.024718: step 868, loss 0.31593, acc 0.859375\n",
      "2022-01-26T01:52:24.210776: step 869, loss 0.364927, acc 0.796875\n",
      "2022-01-26T01:52:24.383275: step 870, loss 0.435347, acc 0.78125\n",
      "2022-01-26T01:52:24.544641: step 871, loss 0.470729, acc 0.828125\n",
      "2022-01-26T01:52:24.712558: step 872, loss 0.471926, acc 0.828125\n",
      "2022-01-26T01:52:24.889660: step 873, loss 0.350834, acc 0.875\n",
      "2022-01-26T01:52:25.062171: step 874, loss 0.351831, acc 0.875\n",
      "2022-01-26T01:52:25.251713: step 875, loss 0.406113, acc 0.8125\n",
      "2022-01-26T01:52:25.442607: step 876, loss 0.406545, acc 0.796875\n",
      "2022-01-26T01:52:25.615636: step 877, loss 0.463664, acc 0.78125\n",
      "2022-01-26T01:52:25.792909: step 878, loss 0.390764, acc 0.8125\n",
      "2022-01-26T01:52:25.955256: step 879, loss 0.358424, acc 0.828125\n",
      "2022-01-26T01:52:26.119427: step 880, loss 0.344497, acc 0.859375\n",
      "2022-01-26T01:52:26.292887: step 881, loss 0.390311, acc 0.8125\n",
      "2022-01-26T01:52:26.457865: step 882, loss 0.31901, acc 0.84375\n",
      "2022-01-26T01:52:26.618204: step 883, loss 0.391826, acc 0.859375\n",
      "2022-01-26T01:52:26.790536: step 884, loss 0.387448, acc 0.8125\n",
      "2022-01-26T01:52:26.948868: step 885, loss 0.403301, acc 0.796875\n",
      "2022-01-26T01:52:27.114455: step 886, loss 0.453105, acc 0.765625\n",
      "2022-01-26T01:52:27.272368: step 887, loss 0.322105, acc 0.84375\n",
      "2022-01-26T01:52:27.438578: step 888, loss 0.417144, acc 0.828125\n",
      "2022-01-26T01:52:27.591091: step 889, loss 0.448805, acc 0.796875\n",
      "2022-01-26T01:52:27.750105: step 890, loss 0.278136, acc 0.90625\n",
      "2022-01-26T01:52:27.904772: step 891, loss 0.340319, acc 0.84375\n",
      "2022-01-26T01:52:28.067533: step 892, loss 0.384316, acc 0.859375\n",
      "2022-01-26T01:52:28.229247: step 893, loss 0.320633, acc 0.875\n",
      "2022-01-26T01:52:28.394466: step 894, loss 0.349082, acc 0.890625\n",
      "2022-01-26T01:52:28.564367: step 895, loss 0.251541, acc 0.953125\n",
      "2022-01-26T01:52:28.741114: step 896, loss 0.327505, acc 0.890625\n",
      "2022-01-26T01:52:29.115029: step 897, loss 0.497108, acc 0.734375\n",
      "2022-01-26T01:52:29.344147: step 898, loss 0.344554, acc 0.84375\n",
      "2022-01-26T01:52:29.537582: step 899, loss 0.371305, acc 0.859375\n",
      "2022-01-26T01:52:29.743048: step 900, loss 0.472922, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:52:30.396134: step 900, loss 0.559671, acc 0.718125\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-900 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:52:30.555056 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-900 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-900\n",
      "\n",
      "2022-01-26T01:52:30.773687: step 901, loss 0.397929, acc 0.84375\n",
      "2022-01-26T01:52:30.956996: step 902, loss 0.507178, acc 0.78125\n",
      "2022-01-26T01:52:31.210344: step 903, loss 0.297471, acc 0.890625\n",
      "2022-01-26T01:52:31.451572: step 904, loss 0.295382, acc 0.84375\n",
      "2022-01-26T01:52:31.689632: step 905, loss 0.301993, acc 0.859375\n",
      "2022-01-26T01:52:31.878850: step 906, loss 0.341809, acc 0.875\n",
      "2022-01-26T01:52:32.073260: step 907, loss 0.454976, acc 0.828125\n",
      "2022-01-26T01:52:32.280778: step 908, loss 0.408863, acc 0.875\n",
      "2022-01-26T01:52:32.471197: step 909, loss 0.437206, acc 0.8125\n",
      "2022-01-26T01:52:32.680039: step 910, loss 0.305983, acc 0.875\n",
      "2022-01-26T01:52:32.871600: step 911, loss 0.32033, acc 0.875\n",
      "2022-01-26T01:52:33.098955: step 912, loss 0.520542, acc 0.734375\n",
      "2022-01-26T01:52:33.296839: step 913, loss 0.456834, acc 0.75\n",
      "2022-01-26T01:52:33.483806: step 914, loss 0.326725, acc 0.875\n",
      "2022-01-26T01:52:33.669355: step 915, loss 0.347426, acc 0.84375\n",
      "2022-01-26T01:52:33.857208: step 916, loss 0.337722, acc 0.90625\n",
      "2022-01-26T01:52:34.053740: step 917, loss 0.376362, acc 0.859375\n",
      "2022-01-26T01:52:34.290687: step 918, loss 0.322992, acc 0.890625\n",
      "2022-01-26T01:52:34.587066: step 919, loss 0.344519, acc 0.828125\n",
      "2022-01-26T01:52:34.849842: step 920, loss 0.301982, acc 0.875\n",
      "2022-01-26T01:52:35.105526: step 921, loss 0.291847, acc 0.8125\n",
      "2022-01-26T01:52:35.335598: step 922, loss 0.330874, acc 0.890625\n",
      "2022-01-26T01:52:35.566149: step 923, loss 0.411977, acc 0.8125\n",
      "2022-01-26T01:52:35.792146: step 924, loss 0.446866, acc 0.71875\n",
      "2022-01-26T01:52:36.071855: step 925, loss 0.309291, acc 0.890625\n",
      "2022-01-26T01:52:36.313363: step 926, loss 0.346345, acc 0.875\n",
      "2022-01-26T01:52:36.494434: step 927, loss 0.434595, acc 0.796875\n",
      "2022-01-26T01:52:36.686067: step 928, loss 0.384226, acc 0.84375\n",
      "2022-01-26T01:52:36.872528: step 929, loss 0.548219, acc 0.78125\n",
      "2022-01-26T01:52:37.082604: step 930, loss 0.367771, acc 0.84375\n",
      "2022-01-26T01:52:37.317348: step 931, loss 0.443533, acc 0.765625\n",
      "2022-01-26T01:52:37.547097: step 932, loss 0.441436, acc 0.828125\n",
      "2022-01-26T01:52:37.714323: step 933, loss 0.351293, acc 0.859375\n",
      "2022-01-26T01:52:37.908883: step 934, loss 0.350786, acc 0.859375\n",
      "2022-01-26T01:52:38.120471: step 935, loss 0.36997, acc 0.78125\n",
      "2022-01-26T01:52:38.305928: step 936, loss 0.394038, acc 0.796875\n",
      "2022-01-26T01:52:38.502094: step 937, loss 0.440175, acc 0.8125\n",
      "2022-01-26T01:52:38.704835: step 938, loss 0.365609, acc 0.828125\n",
      "2022-01-26T01:52:38.893896: step 939, loss 0.27, acc 0.859375\n",
      "2022-01-26T01:52:39.101805: step 940, loss 0.34532, acc 0.90625\n",
      "2022-01-26T01:52:39.300388: step 941, loss 0.279886, acc 0.875\n",
      "2022-01-26T01:52:39.491748: step 942, loss 0.441064, acc 0.8125\n",
      "2022-01-26T01:52:39.659488: step 943, loss 0.411395, acc 0.78125\n",
      "2022-01-26T01:52:39.839070: step 944, loss 0.337576, acc 0.796875\n",
      "2022-01-26T01:52:40.083515: step 945, loss 0.282284, acc 0.859375\n",
      "2022-01-26T01:52:40.267006: step 946, loss 0.452108, acc 0.828125\n",
      "2022-01-26T01:52:40.436491: step 947, loss 0.553024, acc 0.75\n",
      "2022-01-26T01:52:40.595369: step 948, loss 0.319453, acc 0.859375\n",
      "2022-01-26T01:52:40.768110: step 949, loss 0.330797, acc 0.859375\n",
      "2022-01-26T01:52:40.952663: step 950, loss 0.456301, acc 0.75\n",
      "2022-01-26T01:52:41.133413: step 951, loss 0.364577, acc 0.84375\n",
      "2022-01-26T01:52:41.309672: step 952, loss 0.283289, acc 0.890625\n",
      "2022-01-26T01:52:41.481503: step 953, loss 0.442696, acc 0.75\n",
      "2022-01-26T01:52:41.640963: step 954, loss 0.299239, acc 0.84375\n",
      "2022-01-26T01:52:41.806134: step 955, loss 0.323999, acc 0.875\n",
      "2022-01-26T01:52:41.966296: step 956, loss 0.326721, acc 0.796875\n",
      "2022-01-26T01:52:42.131707: step 957, loss 0.440448, acc 0.75\n",
      "2022-01-26T01:52:42.291353: step 958, loss 0.375543, acc 0.8125\n",
      "2022-01-26T01:52:42.463490: step 959, loss 0.272095, acc 0.890625\n",
      "2022-01-26T01:52:42.628597: step 960, loss 0.390852, acc 0.828125\n",
      "2022-01-26T01:52:42.795500: step 961, loss 0.457761, acc 0.828125\n",
      "2022-01-26T01:52:42.958667: step 962, loss 0.299172, acc 0.921875\n",
      "2022-01-26T01:52:43.125314: step 963, loss 0.325454, acc 0.859375\n",
      "2022-01-26T01:52:43.290244: step 964, loss 0.425078, acc 0.78125\n",
      "2022-01-26T01:52:43.457139: step 965, loss 0.373497, acc 0.875\n",
      "2022-01-26T01:52:43.622409: step 966, loss 0.38577, acc 0.84375\n",
      "2022-01-26T01:52:43.825870: step 967, loss 0.351643, acc 0.828125\n",
      "2022-01-26T01:52:43.999739: step 968, loss 0.53002, acc 0.796875\n",
      "2022-01-26T01:52:44.166213: step 969, loss 0.464426, acc 0.734375\n",
      "2022-01-26T01:52:44.328592: step 970, loss 0.266649, acc 0.859375\n",
      "2022-01-26T01:52:44.498813: step 971, loss 0.418632, acc 0.796875\n",
      "2022-01-26T01:52:44.669943: step 972, loss 0.332543, acc 0.875\n",
      "2022-01-26T01:52:44.840551: step 973, loss 0.476921, acc 0.75\n",
      "2022-01-26T01:52:45.009921: step 974, loss 0.487803, acc 0.765625\n",
      "2022-01-26T01:52:45.181912: step 975, loss 0.369505, acc 0.859375\n",
      "2022-01-26T01:52:45.344839: step 976, loss 0.25238, acc 0.9375\n",
      "2022-01-26T01:52:45.512538: step 977, loss 0.40884, acc 0.75\n",
      "2022-01-26T01:52:45.678057: step 978, loss 0.192951, acc 0.890625\n",
      "2022-01-26T01:52:45.841185: step 979, loss 0.44062, acc 0.859375\n",
      "2022-01-26T01:52:45.997676: step 980, loss 0.430791, acc 0.828125\n",
      "2022-01-26T01:52:46.155926: step 981, loss 0.423248, acc 0.796875\n",
      "2022-01-26T01:52:46.310493: step 982, loss 0.369176, acc 0.8125\n",
      "2022-01-26T01:52:46.468586: step 983, loss 0.565728, acc 0.703125\n",
      "2022-01-26T01:52:46.624044: step 984, loss 0.424973, acc 0.78125\n",
      "2022-01-26T01:52:46.796527: step 985, loss 0.388326, acc 0.84375\n",
      "2022-01-26T01:52:46.958738: step 986, loss 0.361828, acc 0.796875\n",
      "2022-01-26T01:52:47.126110: step 987, loss 0.478478, acc 0.71875\n",
      "2022-01-26T01:52:47.281651: step 988, loss 0.359033, acc 0.859375\n",
      "2022-01-26T01:52:47.438128: step 989, loss 0.195739, acc 0.953125\n",
      "2022-01-26T01:52:47.601188: step 990, loss 0.371499, acc 0.8125\n",
      "2022-01-26T01:52:47.787253: step 991, loss 0.314866, acc 0.8125\n",
      "2022-01-26T01:52:47.978945: step 992, loss 0.448137, acc 0.8125\n",
      "2022-01-26T01:52:48.164640: step 993, loss 0.400786, acc 0.8125\n",
      "2022-01-26T01:52:48.300840: step 994, loss 0.256352, acc 0.868421\n",
      "2022-01-26T01:52:48.465941: step 995, loss 0.355016, acc 0.890625\n",
      "2022-01-26T01:52:48.626602: step 996, loss 0.338338, acc 0.859375\n",
      "2022-01-26T01:52:48.791115: step 997, loss 0.279578, acc 0.921875\n",
      "2022-01-26T01:52:48.988847: step 998, loss 0.312245, acc 0.84375\n",
      "2022-01-26T01:52:49.155701: step 999, loss 0.271029, acc 0.875\n",
      "2022-01-26T01:52:49.314769: step 1000, loss 0.265237, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:52:49.960141: step 1000, loss 0.604072, acc 0.67875\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1000 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:52:50.122528 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1000 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1000\n",
      "\n",
      "2022-01-26T01:52:50.368376: step 1001, loss 0.358221, acc 0.84375\n",
      "2022-01-26T01:52:50.556813: step 1002, loss 0.384188, acc 0.8125\n",
      "2022-01-26T01:52:50.824190: step 1003, loss 0.439765, acc 0.8125\n",
      "2022-01-26T01:52:51.058797: step 1004, loss 0.313031, acc 0.84375\n",
      "2022-01-26T01:52:51.354042: step 1005, loss 0.32856, acc 0.828125\n",
      "2022-01-26T01:52:51.552468: step 1006, loss 0.418134, acc 0.828125\n",
      "2022-01-26T01:52:51.750498: step 1007, loss 0.157585, acc 0.953125\n",
      "2022-01-26T01:52:51.928891: step 1008, loss 0.335782, acc 0.875\n",
      "2022-01-26T01:52:52.127434: step 1009, loss 0.399829, acc 0.875\n",
      "2022-01-26T01:52:52.317623: step 1010, loss 0.316934, acc 0.90625\n",
      "2022-01-26T01:52:52.515464: step 1011, loss 0.278042, acc 0.9375\n",
      "2022-01-26T01:52:52.697929: step 1012, loss 0.298406, acc 0.84375\n",
      "2022-01-26T01:52:52.905933: step 1013, loss 0.243241, acc 0.921875\n",
      "2022-01-26T01:52:53.113043: step 1014, loss 0.331925, acc 0.90625\n",
      "2022-01-26T01:52:53.324631: step 1015, loss 0.292863, acc 0.90625\n",
      "2022-01-26T01:52:53.522354: step 1016, loss 0.379107, acc 0.8125\n",
      "2022-01-26T01:52:53.722980: step 1017, loss 0.324425, acc 0.90625\n",
      "2022-01-26T01:52:53.910172: step 1018, loss 0.271332, acc 0.890625\n",
      "2022-01-26T01:52:54.109227: step 1019, loss 0.370961, acc 0.90625\n",
      "2022-01-26T01:52:54.309980: step 1020, loss 0.255403, acc 0.890625\n",
      "2022-01-26T01:52:54.503903: step 1021, loss 0.462565, acc 0.828125\n",
      "2022-01-26T01:52:54.686376: step 1022, loss 0.326604, acc 0.84375\n",
      "2022-01-26T01:52:54.866004: step 1023, loss 0.356747, acc 0.859375\n",
      "2022-01-26T01:52:55.040438: step 1024, loss 0.477537, acc 0.796875\n",
      "2022-01-26T01:52:55.222591: step 1025, loss 0.326181, acc 0.875\n",
      "2022-01-26T01:52:55.400269: step 1026, loss 0.324467, acc 0.828125\n",
      "2022-01-26T01:52:55.589221: step 1027, loss 0.249485, acc 0.921875\n",
      "2022-01-26T01:52:55.769984: step 1028, loss 0.34737, acc 0.828125\n",
      "2022-01-26T01:52:55.975783: step 1029, loss 0.357332, acc 0.859375\n",
      "2022-01-26T01:52:56.183965: step 1030, loss 0.282212, acc 0.90625\n",
      "2022-01-26T01:52:56.362341: step 1031, loss 0.197234, acc 0.921875\n",
      "2022-01-26T01:52:56.609213: step 1032, loss 0.321673, acc 0.859375\n",
      "2022-01-26T01:52:56.802195: step 1033, loss 0.274454, acc 0.890625\n",
      "2022-01-26T01:52:57.013260: step 1034, loss 0.361594, acc 0.859375\n",
      "2022-01-26T01:52:57.316305: step 1035, loss 0.374656, acc 0.8125\n",
      "2022-01-26T01:52:57.517513: step 1036, loss 0.344799, acc 0.859375\n",
      "2022-01-26T01:52:57.701579: step 1037, loss 0.446875, acc 0.796875\n",
      "2022-01-26T01:52:57.901332: step 1038, loss 0.3613, acc 0.875\n",
      "2022-01-26T01:52:58.084505: step 1039, loss 0.381628, acc 0.828125\n",
      "2022-01-26T01:52:58.270431: step 1040, loss 0.301149, acc 0.859375\n",
      "2022-01-26T01:52:58.441368: step 1041, loss 0.133885, acc 0.984375\n",
      "2022-01-26T01:52:58.606919: step 1042, loss 0.278275, acc 0.875\n",
      "2022-01-26T01:52:58.785120: step 1043, loss 0.289059, acc 0.875\n",
      "2022-01-26T01:52:58.944571: step 1044, loss 0.377225, acc 0.875\n",
      "2022-01-26T01:52:59.107038: step 1045, loss 0.340607, acc 0.890625\n",
      "2022-01-26T01:52:59.291707: step 1046, loss 0.242436, acc 0.90625\n",
      "2022-01-26T01:52:59.474436: step 1047, loss 0.374332, acc 0.828125\n",
      "2022-01-26T01:52:59.646958: step 1048, loss 0.271911, acc 0.90625\n",
      "2022-01-26T01:52:59.804823: step 1049, loss 0.29285, acc 0.890625\n",
      "2022-01-26T01:52:59.965147: step 1050, loss 0.463196, acc 0.796875\n",
      "2022-01-26T01:53:00.124238: step 1051, loss 0.349253, acc 0.796875\n",
      "2022-01-26T01:53:00.280699: step 1052, loss 0.351732, acc 0.78125\n",
      "2022-01-26T01:53:00.439611: step 1053, loss 0.380151, acc 0.8125\n",
      "2022-01-26T01:53:00.594004: step 1054, loss 0.364648, acc 0.859375\n",
      "2022-01-26T01:53:00.768248: step 1055, loss 0.209328, acc 0.953125\n",
      "2022-01-26T01:53:00.995506: step 1056, loss 0.256616, acc 0.890625\n",
      "2022-01-26T01:53:01.181810: step 1057, loss 0.240618, acc 0.90625\n",
      "2022-01-26T01:53:01.372262: step 1058, loss 0.372413, acc 0.8125\n",
      "2022-01-26T01:53:01.595893: step 1059, loss 0.337931, acc 0.859375\n",
      "2022-01-26T01:53:01.782077: step 1060, loss 0.390473, acc 0.84375\n",
      "2022-01-26T01:53:01.948823: step 1061, loss 0.32721, acc 0.84375\n",
      "2022-01-26T01:53:02.113429: step 1062, loss 0.293252, acc 0.875\n",
      "2022-01-26T01:53:02.299241: step 1063, loss 0.364987, acc 0.84375\n",
      "2022-01-26T01:53:02.474797: step 1064, loss 0.308668, acc 0.828125\n",
      "2022-01-26T01:53:02.682122: step 1065, loss 0.278845, acc 0.84375\n",
      "2022-01-26T01:53:02.854715: step 1066, loss 0.32306, acc 0.875\n",
      "2022-01-26T01:53:03.047326: step 1067, loss 0.391717, acc 0.78125\n",
      "2022-01-26T01:53:03.217403: step 1068, loss 0.344165, acc 0.84375\n",
      "2022-01-26T01:53:03.392725: step 1069, loss 0.284507, acc 0.890625\n",
      "2022-01-26T01:53:03.592976: step 1070, loss 0.286516, acc 0.859375\n",
      "2022-01-26T01:53:03.861239: step 1071, loss 0.367872, acc 0.84375\n",
      "2022-01-26T01:53:04.134568: step 1072, loss 0.24011, acc 0.890625\n",
      "2022-01-26T01:53:04.385702: step 1073, loss 0.401393, acc 0.859375\n",
      "2022-01-26T01:53:04.567354: step 1074, loss 0.314435, acc 0.890625\n",
      "2022-01-26T01:53:04.737645: step 1075, loss 0.245356, acc 0.890625\n",
      "2022-01-26T01:53:04.917054: step 1076, loss 0.25773, acc 0.921875\n",
      "2022-01-26T01:53:05.098504: step 1077, loss 0.259954, acc 0.90625\n",
      "2022-01-26T01:53:05.288803: step 1078, loss 0.359934, acc 0.890625\n",
      "2022-01-26T01:53:05.491269: step 1079, loss 0.301145, acc 0.90625\n",
      "2022-01-26T01:53:05.738209: step 1080, loss 0.319472, acc 0.828125\n",
      "2022-01-26T01:53:05.988871: step 1081, loss 0.323677, acc 0.84375\n",
      "2022-01-26T01:53:06.205917: step 1082, loss 0.287221, acc 0.90625\n",
      "2022-01-26T01:53:06.398047: step 1083, loss 0.237495, acc 0.921875\n",
      "2022-01-26T01:53:06.585085: step 1084, loss 0.257319, acc 0.859375\n",
      "2022-01-26T01:53:06.759943: step 1085, loss 0.335932, acc 0.796875\n",
      "2022-01-26T01:53:06.930096: step 1086, loss 0.239776, acc 0.921875\n",
      "2022-01-26T01:53:07.104534: step 1087, loss 0.378507, acc 0.84375\n",
      "2022-01-26T01:53:07.269904: step 1088, loss 0.197904, acc 0.9375\n",
      "2022-01-26T01:53:07.437783: step 1089, loss 0.364516, acc 0.875\n",
      "2022-01-26T01:53:07.612057: step 1090, loss 0.424929, acc 0.828125\n",
      "2022-01-26T01:53:07.784840: step 1091, loss 0.236283, acc 0.890625\n",
      "2022-01-26T01:53:07.972394: step 1092, loss 0.289661, acc 0.890625\n",
      "2022-01-26T01:53:08.150372: step 1093, loss 0.295841, acc 0.875\n",
      "2022-01-26T01:53:08.339421: step 1094, loss 0.312633, acc 0.859375\n",
      "2022-01-26T01:53:08.500135: step 1095, loss 0.330215, acc 0.890625\n",
      "2022-01-26T01:53:08.667469: step 1096, loss 0.199482, acc 0.9375\n",
      "2022-01-26T01:53:08.832805: step 1097, loss 0.251025, acc 0.875\n",
      "2022-01-26T01:53:09.023135: step 1098, loss 0.395267, acc 0.828125\n",
      "2022-01-26T01:53:09.202705: step 1099, loss 0.22432, acc 0.875\n",
      "2022-01-26T01:53:09.375813: step 1100, loss 0.183898, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:53:10.000706: step 1100, loss 0.583421, acc 0.70875\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1100 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:53:10.156022 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1100 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1100\n",
      "\n",
      "2022-01-26T01:53:10.396615: step 1101, loss 0.277658, acc 0.890625\n",
      "2022-01-26T01:53:10.599420: step 1102, loss 0.363194, acc 0.828125\n",
      "2022-01-26T01:53:10.918086: step 1103, loss 0.311154, acc 0.875\n",
      "2022-01-26T01:53:11.396888: step 1104, loss 0.256094, acc 0.875\n",
      "2022-01-26T01:53:11.745745: step 1105, loss 0.270271, acc 0.859375\n",
      "2022-01-26T01:53:11.991250: step 1106, loss 0.358797, acc 0.84375\n",
      "2022-01-26T01:53:12.209517: step 1107, loss 0.375713, acc 0.84375\n",
      "2022-01-26T01:53:12.411761: step 1108, loss 0.267274, acc 0.875\n",
      "2022-01-26T01:53:12.608753: step 1109, loss 0.310293, acc 0.859375\n",
      "2022-01-26T01:53:12.800466: step 1110, loss 0.193296, acc 0.9375\n",
      "2022-01-26T01:53:13.022361: step 1111, loss 0.3287, acc 0.859375\n",
      "2022-01-26T01:53:13.242831: step 1112, loss 0.367996, acc 0.84375\n",
      "2022-01-26T01:53:13.449094: step 1113, loss 0.339847, acc 0.84375\n",
      "2022-01-26T01:53:13.634250: step 1114, loss 0.236162, acc 0.921875\n",
      "2022-01-26T01:53:13.832396: step 1115, loss 0.220734, acc 0.90625\n",
      "2022-01-26T01:53:14.028977: step 1116, loss 0.312166, acc 0.859375\n",
      "2022-01-26T01:53:14.225835: step 1117, loss 0.361661, acc 0.828125\n",
      "2022-01-26T01:53:14.411307: step 1118, loss 0.413868, acc 0.796875\n",
      "2022-01-26T01:53:14.608248: step 1119, loss 0.303374, acc 0.859375\n",
      "2022-01-26T01:53:14.791142: step 1120, loss 0.350541, acc 0.84375\n",
      "2022-01-26T01:53:15.009530: step 1121, loss 0.276919, acc 0.921875\n",
      "2022-01-26T01:53:15.203621: step 1122, loss 0.181875, acc 0.9375\n",
      "2022-01-26T01:53:15.404125: step 1123, loss 0.4437, acc 0.796875\n",
      "2022-01-26T01:53:15.579548: step 1124, loss 0.327208, acc 0.859375\n",
      "2022-01-26T01:53:15.775183: step 1125, loss 0.265187, acc 0.890625\n",
      "2022-01-26T01:53:15.950112: step 1126, loss 0.305056, acc 0.859375\n",
      "2022-01-26T01:53:16.143709: step 1127, loss 0.352882, acc 0.828125\n",
      "2022-01-26T01:53:16.330207: step 1128, loss 0.275546, acc 0.875\n",
      "2022-01-26T01:53:16.561868: step 1129, loss 0.444297, acc 0.828125\n",
      "2022-01-26T01:53:16.741140: step 1130, loss 0.408067, acc 0.84375\n",
      "2022-01-26T01:53:16.915290: step 1131, loss 0.294823, acc 0.890625\n",
      "2022-01-26T01:53:17.121609: step 1132, loss 0.212414, acc 0.9375\n",
      "2022-01-26T01:53:17.328573: step 1133, loss 0.231898, acc 0.9375\n",
      "2022-01-26T01:53:17.526379: step 1134, loss 0.383478, acc 0.828125\n",
      "2022-01-26T01:53:17.728228: step 1135, loss 0.365103, acc 0.8125\n",
      "2022-01-26T01:53:17.882504: step 1136, loss 0.252879, acc 0.894737\n",
      "2022-01-26T01:53:18.074312: step 1137, loss 0.214223, acc 0.9375\n",
      "2022-01-26T01:53:18.242608: step 1138, loss 0.237981, acc 0.921875\n",
      "2022-01-26T01:53:18.427388: step 1139, loss 0.283305, acc 0.90625\n",
      "2022-01-26T01:53:18.608105: step 1140, loss 0.335528, acc 0.828125\n",
      "2022-01-26T01:53:18.780218: step 1141, loss 0.222451, acc 0.921875\n",
      "2022-01-26T01:53:18.965446: step 1142, loss 0.299058, acc 0.859375\n",
      "2022-01-26T01:53:19.135615: step 1143, loss 0.28369, acc 0.828125\n",
      "2022-01-26T01:53:19.292924: step 1144, loss 0.284097, acc 0.859375\n",
      "2022-01-26T01:53:19.450848: step 1145, loss 0.205587, acc 0.90625\n",
      "2022-01-26T01:53:19.613790: step 1146, loss 0.257357, acc 0.90625\n",
      "2022-01-26T01:53:19.830481: step 1147, loss 0.188882, acc 0.9375\n",
      "2022-01-26T01:53:20.028611: step 1148, loss 0.295205, acc 0.828125\n",
      "2022-01-26T01:53:20.206509: step 1149, loss 0.38037, acc 0.84375\n",
      "2022-01-26T01:53:20.387186: step 1150, loss 0.357476, acc 0.84375\n",
      "2022-01-26T01:53:20.563503: step 1151, loss 0.196714, acc 0.921875\n",
      "2022-01-26T01:53:20.736022: step 1152, loss 0.329, acc 0.859375\n",
      "2022-01-26T01:53:20.914728: step 1153, loss 0.186035, acc 0.921875\n",
      "2022-01-26T01:53:21.087726: step 1154, loss 0.257999, acc 0.921875\n",
      "2022-01-26T01:53:21.254599: step 1155, loss 0.170403, acc 0.96875\n",
      "2022-01-26T01:53:21.425287: step 1156, loss 0.157834, acc 0.953125\n",
      "2022-01-26T01:53:21.600450: step 1157, loss 0.183277, acc 0.953125\n",
      "2022-01-26T01:53:21.774442: step 1158, loss 0.19559, acc 0.953125\n",
      "2022-01-26T01:53:21.966414: step 1159, loss 0.260907, acc 0.890625\n",
      "2022-01-26T01:53:22.140154: step 1160, loss 0.24735, acc 0.875\n",
      "2022-01-26T01:53:22.310164: step 1161, loss 0.294318, acc 0.9375\n",
      "2022-01-26T01:53:22.474782: step 1162, loss 0.261586, acc 0.890625\n",
      "2022-01-26T01:53:22.640709: step 1163, loss 0.230366, acc 0.90625\n",
      "2022-01-26T01:53:22.804697: step 1164, loss 0.319047, acc 0.890625\n",
      "2022-01-26T01:53:22.971854: step 1165, loss 0.281979, acc 0.84375\n",
      "2022-01-26T01:53:23.132818: step 1166, loss 0.286017, acc 0.859375\n",
      "2022-01-26T01:53:23.294801: step 1167, loss 0.210015, acc 0.90625\n",
      "2022-01-26T01:53:23.466251: step 1168, loss 0.264542, acc 0.9375\n",
      "2022-01-26T01:53:23.633944: step 1169, loss 0.16629, acc 0.953125\n",
      "2022-01-26T01:53:23.799311: step 1170, loss 0.303122, acc 0.859375\n",
      "2022-01-26T01:53:23.998477: step 1171, loss 0.244227, acc 0.875\n",
      "2022-01-26T01:53:24.178367: step 1172, loss 0.158838, acc 0.953125\n",
      "2022-01-26T01:53:24.360427: step 1173, loss 0.382657, acc 0.859375\n",
      "2022-01-26T01:53:24.532119: step 1174, loss 0.219291, acc 0.9375\n",
      "2022-01-26T01:53:24.708653: step 1175, loss 0.219966, acc 0.90625\n",
      "2022-01-26T01:53:24.876239: step 1176, loss 0.3429, acc 0.859375\n",
      "2022-01-26T01:53:25.050749: step 1177, loss 0.206416, acc 0.921875\n",
      "2022-01-26T01:53:25.224153: step 1178, loss 0.209552, acc 0.9375\n",
      "2022-01-26T01:53:25.391178: step 1179, loss 0.219148, acc 0.921875\n",
      "2022-01-26T01:53:25.566215: step 1180, loss 0.240481, acc 0.90625\n",
      "2022-01-26T01:53:25.741035: step 1181, loss 0.336955, acc 0.859375\n",
      "2022-01-26T01:53:25.916680: step 1182, loss 0.302416, acc 0.859375\n",
      "2022-01-26T01:53:26.099616: step 1183, loss 0.264616, acc 0.890625\n",
      "2022-01-26T01:53:26.274289: step 1184, loss 0.330907, acc 0.828125\n",
      "2022-01-26T01:53:26.452025: step 1185, loss 0.286847, acc 0.890625\n",
      "2022-01-26T01:53:26.610104: step 1186, loss 0.277145, acc 0.90625\n",
      "2022-01-26T01:53:26.770541: step 1187, loss 0.306042, acc 0.84375\n",
      "2022-01-26T01:53:26.930066: step 1188, loss 0.225026, acc 0.890625\n",
      "2022-01-26T01:53:27.097949: step 1189, loss 0.310282, acc 0.875\n",
      "2022-01-26T01:53:27.268623: step 1190, loss 0.319594, acc 0.890625\n",
      "2022-01-26T01:53:27.433678: step 1191, loss 0.359876, acc 0.8125\n",
      "2022-01-26T01:53:27.608208: step 1192, loss 0.265251, acc 0.875\n",
      "2022-01-26T01:53:27.774864: step 1193, loss 0.298722, acc 0.890625\n",
      "2022-01-26T01:53:27.943991: step 1194, loss 0.27831, acc 0.875\n",
      "2022-01-26T01:53:28.122094: step 1195, loss 0.159694, acc 0.9375\n",
      "2022-01-26T01:53:28.297630: step 1196, loss 0.334056, acc 0.890625\n",
      "2022-01-26T01:53:28.458385: step 1197, loss 0.220486, acc 0.921875\n",
      "2022-01-26T01:53:28.662193: step 1198, loss 0.212882, acc 0.921875\n",
      "2022-01-26T01:53:28.841386: step 1199, loss 0.193717, acc 0.90625\n",
      "2022-01-26T01:53:29.003978: step 1200, loss 0.245668, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:53:29.637439: step 1200, loss 0.567421, acc 0.734375\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1200 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:53:29.813119 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1200 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1200\n",
      "\n",
      "2022-01-26T01:53:30.047249: step 1201, loss 0.30743, acc 0.84375\n",
      "2022-01-26T01:53:30.226643: step 1202, loss 0.280354, acc 0.890625\n",
      "2022-01-26T01:53:30.560779: step 1203, loss 0.248397, acc 0.90625\n",
      "2022-01-26T01:53:30.857849: step 1204, loss 0.243436, acc 0.890625\n",
      "2022-01-26T01:53:31.096190: step 1205, loss 0.288231, acc 0.875\n",
      "2022-01-26T01:53:31.341616: step 1206, loss 0.247235, acc 0.875\n",
      "2022-01-26T01:53:31.555532: step 1207, loss 0.233431, acc 0.890625\n",
      "2022-01-26T01:53:31.787436: step 1208, loss 0.227725, acc 0.921875\n",
      "2022-01-26T01:53:32.011052: step 1209, loss 0.193327, acc 0.9375\n",
      "2022-01-26T01:53:32.219440: step 1210, loss 0.254359, acc 0.890625\n",
      "2022-01-26T01:53:32.451572: step 1211, loss 0.212221, acc 0.875\n",
      "2022-01-26T01:53:32.635095: step 1212, loss 0.332542, acc 0.828125\n",
      "2022-01-26T01:53:32.849450: step 1213, loss 0.413148, acc 0.796875\n",
      "2022-01-26T01:53:33.079145: step 1214, loss 0.192079, acc 0.921875\n",
      "2022-01-26T01:53:33.343106: step 1215, loss 0.238879, acc 0.9375\n",
      "2022-01-26T01:53:33.530182: step 1216, loss 0.177498, acc 0.921875\n",
      "2022-01-26T01:53:33.782381: step 1217, loss 0.318568, acc 0.875\n",
      "2022-01-26T01:53:33.971708: step 1218, loss 0.321104, acc 0.8125\n",
      "2022-01-26T01:53:34.153765: step 1219, loss 0.438752, acc 0.8125\n",
      "2022-01-26T01:53:34.332800: step 1220, loss 0.19381, acc 0.96875\n",
      "2022-01-26T01:53:34.528352: step 1221, loss 0.210408, acc 0.921875\n",
      "2022-01-26T01:53:34.763261: step 1222, loss 0.32037, acc 0.84375\n",
      "2022-01-26T01:53:34.949941: step 1223, loss 0.217223, acc 0.90625\n",
      "2022-01-26T01:53:35.136907: step 1224, loss 0.345751, acc 0.859375\n",
      "2022-01-26T01:53:35.322889: step 1225, loss 0.215271, acc 0.96875\n",
      "2022-01-26T01:53:35.548441: step 1226, loss 0.178055, acc 0.953125\n",
      "2022-01-26T01:53:35.758108: step 1227, loss 0.163907, acc 0.9375\n",
      "2022-01-26T01:53:35.945954: step 1228, loss 0.354713, acc 0.84375\n",
      "2022-01-26T01:53:36.151431: step 1229, loss 0.195874, acc 0.953125\n",
      "2022-01-26T01:53:36.330097: step 1230, loss 0.316865, acc 0.828125\n",
      "2022-01-26T01:53:36.528024: step 1231, loss 0.340663, acc 0.84375\n",
      "2022-01-26T01:53:36.733871: step 1232, loss 0.110579, acc 0.984375\n",
      "2022-01-26T01:53:36.921873: step 1233, loss 0.245566, acc 0.875\n",
      "2022-01-26T01:53:37.106874: step 1234, loss 0.209416, acc 0.9375\n",
      "2022-01-26T01:53:37.285980: step 1235, loss 0.223515, acc 0.921875\n",
      "2022-01-26T01:53:37.453197: step 1236, loss 0.305284, acc 0.875\n",
      "2022-01-26T01:53:37.623671: step 1237, loss 0.304769, acc 0.890625\n",
      "2022-01-26T01:53:37.780043: step 1238, loss 0.279104, acc 0.828125\n",
      "2022-01-26T01:53:37.940026: step 1239, loss 0.222226, acc 0.953125\n",
      "2022-01-26T01:53:38.102040: step 1240, loss 0.247505, acc 0.90625\n",
      "2022-01-26T01:53:38.261030: step 1241, loss 0.166878, acc 0.9375\n",
      "2022-01-26T01:53:38.415480: step 1242, loss 0.23326, acc 0.890625\n",
      "2022-01-26T01:53:38.573797: step 1243, loss 0.343568, acc 0.859375\n",
      "2022-01-26T01:53:38.733041: step 1244, loss 0.316013, acc 0.875\n",
      "2022-01-26T01:53:38.891905: step 1245, loss 0.146323, acc 0.96875\n",
      "2022-01-26T01:53:39.047225: step 1246, loss 0.27769, acc 0.828125\n",
      "2022-01-26T01:53:39.210874: step 1247, loss 0.299275, acc 0.890625\n",
      "2022-01-26T01:53:39.369957: step 1248, loss 0.238553, acc 0.90625\n",
      "2022-01-26T01:53:39.532914: step 1249, loss 0.244946, acc 0.890625\n",
      "2022-01-26T01:53:39.698365: step 1250, loss 0.272348, acc 0.890625\n",
      "2022-01-26T01:53:39.874281: step 1251, loss 0.286944, acc 0.921875\n",
      "2022-01-26T01:53:40.035169: step 1252, loss 0.308578, acc 0.84375\n",
      "2022-01-26T01:53:40.197920: step 1253, loss 0.238033, acc 0.90625\n",
      "2022-01-26T01:53:40.352171: step 1254, loss 0.185612, acc 0.921875\n",
      "2022-01-26T01:53:40.517183: step 1255, loss 0.29317, acc 0.890625\n",
      "2022-01-26T01:53:40.683371: step 1256, loss 0.204335, acc 0.9375\n",
      "2022-01-26T01:53:40.901407: step 1257, loss 0.268822, acc 0.921875\n",
      "2022-01-26T01:53:41.078728: step 1258, loss 0.286681, acc 0.859375\n",
      "2022-01-26T01:53:41.269312: step 1259, loss 0.308236, acc 0.84375\n",
      "2022-01-26T01:53:41.452032: step 1260, loss 0.43611, acc 0.84375\n",
      "2022-01-26T01:53:41.675557: step 1261, loss 0.217708, acc 0.90625\n",
      "2022-01-26T01:53:41.840713: step 1262, loss 0.261573, acc 0.9375\n",
      "2022-01-26T01:53:42.007134: step 1263, loss 0.258909, acc 0.875\n",
      "2022-01-26T01:53:42.170229: step 1264, loss 0.242407, acc 0.90625\n",
      "2022-01-26T01:53:42.337005: step 1265, loss 0.231549, acc 0.921875\n",
      "2022-01-26T01:53:42.504392: step 1266, loss 0.318647, acc 0.875\n",
      "2022-01-26T01:53:42.685688: step 1267, loss 0.234804, acc 0.921875\n",
      "2022-01-26T01:53:42.842624: step 1268, loss 0.326392, acc 0.84375\n",
      "2022-01-26T01:53:43.022061: step 1269, loss 0.233218, acc 0.890625\n",
      "2022-01-26T01:53:43.189062: step 1270, loss 0.254826, acc 0.921875\n",
      "2022-01-26T01:53:43.348982: step 1271, loss 0.257754, acc 0.859375\n",
      "2022-01-26T01:53:43.512167: step 1272, loss 0.33714, acc 0.8125\n",
      "2022-01-26T01:53:43.687363: step 1273, loss 0.184551, acc 0.921875\n",
      "2022-01-26T01:53:43.857080: step 1274, loss 0.208603, acc 0.9375\n",
      "2022-01-26T01:53:44.033674: step 1275, loss 0.27741, acc 0.890625\n",
      "2022-01-26T01:53:44.204191: step 1276, loss 0.248215, acc 0.90625\n",
      "2022-01-26T01:53:44.379243: step 1277, loss 0.287251, acc 0.875\n",
      "2022-01-26T01:53:44.537595: step 1278, loss 0.359869, acc 0.842105\n",
      "2022-01-26T01:53:44.719333: step 1279, loss 0.230366, acc 0.890625\n",
      "2022-01-26T01:53:44.882595: step 1280, loss 0.278151, acc 0.875\n",
      "2022-01-26T01:53:45.051357: step 1281, loss 0.198427, acc 0.921875\n",
      "2022-01-26T01:53:45.223727: step 1282, loss 0.225919, acc 0.921875\n",
      "2022-01-26T01:53:45.394466: step 1283, loss 0.138394, acc 0.953125\n",
      "2022-01-26T01:53:45.557019: step 1284, loss 0.162618, acc 0.9375\n",
      "2022-01-26T01:53:45.721710: step 1285, loss 0.178802, acc 0.9375\n",
      "2022-01-26T01:53:45.883364: step 1286, loss 0.199406, acc 0.90625\n",
      "2022-01-26T01:53:46.061338: step 1287, loss 0.158771, acc 0.9375\n",
      "2022-01-26T01:53:46.232788: step 1288, loss 0.241503, acc 0.90625\n",
      "2022-01-26T01:53:46.421359: step 1289, loss 0.192827, acc 0.9375\n",
      "2022-01-26T01:53:46.592301: step 1290, loss 0.200592, acc 0.953125\n",
      "2022-01-26T01:53:46.786597: step 1291, loss 0.162763, acc 0.9375\n",
      "2022-01-26T01:53:46.955288: step 1292, loss 0.197086, acc 0.890625\n",
      "2022-01-26T01:53:47.133499: step 1293, loss 0.142683, acc 0.953125\n",
      "2022-01-26T01:53:47.312391: step 1294, loss 0.132636, acc 0.953125\n",
      "2022-01-26T01:53:47.504825: step 1295, loss 0.130905, acc 0.96875\n",
      "2022-01-26T01:53:47.689665: step 1296, loss 0.200555, acc 0.9375\n",
      "2022-01-26T01:53:47.877717: step 1297, loss 0.165206, acc 0.953125\n",
      "2022-01-26T01:53:48.058693: step 1298, loss 0.211596, acc 0.90625\n",
      "2022-01-26T01:53:48.227277: step 1299, loss 0.148644, acc 0.984375\n",
      "2022-01-26T01:53:48.388799: step 1300, loss 0.354645, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:53:49.015243: step 1300, loss 0.593531, acc 0.7225\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1300 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:53:49.177882 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1300 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1300\n",
      "\n",
      "2022-01-26T01:53:49.417611: step 1301, loss 0.172413, acc 0.9375\n",
      "2022-01-26T01:53:49.603170: step 1302, loss 0.240955, acc 0.859375\n",
      "2022-01-26T01:53:49.852409: step 1303, loss 0.165877, acc 0.9375\n",
      "2022-01-26T01:53:50.096811: step 1304, loss 0.213554, acc 0.921875\n",
      "2022-01-26T01:53:50.337756: step 1305, loss 0.226761, acc 0.890625\n",
      "2022-01-26T01:53:50.551116: step 1306, loss 0.27702, acc 0.890625\n",
      "2022-01-26T01:53:50.748257: step 1307, loss 0.278213, acc 0.859375\n",
      "2022-01-26T01:53:50.941626: step 1308, loss 0.337267, acc 0.875\n",
      "2022-01-26T01:53:51.150017: step 1309, loss 0.18477, acc 0.9375\n",
      "2022-01-26T01:53:51.348123: step 1310, loss 0.212997, acc 0.90625\n",
      "2022-01-26T01:53:51.553729: step 1311, loss 0.222097, acc 0.90625\n",
      "2022-01-26T01:53:51.741275: step 1312, loss 0.240559, acc 0.890625\n",
      "2022-01-26T01:53:51.949108: step 1313, loss 0.231228, acc 0.9375\n",
      "2022-01-26T01:53:52.156015: step 1314, loss 0.25452, acc 0.9375\n",
      "2022-01-26T01:53:52.398937: step 1315, loss 0.204014, acc 0.921875\n",
      "2022-01-26T01:53:52.586313: step 1316, loss 0.167101, acc 0.9375\n",
      "2022-01-26T01:53:52.793940: step 1317, loss 0.190835, acc 0.9375\n",
      "2022-01-26T01:53:52.980729: step 1318, loss 0.177454, acc 0.9375\n",
      "2022-01-26T01:53:53.183467: step 1319, loss 0.259392, acc 0.90625\n",
      "2022-01-26T01:53:53.366551: step 1320, loss 0.276848, acc 0.875\n",
      "2022-01-26T01:53:53.576679: step 1321, loss 0.258509, acc 0.921875\n",
      "2022-01-26T01:53:53.785655: step 1322, loss 0.165117, acc 0.90625\n",
      "2022-01-26T01:53:54.012918: step 1323, loss 0.20637, acc 0.9375\n",
      "2022-01-26T01:53:54.382783: step 1324, loss 0.192739, acc 0.921875\n",
      "2022-01-26T01:53:54.674024: step 1325, loss 0.224173, acc 0.921875\n",
      "2022-01-26T01:53:54.904820: step 1326, loss 0.15055, acc 0.921875\n",
      "2022-01-26T01:53:55.143965: step 1327, loss 0.240373, acc 0.890625\n",
      "2022-01-26T01:53:55.326407: step 1328, loss 0.106553, acc 0.96875\n",
      "2022-01-26T01:53:55.568394: step 1329, loss 0.183096, acc 0.953125\n",
      "2022-01-26T01:53:55.826491: step 1330, loss 0.145955, acc 0.953125\n",
      "2022-01-26T01:53:56.030780: step 1331, loss 0.174707, acc 0.9375\n",
      "2022-01-26T01:53:56.242847: step 1332, loss 0.185248, acc 0.9375\n",
      "2022-01-26T01:53:56.459902: step 1333, loss 0.34628, acc 0.8125\n",
      "2022-01-26T01:53:56.688979: step 1334, loss 0.131136, acc 0.953125\n",
      "2022-01-26T01:53:56.885847: step 1335, loss 0.196206, acc 0.859375\n",
      "2022-01-26T01:53:57.084699: step 1336, loss 0.136465, acc 0.953125\n",
      "2022-01-26T01:53:57.313458: step 1337, loss 0.154909, acc 0.953125\n",
      "2022-01-26T01:53:57.543400: step 1338, loss 0.219875, acc 0.921875\n",
      "2022-01-26T01:53:57.799704: step 1339, loss 0.316697, acc 0.875\n",
      "2022-01-26T01:53:58.233171: step 1340, loss 0.268979, acc 0.875\n",
      "2022-01-26T01:53:58.479021: step 1341, loss 0.194962, acc 0.890625\n",
      "2022-01-26T01:53:58.682862: step 1342, loss 0.159671, acc 0.96875\n",
      "2022-01-26T01:53:58.932054: step 1343, loss 0.180709, acc 0.921875\n",
      "2022-01-26T01:53:59.139719: step 1344, loss 0.14911, acc 0.96875\n",
      "2022-01-26T01:53:59.329976: step 1345, loss 0.246272, acc 0.9375\n",
      "2022-01-26T01:53:59.518525: step 1346, loss 0.152828, acc 1\n",
      "2022-01-26T01:53:59.709202: step 1347, loss 0.128117, acc 0.96875\n",
      "2022-01-26T01:53:59.924290: step 1348, loss 0.330906, acc 0.828125\n",
      "2022-01-26T01:54:00.111926: step 1349, loss 0.242095, acc 0.921875\n",
      "2022-01-26T01:54:00.320819: step 1350, loss 0.176918, acc 0.9375\n",
      "2022-01-26T01:54:00.506239: step 1351, loss 0.186031, acc 0.9375\n",
      "2022-01-26T01:54:00.723695: step 1352, loss 0.221582, acc 0.90625\n",
      "2022-01-26T01:54:00.919807: step 1353, loss 0.28188, acc 0.890625\n",
      "2022-01-26T01:54:01.134863: step 1354, loss 0.11018, acc 0.984375\n",
      "2022-01-26T01:54:01.369755: step 1355, loss 0.190298, acc 0.96875\n",
      "2022-01-26T01:54:01.564132: step 1356, loss 0.259467, acc 0.875\n",
      "2022-01-26T01:54:01.737319: step 1357, loss 0.186211, acc 0.953125\n",
      "2022-01-26T01:54:01.912307: step 1358, loss 0.26822, acc 0.90625\n",
      "2022-01-26T01:54:02.105768: step 1359, loss 0.262855, acc 0.890625\n",
      "2022-01-26T01:54:02.285368: step 1360, loss 0.303303, acc 0.875\n",
      "2022-01-26T01:54:02.476500: step 1361, loss 0.272175, acc 0.921875\n",
      "2022-01-26T01:54:02.669268: step 1362, loss 0.268013, acc 0.890625\n",
      "2022-01-26T01:54:02.862048: step 1363, loss 0.249938, acc 0.890625\n",
      "2022-01-26T01:54:03.038517: step 1364, loss 0.197601, acc 0.9375\n",
      "2022-01-26T01:54:03.224686: step 1365, loss 0.126195, acc 0.9375\n",
      "2022-01-26T01:54:03.388447: step 1366, loss 0.247499, acc 0.953125\n",
      "2022-01-26T01:54:03.567744: step 1367, loss 0.18231, acc 0.890625\n",
      "2022-01-26T01:54:03.736697: step 1368, loss 0.102364, acc 0.984375\n",
      "2022-01-26T01:54:03.922472: step 1369, loss 0.204617, acc 0.921875\n",
      "2022-01-26T01:54:04.110608: step 1370, loss 0.184747, acc 0.921875\n",
      "2022-01-26T01:54:04.387173: step 1371, loss 0.195421, acc 0.9375\n",
      "2022-01-26T01:54:04.565655: step 1372, loss 0.108009, acc 0.96875\n",
      "2022-01-26T01:54:04.746723: step 1373, loss 0.3362, acc 0.859375\n",
      "2022-01-26T01:54:04.926954: step 1374, loss 0.287691, acc 0.875\n",
      "2022-01-26T01:54:05.105308: step 1375, loss 0.183351, acc 0.9375\n",
      "2022-01-26T01:54:05.266379: step 1376, loss 0.113879, acc 0.96875\n",
      "2022-01-26T01:54:05.430273: step 1377, loss 0.401766, acc 0.796875\n",
      "2022-01-26T01:54:05.600619: step 1378, loss 0.154279, acc 0.9375\n",
      "2022-01-26T01:54:05.773851: step 1379, loss 0.180142, acc 0.9375\n",
      "2022-01-26T01:54:05.942889: step 1380, loss 0.210134, acc 0.9375\n",
      "2022-01-26T01:54:06.111278: step 1381, loss 0.242675, acc 0.90625\n",
      "2022-01-26T01:54:06.278977: step 1382, loss 0.28743, acc 0.875\n",
      "2022-01-26T01:54:06.472991: step 1383, loss 0.220899, acc 0.890625\n",
      "2022-01-26T01:54:06.640714: step 1384, loss 0.173133, acc 0.9375\n",
      "2022-01-26T01:54:06.831722: step 1385, loss 0.195774, acc 0.9375\n",
      "2022-01-26T01:54:06.991134: step 1386, loss 0.158646, acc 0.953125\n",
      "2022-01-26T01:54:07.159184: step 1387, loss 0.135556, acc 0.953125\n",
      "2022-01-26T01:54:07.366203: step 1388, loss 0.167659, acc 0.9375\n",
      "2022-01-26T01:54:07.562080: step 1389, loss 0.201328, acc 0.90625\n",
      "2022-01-26T01:54:07.754986: step 1390, loss 0.164472, acc 0.90625\n",
      "2022-01-26T01:54:07.925095: step 1391, loss 0.157252, acc 0.9375\n",
      "2022-01-26T01:54:08.131762: step 1392, loss 0.358527, acc 0.84375\n",
      "2022-01-26T01:54:08.334492: step 1393, loss 0.28161, acc 0.859375\n",
      "2022-01-26T01:54:08.526606: step 1394, loss 0.288111, acc 0.890625\n",
      "2022-01-26T01:54:08.709923: step 1395, loss 0.261514, acc 0.890625\n",
      "2022-01-26T01:54:08.877564: step 1396, loss 0.175636, acc 0.9375\n",
      "2022-01-26T01:54:09.061282: step 1397, loss 0.252889, acc 0.859375\n",
      "2022-01-26T01:54:09.279297: step 1398, loss 0.118416, acc 0.96875\n",
      "2022-01-26T01:54:09.464009: step 1399, loss 0.254811, acc 0.890625\n",
      "2022-01-26T01:54:09.636280: step 1400, loss 0.216168, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:54:10.289048: step 1400, loss 0.590071, acc 0.7325\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1400 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:54:10.465785 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1400 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1400\n",
      "\n",
      "2022-01-26T01:54:10.789113: step 1401, loss 0.158376, acc 0.921875\n",
      "2022-01-26T01:54:10.991183: step 1402, loss 0.153815, acc 0.9375\n",
      "2022-01-26T01:54:11.180213: step 1403, loss 0.233417, acc 0.90625\n",
      "2022-01-26T01:54:11.416304: step 1404, loss 0.162901, acc 0.9375\n",
      "2022-01-26T01:54:11.987620: step 1405, loss 0.220179, acc 0.90625\n",
      "2022-01-26T01:54:12.476906: step 1406, loss 0.233977, acc 0.890625\n",
      "2022-01-26T01:54:12.742661: step 1407, loss 0.152249, acc 0.953125\n",
      "2022-01-26T01:54:12.982149: step 1408, loss 0.12902, acc 0.953125\n",
      "2022-01-26T01:54:13.269446: step 1409, loss 0.181523, acc 0.921875\n",
      "2022-01-26T01:54:13.528185: step 1410, loss 0.233228, acc 0.859375\n",
      "2022-01-26T01:54:13.731722: step 1411, loss 0.113919, acc 0.984375\n",
      "2022-01-26T01:54:13.930373: step 1412, loss 0.176334, acc 0.921875\n",
      "2022-01-26T01:54:14.128284: step 1413, loss 0.213901, acc 0.921875\n",
      "2022-01-26T01:54:14.364229: step 1414, loss 0.1613, acc 0.953125\n",
      "2022-01-26T01:54:14.558136: step 1415, loss 0.160637, acc 0.9375\n",
      "2022-01-26T01:54:14.760102: step 1416, loss 0.25404, acc 0.859375\n",
      "2022-01-26T01:54:14.940127: step 1417, loss 0.242801, acc 0.890625\n",
      "2022-01-26T01:54:15.133740: step 1418, loss 0.210961, acc 0.90625\n",
      "2022-01-26T01:54:15.359596: step 1419, loss 0.201397, acc 0.890625\n",
      "2022-01-26T01:54:15.535328: step 1420, loss 0.170302, acc 0.894737\n",
      "2022-01-26T01:54:15.836948: step 1421, loss 0.0747191, acc 0.96875\n",
      "2022-01-26T01:54:16.139143: step 1422, loss 0.131507, acc 0.953125\n",
      "2022-01-26T01:54:16.419786: step 1423, loss 0.148501, acc 0.953125\n",
      "2022-01-26T01:54:16.712168: step 1424, loss 0.130103, acc 0.953125\n",
      "2022-01-26T01:54:16.954682: step 1425, loss 0.11303, acc 0.953125\n",
      "2022-01-26T01:54:17.158224: step 1426, loss 0.147518, acc 0.953125\n",
      "2022-01-26T01:54:17.372949: step 1427, loss 0.110406, acc 0.984375\n",
      "2022-01-26T01:54:17.581618: step 1428, loss 0.210888, acc 0.875\n",
      "2022-01-26T01:54:17.781674: step 1429, loss 0.238269, acc 0.90625\n",
      "2022-01-26T01:54:17.956477: step 1430, loss 0.205923, acc 0.921875\n",
      "2022-01-26T01:54:18.130966: step 1431, loss 0.212999, acc 0.9375\n",
      "2022-01-26T01:54:18.349802: step 1432, loss 0.232273, acc 0.890625\n",
      "2022-01-26T01:54:18.529724: step 1433, loss 0.125766, acc 0.96875\n",
      "2022-01-26T01:54:18.711068: step 1434, loss 0.135239, acc 0.9375\n",
      "2022-01-26T01:54:18.885807: step 1435, loss 0.19914, acc 0.9375\n",
      "2022-01-26T01:54:19.084592: step 1436, loss 0.115592, acc 0.953125\n",
      "2022-01-26T01:54:19.256750: step 1437, loss 0.324337, acc 0.875\n",
      "2022-01-26T01:54:19.429770: step 1438, loss 0.212609, acc 0.90625\n",
      "2022-01-26T01:54:19.594201: step 1439, loss 0.154082, acc 0.9375\n",
      "2022-01-26T01:54:19.756510: step 1440, loss 0.182863, acc 0.90625\n",
      "2022-01-26T01:54:19.916176: step 1441, loss 0.109412, acc 0.96875\n",
      "2022-01-26T01:54:20.075787: step 1442, loss 0.0765215, acc 0.984375\n",
      "2022-01-26T01:54:20.234047: step 1443, loss 0.207781, acc 0.921875\n",
      "2022-01-26T01:54:20.405798: step 1444, loss 0.200808, acc 0.890625\n",
      "2022-01-26T01:54:20.582635: step 1445, loss 0.185055, acc 0.921875\n",
      "2022-01-26T01:54:20.751649: step 1446, loss 0.164699, acc 0.921875\n",
      "2022-01-26T01:54:20.905592: step 1447, loss 0.154483, acc 0.9375\n",
      "2022-01-26T01:54:21.060948: step 1448, loss 0.130704, acc 0.9375\n",
      "2022-01-26T01:54:21.220948: step 1449, loss 0.273841, acc 0.890625\n",
      "2022-01-26T01:54:21.388238: step 1450, loss 0.213224, acc 0.875\n",
      "2022-01-26T01:54:21.557939: step 1451, loss 0.161672, acc 0.921875\n",
      "2022-01-26T01:54:21.717234: step 1452, loss 0.19967, acc 0.9375\n",
      "2022-01-26T01:54:21.876231: step 1453, loss 0.12103, acc 0.96875\n",
      "2022-01-26T01:54:22.034594: step 1454, loss 0.183123, acc 0.921875\n",
      "2022-01-26T01:54:22.189597: step 1455, loss 0.149913, acc 0.9375\n",
      "2022-01-26T01:54:22.353216: step 1456, loss 0.108987, acc 0.953125\n",
      "2022-01-26T01:54:22.529356: step 1457, loss 0.0998761, acc 0.953125\n",
      "2022-01-26T01:54:22.695421: step 1458, loss 0.188622, acc 0.921875\n",
      "2022-01-26T01:54:22.864509: step 1459, loss 0.154694, acc 0.921875\n",
      "2022-01-26T01:54:23.023614: step 1460, loss 0.232288, acc 0.921875\n",
      "2022-01-26T01:54:23.179907: step 1461, loss 0.103994, acc 0.984375\n",
      "2022-01-26T01:54:23.336105: step 1462, loss 0.1547, acc 0.90625\n",
      "2022-01-26T01:54:23.491537: step 1463, loss 0.166577, acc 0.921875\n",
      "2022-01-26T01:54:23.647977: step 1464, loss 0.115176, acc 0.953125\n",
      "2022-01-26T01:54:23.800101: step 1465, loss 0.226823, acc 0.859375\n",
      "2022-01-26T01:54:23.959624: step 1466, loss 0.133646, acc 0.953125\n",
      "2022-01-26T01:54:24.113852: step 1467, loss 0.105989, acc 0.984375\n",
      "2022-01-26T01:54:24.292700: step 1468, loss 0.22711, acc 0.90625\n",
      "2022-01-26T01:54:24.455991: step 1469, loss 0.164106, acc 0.921875\n",
      "2022-01-26T01:54:24.621284: step 1470, loss 0.128495, acc 0.953125\n",
      "2022-01-26T01:54:24.781772: step 1471, loss 0.0736251, acc 0.96875\n",
      "2022-01-26T01:54:24.939519: step 1472, loss 0.217357, acc 0.890625\n",
      "2022-01-26T01:54:25.091284: step 1473, loss 0.229161, acc 0.90625\n",
      "2022-01-26T01:54:25.248638: step 1474, loss 0.204149, acc 0.890625\n",
      "2022-01-26T01:54:25.400813: step 1475, loss 0.137529, acc 0.96875\n",
      "2022-01-26T01:54:25.563187: step 1476, loss 0.138892, acc 0.953125\n",
      "2022-01-26T01:54:25.716613: step 1477, loss 0.197834, acc 0.90625\n",
      "2022-01-26T01:54:25.874541: step 1478, loss 0.22239, acc 0.90625\n",
      "2022-01-26T01:54:26.032668: step 1479, loss 0.179657, acc 0.9375\n",
      "2022-01-26T01:54:26.189624: step 1480, loss 0.0867421, acc 0.96875\n",
      "2022-01-26T01:54:26.355575: step 1481, loss 0.159736, acc 0.953125\n",
      "2022-01-26T01:54:26.516259: step 1482, loss 0.0998227, acc 0.96875\n",
      "2022-01-26T01:54:26.674483: step 1483, loss 0.261781, acc 0.921875\n",
      "2022-01-26T01:54:26.837172: step 1484, loss 0.147911, acc 0.9375\n",
      "2022-01-26T01:54:26.990508: step 1485, loss 0.143751, acc 0.921875\n",
      "2022-01-26T01:54:27.156319: step 1486, loss 0.222511, acc 0.90625\n",
      "2022-01-26T01:54:27.329337: step 1487, loss 0.182566, acc 0.90625\n",
      "2022-01-26T01:54:27.488300: step 1488, loss 0.107922, acc 0.9375\n",
      "2022-01-26T01:54:27.648396: step 1489, loss 0.178246, acc 0.9375\n",
      "2022-01-26T01:54:27.816706: step 1490, loss 0.0991087, acc 0.953125\n",
      "2022-01-26T01:54:27.979407: step 1491, loss 0.25892, acc 0.90625\n",
      "2022-01-26T01:54:28.140485: step 1492, loss 0.195111, acc 0.9375\n",
      "2022-01-26T01:54:28.300822: step 1493, loss 0.1619, acc 0.90625\n",
      "2022-01-26T01:54:28.467112: step 1494, loss 0.317296, acc 0.859375\n",
      "2022-01-26T01:54:28.625855: step 1495, loss 0.206051, acc 0.921875\n",
      "2022-01-26T01:54:28.786342: step 1496, loss 0.157587, acc 0.90625\n",
      "2022-01-26T01:54:28.988219: step 1497, loss 0.111837, acc 0.96875\n",
      "2022-01-26T01:54:29.167015: step 1498, loss 0.216766, acc 0.921875\n",
      "2022-01-26T01:54:29.334199: step 1499, loss 0.162385, acc 0.921875\n",
      "2022-01-26T01:54:29.496492: step 1500, loss 0.131145, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:54:30.111825: step 1500, loss 0.614346, acc 0.725625\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1500 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:54:30.269392 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1500 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1500\n",
      "\n",
      "2022-01-26T01:54:30.499220: step 1501, loss 0.0964149, acc 0.984375\n",
      "2022-01-26T01:54:30.684053: step 1502, loss 0.185152, acc 0.9375\n",
      "2022-01-26T01:54:30.928709: step 1503, loss 0.0930832, acc 0.96875\n",
      "2022-01-26T01:54:31.156910: step 1504, loss 0.203251, acc 0.875\n",
      "2022-01-26T01:54:31.420455: step 1505, loss 0.199745, acc 0.890625\n",
      "2022-01-26T01:54:31.647648: step 1506, loss 0.184131, acc 0.90625\n",
      "2022-01-26T01:54:31.837496: step 1507, loss 0.108505, acc 0.96875\n",
      "2022-01-26T01:54:32.034315: step 1508, loss 0.0894673, acc 0.96875\n",
      "2022-01-26T01:54:32.224865: step 1509, loss 0.147825, acc 0.921875\n",
      "2022-01-26T01:54:32.421731: step 1510, loss 0.128307, acc 0.953125\n",
      "2022-01-26T01:54:32.592251: step 1511, loss 0.245628, acc 0.859375\n",
      "2022-01-26T01:54:32.772791: step 1512, loss 0.213273, acc 0.875\n",
      "2022-01-26T01:54:32.963810: step 1513, loss 0.172292, acc 0.9375\n",
      "2022-01-26T01:54:33.201936: step 1514, loss 0.117696, acc 0.953125\n",
      "2022-01-26T01:54:33.392443: step 1515, loss 0.15961, acc 0.9375\n",
      "2022-01-26T01:54:33.608003: step 1516, loss 0.159331, acc 0.9375\n",
      "2022-01-26T01:54:33.839994: step 1517, loss 0.170738, acc 0.9375\n",
      "2022-01-26T01:54:34.037921: step 1518, loss 0.183284, acc 0.953125\n",
      "2022-01-26T01:54:34.239642: step 1519, loss 0.157004, acc 0.921875\n",
      "2022-01-26T01:54:34.412521: step 1520, loss 0.180539, acc 0.953125\n",
      "2022-01-26T01:54:34.590590: step 1521, loss 0.116044, acc 0.96875\n",
      "2022-01-26T01:54:34.758239: step 1522, loss 0.226536, acc 0.90625\n",
      "2022-01-26T01:54:34.932492: step 1523, loss 0.0535654, acc 1\n",
      "2022-01-26T01:54:35.098271: step 1524, loss 0.0816923, acc 0.96875\n",
      "2022-01-26T01:54:35.289187: step 1525, loss 0.219331, acc 0.90625\n",
      "2022-01-26T01:54:35.485895: step 1526, loss 0.254802, acc 0.875\n",
      "2022-01-26T01:54:35.670558: step 1527, loss 0.108115, acc 0.9375\n",
      "2022-01-26T01:54:35.847619: step 1528, loss 0.321772, acc 0.859375\n",
      "2022-01-26T01:54:36.022711: step 1529, loss 0.125953, acc 0.953125\n",
      "2022-01-26T01:54:36.192905: step 1530, loss 0.205605, acc 0.921875\n",
      "2022-01-26T01:54:36.366924: step 1531, loss 0.163313, acc 0.9375\n",
      "2022-01-26T01:54:36.530463: step 1532, loss 0.154332, acc 0.953125\n",
      "2022-01-26T01:54:36.718715: step 1533, loss 0.161668, acc 0.9375\n",
      "2022-01-26T01:54:36.896258: step 1534, loss 0.110109, acc 0.9375\n",
      "2022-01-26T01:54:37.087002: step 1535, loss 0.187395, acc 0.890625\n",
      "2022-01-26T01:54:37.287196: step 1536, loss 0.243252, acc 0.90625\n",
      "2022-01-26T01:54:37.453495: step 1537, loss 0.296924, acc 0.890625\n",
      "2022-01-26T01:54:37.613226: step 1538, loss 0.220982, acc 0.875\n",
      "2022-01-26T01:54:37.774814: step 1539, loss 0.142689, acc 0.953125\n",
      "2022-01-26T01:54:37.936487: step 1540, loss 0.118519, acc 0.9375\n",
      "2022-01-26T01:54:38.110543: step 1541, loss 0.134633, acc 0.953125\n",
      "2022-01-26T01:54:38.277110: step 1542, loss 0.126253, acc 0.984375\n",
      "2022-01-26T01:54:38.449746: step 1543, loss 0.149085, acc 0.953125\n",
      "2022-01-26T01:54:38.623883: step 1544, loss 0.185544, acc 0.90625\n",
      "2022-01-26T01:54:38.801729: step 1545, loss 0.286677, acc 0.875\n",
      "2022-01-26T01:54:38.960865: step 1546, loss 0.247352, acc 0.921875\n",
      "2022-01-26T01:54:39.124145: step 1547, loss 0.248866, acc 0.890625\n",
      "2022-01-26T01:54:39.286265: step 1548, loss 0.227055, acc 0.921875\n",
      "2022-01-26T01:54:39.454975: step 1549, loss 0.142847, acc 0.9375\n",
      "2022-01-26T01:54:39.617989: step 1550, loss 0.123052, acc 0.953125\n",
      "2022-01-26T01:54:39.782632: step 1551, loss 0.129469, acc 0.953125\n",
      "2022-01-26T01:54:39.940531: step 1552, loss 0.10271, acc 0.953125\n",
      "2022-01-26T01:54:40.100080: step 1553, loss 0.199157, acc 0.90625\n",
      "2022-01-26T01:54:40.257798: step 1554, loss 0.193117, acc 0.921875\n",
      "2022-01-26T01:54:40.415678: step 1555, loss 0.218147, acc 0.921875\n",
      "2022-01-26T01:54:40.575581: step 1556, loss 0.143494, acc 0.9375\n",
      "2022-01-26T01:54:40.731562: step 1557, loss 0.0887676, acc 1\n",
      "2022-01-26T01:54:40.889181: step 1558, loss 0.226658, acc 0.875\n",
      "2022-01-26T01:54:41.057972: step 1559, loss 0.113426, acc 0.96875\n",
      "2022-01-26T01:54:41.227739: step 1560, loss 0.1781, acc 0.9375\n",
      "2022-01-26T01:54:41.387091: step 1561, loss 0.227192, acc 0.9375\n",
      "2022-01-26T01:54:41.522678: step 1562, loss 0.136842, acc 0.947368\n",
      "2022-01-26T01:54:41.682390: step 1563, loss 0.150286, acc 0.9375\n",
      "2022-01-26T01:54:41.838369: step 1564, loss 0.135285, acc 0.953125\n",
      "2022-01-26T01:54:42.013299: step 1565, loss 0.0820076, acc 0.984375\n",
      "2022-01-26T01:54:42.175200: step 1566, loss 0.096823, acc 0.984375\n",
      "2022-01-26T01:54:42.344321: step 1567, loss 0.130889, acc 0.9375\n",
      "2022-01-26T01:54:42.509212: step 1568, loss 0.0898108, acc 0.96875\n",
      "2022-01-26T01:54:42.691725: step 1569, loss 0.10285, acc 0.984375\n",
      "2022-01-26T01:54:42.852900: step 1570, loss 0.092823, acc 0.96875\n",
      "2022-01-26T01:54:43.032432: step 1571, loss 0.0786476, acc 0.984375\n",
      "2022-01-26T01:54:43.192688: step 1572, loss 0.0829328, acc 0.984375\n",
      "2022-01-26T01:54:43.365024: step 1573, loss 0.150097, acc 0.953125\n",
      "2022-01-26T01:54:43.524046: step 1574, loss 0.179081, acc 0.921875\n",
      "2022-01-26T01:54:43.701340: step 1575, loss 0.116021, acc 0.96875\n",
      "2022-01-26T01:54:43.866476: step 1576, loss 0.114405, acc 0.96875\n",
      "2022-01-26T01:54:44.040404: step 1577, loss 0.0946403, acc 0.96875\n",
      "2022-01-26T01:54:44.208883: step 1578, loss 0.202253, acc 0.90625\n",
      "2022-01-26T01:54:44.380478: step 1579, loss 0.108547, acc 0.953125\n",
      "2022-01-26T01:54:44.551675: step 1580, loss 0.17451, acc 0.9375\n",
      "2022-01-26T01:54:44.732849: step 1581, loss 0.119872, acc 0.96875\n",
      "2022-01-26T01:54:44.916567: step 1582, loss 0.105278, acc 0.9375\n",
      "2022-01-26T01:54:45.076391: step 1583, loss 0.147546, acc 0.953125\n",
      "2022-01-26T01:54:45.237133: step 1584, loss 0.131557, acc 0.9375\n",
      "2022-01-26T01:54:45.400468: step 1585, loss 0.0543852, acc 0.984375\n",
      "2022-01-26T01:54:45.564094: step 1586, loss 0.154297, acc 0.953125\n",
      "2022-01-26T01:54:45.723838: step 1587, loss 0.0849473, acc 0.984375\n",
      "2022-01-26T01:54:45.882762: step 1588, loss 0.094899, acc 0.96875\n",
      "2022-01-26T01:54:46.041664: step 1589, loss 0.0847614, acc 0.984375\n",
      "2022-01-26T01:54:46.201485: step 1590, loss 0.104856, acc 0.96875\n",
      "2022-01-26T01:54:46.370868: step 1591, loss 0.100602, acc 0.9375\n",
      "2022-01-26T01:54:46.535901: step 1592, loss 0.182232, acc 0.921875\n",
      "2022-01-26T01:54:46.706743: step 1593, loss 0.119673, acc 0.96875\n",
      "2022-01-26T01:54:46.865805: step 1594, loss 0.0666838, acc 1\n",
      "2022-01-26T01:54:47.041948: step 1595, loss 0.135068, acc 0.9375\n",
      "2022-01-26T01:54:47.196757: step 1596, loss 0.0819664, acc 0.984375\n",
      "2022-01-26T01:54:47.352348: step 1597, loss 0.189528, acc 0.96875\n",
      "2022-01-26T01:54:47.506582: step 1598, loss 0.194076, acc 0.9375\n",
      "2022-01-26T01:54:47.670056: step 1599, loss 0.0916861, acc 0.9375\n",
      "2022-01-26T01:54:47.824057: step 1600, loss 0.0615288, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:54:48.425986: step 1600, loss 0.659301, acc 0.73375\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1600 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:54:48.570142 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1600 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1600\n",
      "\n",
      "2022-01-26T01:54:48.796833: step 1601, loss 0.0955227, acc 0.96875\n",
      "2022-01-26T01:54:48.978459: step 1602, loss 0.129521, acc 0.953125\n",
      "2022-01-26T01:54:49.141008: step 1603, loss 0.203115, acc 0.90625\n",
      "2022-01-26T01:54:49.370772: step 1604, loss 0.191713, acc 0.953125\n",
      "2022-01-26T01:54:49.594152: step 1605, loss 0.146552, acc 0.953125\n",
      "2022-01-26T01:54:49.842606: step 1606, loss 0.126811, acc 0.953125\n",
      "2022-01-26T01:54:50.048797: step 1607, loss 0.212305, acc 0.921875\n",
      "2022-01-26T01:54:50.271927: step 1608, loss 0.147125, acc 0.953125\n",
      "2022-01-26T01:54:50.468010: step 1609, loss 0.127622, acc 0.9375\n",
      "2022-01-26T01:54:50.641432: step 1610, loss 0.286326, acc 0.890625\n",
      "2022-01-26T01:54:50.811822: step 1611, loss 0.099869, acc 0.96875\n",
      "2022-01-26T01:54:51.021788: step 1612, loss 0.19136, acc 0.9375\n",
      "2022-01-26T01:54:51.202799: step 1613, loss 0.125756, acc 0.953125\n",
      "2022-01-26T01:54:51.413988: step 1614, loss 0.261404, acc 0.859375\n",
      "2022-01-26T01:54:51.620067: step 1615, loss 0.0756623, acc 0.96875\n",
      "2022-01-26T01:54:51.838659: step 1616, loss 0.0264683, acc 1\n",
      "2022-01-26T01:54:52.032404: step 1617, loss 0.195573, acc 0.921875\n",
      "2022-01-26T01:54:52.221087: step 1618, loss 0.126074, acc 0.953125\n",
      "2022-01-26T01:54:52.389252: step 1619, loss 0.115674, acc 0.953125\n",
      "2022-01-26T01:54:52.558828: step 1620, loss 0.127641, acc 0.921875\n",
      "2022-01-26T01:54:52.727187: step 1621, loss 0.126807, acc 0.96875\n",
      "2022-01-26T01:54:52.908073: step 1622, loss 0.164156, acc 0.953125\n",
      "2022-01-26T01:54:53.089902: step 1623, loss 0.133507, acc 0.9375\n",
      "2022-01-26T01:54:53.286102: step 1624, loss 0.0846665, acc 0.96875\n",
      "2022-01-26T01:54:53.453381: step 1625, loss 0.103969, acc 0.96875\n",
      "2022-01-26T01:54:53.670623: step 1626, loss 0.167801, acc 0.90625\n",
      "2022-01-26T01:54:53.862436: step 1627, loss 0.0783527, acc 0.984375\n",
      "2022-01-26T01:54:54.096672: step 1628, loss 0.0728973, acc 1\n",
      "2022-01-26T01:54:54.291959: step 1629, loss 0.141412, acc 0.96875\n",
      "2022-01-26T01:54:54.509261: step 1630, loss 0.216654, acc 0.921875\n",
      "2022-01-26T01:54:54.708451: step 1631, loss 0.127977, acc 0.921875\n",
      "2022-01-26T01:54:54.892490: step 1632, loss 0.120196, acc 0.96875\n",
      "2022-01-26T01:54:55.070081: step 1633, loss 0.132679, acc 0.96875\n",
      "2022-01-26T01:54:55.253457: step 1634, loss 0.141446, acc 0.9375\n",
      "2022-01-26T01:54:55.449062: step 1635, loss 0.13319, acc 0.953125\n",
      "2022-01-26T01:54:55.627868: step 1636, loss 0.0783175, acc 0.953125\n",
      "2022-01-26T01:54:55.785294: step 1637, loss 0.135342, acc 0.953125\n",
      "2022-01-26T01:54:55.961882: step 1638, loss 0.129098, acc 0.96875\n",
      "2022-01-26T01:54:56.138365: step 1639, loss 0.0826148, acc 0.96875\n",
      "2022-01-26T01:54:56.310099: step 1640, loss 0.195677, acc 0.9375\n",
      "2022-01-26T01:54:56.483066: step 1641, loss 0.081452, acc 0.96875\n",
      "2022-01-26T01:54:56.662790: step 1642, loss 0.126853, acc 0.9375\n",
      "2022-01-26T01:54:56.827180: step 1643, loss 0.201085, acc 0.921875\n",
      "2022-01-26T01:54:56.998829: step 1644, loss 0.092562, acc 0.96875\n",
      "2022-01-26T01:54:57.164772: step 1645, loss 0.0652697, acc 0.96875\n",
      "2022-01-26T01:54:57.337350: step 1646, loss 0.102258, acc 0.96875\n",
      "2022-01-26T01:54:57.493925: step 1647, loss 0.116553, acc 0.96875\n",
      "2022-01-26T01:54:57.663089: step 1648, loss 0.153085, acc 0.953125\n",
      "2022-01-26T01:54:57.827103: step 1649, loss 0.0899941, acc 0.984375\n",
      "2022-01-26T01:54:58.000963: step 1650, loss 0.0643719, acc 0.96875\n",
      "2022-01-26T01:54:58.172869: step 1651, loss 0.0980611, acc 0.96875\n",
      "2022-01-26T01:54:58.349344: step 1652, loss 0.0570202, acc 1\n",
      "2022-01-26T01:54:58.504868: step 1653, loss 0.149257, acc 0.921875\n",
      "2022-01-26T01:54:58.671093: step 1654, loss 0.0556219, acc 1\n",
      "2022-01-26T01:54:58.827760: step 1655, loss 0.100355, acc 0.96875\n",
      "2022-01-26T01:54:58.988030: step 1656, loss 0.0879714, acc 0.984375\n",
      "2022-01-26T01:54:59.153370: step 1657, loss 0.0654896, acc 1\n",
      "2022-01-26T01:54:59.332345: step 1658, loss 0.261007, acc 0.890625\n",
      "2022-01-26T01:54:59.486006: step 1659, loss 0.110435, acc 0.953125\n",
      "2022-01-26T01:54:59.647336: step 1660, loss 0.100243, acc 0.96875\n",
      "2022-01-26T01:54:59.802701: step 1661, loss 0.140367, acc 0.9375\n",
      "2022-01-26T01:54:59.968444: step 1662, loss 0.223954, acc 0.921875\n",
      "2022-01-26T01:55:00.123464: step 1663, loss 0.0820537, acc 0.984375\n",
      "2022-01-26T01:55:00.294797: step 1664, loss 0.0455473, acc 1\n",
      "2022-01-26T01:55:00.454105: step 1665, loss 0.165478, acc 0.953125\n",
      "2022-01-26T01:55:00.628287: step 1666, loss 0.164473, acc 0.953125\n",
      "2022-01-26T01:55:00.798430: step 1667, loss 0.0878168, acc 0.984375\n",
      "2022-01-26T01:55:00.967299: step 1668, loss 0.168725, acc 0.90625\n",
      "2022-01-26T01:55:01.140997: step 1669, loss 0.158501, acc 0.953125\n",
      "2022-01-26T01:55:01.322343: step 1670, loss 0.190469, acc 0.9375\n",
      "2022-01-26T01:55:01.481219: step 1671, loss 0.125654, acc 0.953125\n",
      "2022-01-26T01:55:01.648896: step 1672, loss 0.0871228, acc 0.953125\n",
      "2022-01-26T01:55:01.815007: step 1673, loss 0.0768154, acc 0.96875\n",
      "2022-01-26T01:55:01.992949: step 1674, loss 0.0499566, acc 0.984375\n",
      "2022-01-26T01:55:02.164125: step 1675, loss 0.146732, acc 0.9375\n",
      "2022-01-26T01:55:02.338446: step 1676, loss 0.0548058, acc 1\n",
      "2022-01-26T01:55:02.506933: step 1677, loss 0.122973, acc 0.953125\n",
      "2022-01-26T01:55:02.672758: step 1678, loss 0.170844, acc 0.9375\n",
      "2022-01-26T01:55:02.836128: step 1679, loss 0.140032, acc 0.921875\n",
      "2022-01-26T01:55:03.003036: step 1680, loss 0.0876281, acc 0.953125\n",
      "2022-01-26T01:55:03.173382: step 1681, loss 0.152561, acc 0.96875\n",
      "2022-01-26T01:55:03.352957: step 1682, loss 0.162532, acc 0.921875\n",
      "2022-01-26T01:55:03.512834: step 1683, loss 0.0789446, acc 0.984375\n",
      "2022-01-26T01:55:03.684786: step 1684, loss 0.106322, acc 0.96875\n",
      "2022-01-26T01:55:03.850031: step 1685, loss 0.194768, acc 0.921875\n",
      "2022-01-26T01:55:04.012048: step 1686, loss 0.163334, acc 0.921875\n",
      "2022-01-26T01:55:04.177782: step 1687, loss 0.119163, acc 0.953125\n",
      "2022-01-26T01:55:04.355613: step 1688, loss 0.160333, acc 0.921875\n",
      "2022-01-26T01:55:04.524373: step 1689, loss 0.111356, acc 0.953125\n",
      "2022-01-26T01:55:04.693016: step 1690, loss 0.175504, acc 0.921875\n",
      "2022-01-26T01:55:04.857305: step 1691, loss 0.091169, acc 0.953125\n",
      "2022-01-26T01:55:05.028495: step 1692, loss 0.0851406, acc 0.96875\n",
      "2022-01-26T01:55:05.191642: step 1693, loss 0.0726846, acc 0.984375\n",
      "2022-01-26T01:55:05.351075: step 1694, loss 0.102676, acc 0.953125\n",
      "2022-01-26T01:55:05.507323: step 1695, loss 0.119204, acc 0.9375\n",
      "2022-01-26T01:55:05.683300: step 1696, loss 0.127473, acc 0.9375\n",
      "2022-01-26T01:55:05.854773: step 1697, loss 0.0724434, acc 0.984375\n",
      "2022-01-26T01:55:06.027760: step 1698, loss 0.105653, acc 0.96875\n",
      "2022-01-26T01:55:06.190542: step 1699, loss 0.20562, acc 0.890625\n",
      "2022-01-26T01:55:06.360286: step 1700, loss 0.135458, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:55:06.981229: step 1700, loss 0.660915, acc 0.738125\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1700 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:55:07.144229 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1700 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1700\n",
      "\n",
      "2022-01-26T01:55:07.392963: step 1701, loss 0.112597, acc 0.953125\n",
      "2022-01-26T01:55:07.589185: step 1702, loss 0.189387, acc 0.9375\n",
      "2022-01-26T01:55:07.825764: step 1703, loss 0.1165, acc 0.96875\n",
      "2022-01-26T01:55:08.057373: step 1704, loss 0.200131, acc 0.894737\n",
      "2022-01-26T01:55:08.372745: step 1705, loss 0.0527762, acc 0.984375\n",
      "2022-01-26T01:55:08.623429: step 1706, loss 0.115929, acc 0.984375\n",
      "2022-01-26T01:55:08.851452: step 1707, loss 0.0896292, acc 0.984375\n",
      "2022-01-26T01:55:09.066550: step 1708, loss 0.110442, acc 0.9375\n",
      "2022-01-26T01:55:09.270202: step 1709, loss 0.0692176, acc 0.96875\n",
      "2022-01-26T01:55:09.470008: step 1710, loss 0.042532, acc 1\n",
      "2022-01-26T01:55:09.652934: step 1711, loss 0.0500599, acc 0.96875\n",
      "2022-01-26T01:55:09.831339: step 1712, loss 0.0606091, acc 1\n",
      "2022-01-26T01:55:10.040247: step 1713, loss 0.11683, acc 0.953125\n",
      "2022-01-26T01:55:10.265663: step 1714, loss 0.146759, acc 0.9375\n",
      "2022-01-26T01:55:10.485229: step 1715, loss 0.121264, acc 0.9375\n",
      "2022-01-26T01:55:10.705972: step 1716, loss 0.0698573, acc 0.96875\n",
      "2022-01-26T01:55:10.909686: step 1717, loss 0.0535118, acc 0.984375\n",
      "2022-01-26T01:55:11.094114: step 1718, loss 0.0711578, acc 0.96875\n",
      "2022-01-26T01:55:11.296584: step 1719, loss 0.0527261, acc 0.984375\n",
      "2022-01-26T01:55:11.467880: step 1720, loss 0.117609, acc 0.9375\n",
      "2022-01-26T01:55:11.645213: step 1721, loss 0.0626312, acc 1\n",
      "2022-01-26T01:55:11.834820: step 1722, loss 0.133789, acc 0.953125\n",
      "2022-01-26T01:55:12.039573: step 1723, loss 0.0660373, acc 0.984375\n",
      "2022-01-26T01:55:12.246846: step 1724, loss 0.0627911, acc 0.984375\n",
      "2022-01-26T01:55:12.433040: step 1725, loss 0.10866, acc 0.984375\n",
      "2022-01-26T01:55:12.616666: step 1726, loss 0.0738643, acc 0.96875\n",
      "2022-01-26T01:55:12.786642: step 1727, loss 0.1257, acc 0.96875\n",
      "2022-01-26T01:55:12.982085: step 1728, loss 0.0932308, acc 0.953125\n",
      "2022-01-26T01:55:13.151977: step 1729, loss 0.0919346, acc 0.953125\n",
      "2022-01-26T01:55:13.324111: step 1730, loss 0.0626967, acc 0.984375\n",
      "2022-01-26T01:55:13.490113: step 1731, loss 0.0777403, acc 0.96875\n",
      "2022-01-26T01:55:13.685412: step 1732, loss 0.0706653, acc 0.96875\n",
      "2022-01-26T01:55:13.855621: step 1733, loss 0.0315493, acc 1\n",
      "2022-01-26T01:55:14.042425: step 1734, loss 0.0899906, acc 0.96875\n",
      "2022-01-26T01:55:14.216629: step 1735, loss 0.109806, acc 0.953125\n",
      "2022-01-26T01:55:14.382382: step 1736, loss 0.153884, acc 0.953125\n",
      "2022-01-26T01:55:14.545855: step 1737, loss 0.154657, acc 0.953125\n",
      "2022-01-26T01:55:14.711262: step 1738, loss 0.0344489, acc 1\n",
      "2022-01-26T01:55:14.870109: step 1739, loss 0.130314, acc 0.921875\n",
      "2022-01-26T01:55:15.052959: step 1740, loss 0.0878876, acc 0.984375\n",
      "2022-01-26T01:55:15.224333: step 1741, loss 0.0898746, acc 0.96875\n",
      "2022-01-26T01:55:15.411350: step 1742, loss 0.13292, acc 0.921875\n",
      "2022-01-26T01:55:15.582033: step 1743, loss 0.0489025, acc 0.984375\n",
      "2022-01-26T01:55:15.764379: step 1744, loss 0.076774, acc 0.984375\n",
      "2022-01-26T01:55:15.926188: step 1745, loss 0.111972, acc 0.96875\n",
      "2022-01-26T01:55:16.086473: step 1746, loss 0.089277, acc 0.96875\n",
      "2022-01-26T01:55:16.255790: step 1747, loss 0.0444694, acc 0.984375\n",
      "2022-01-26T01:55:16.430359: step 1748, loss 0.120625, acc 0.953125\n",
      "2022-01-26T01:55:16.586811: step 1749, loss 0.0560587, acc 0.984375\n",
      "2022-01-26T01:55:16.747253: step 1750, loss 0.0924046, acc 0.984375\n",
      "2022-01-26T01:55:16.901198: step 1751, loss 0.0636507, acc 0.984375\n",
      "2022-01-26T01:55:17.056351: step 1752, loss 0.0873615, acc 0.96875\n",
      "2022-01-26T01:55:17.213711: step 1753, loss 0.0622693, acc 0.984375\n",
      "2022-01-26T01:55:17.383496: step 1754, loss 0.126926, acc 0.9375\n",
      "2022-01-26T01:55:17.537605: step 1755, loss 0.0735421, acc 0.96875\n",
      "2022-01-26T01:55:17.693407: step 1756, loss 0.105393, acc 0.953125\n",
      "2022-01-26T01:55:17.849881: step 1757, loss 0.120479, acc 0.9375\n",
      "2022-01-26T01:55:18.009612: step 1758, loss 0.0614017, acc 0.96875\n",
      "2022-01-26T01:55:18.165833: step 1759, loss 0.0978574, acc 0.9375\n",
      "2022-01-26T01:55:18.325352: step 1760, loss 0.128557, acc 0.96875\n",
      "2022-01-26T01:55:18.479651: step 1761, loss 0.0997678, acc 0.96875\n",
      "2022-01-26T01:55:18.643353: step 1762, loss 0.195847, acc 0.921875\n",
      "2022-01-26T01:55:18.793071: step 1763, loss 0.0501032, acc 0.984375\n",
      "2022-01-26T01:55:18.949324: step 1764, loss 0.0767896, acc 0.96875\n",
      "2022-01-26T01:55:19.100598: step 1765, loss 0.106412, acc 0.953125\n",
      "2022-01-26T01:55:19.252898: step 1766, loss 0.0704422, acc 0.984375\n",
      "2022-01-26T01:55:19.409105: step 1767, loss 0.126797, acc 0.9375\n",
      "2022-01-26T01:55:19.570587: step 1768, loss 0.101154, acc 0.953125\n",
      "2022-01-26T01:55:19.726158: step 1769, loss 0.102409, acc 0.96875\n",
      "2022-01-26T01:55:19.883069: step 1770, loss 0.04978, acc 0.984375\n",
      "2022-01-26T01:55:20.038689: step 1771, loss 0.100936, acc 0.9375\n",
      "2022-01-26T01:55:20.196341: step 1772, loss 0.116177, acc 0.96875\n",
      "2022-01-26T01:55:20.351151: step 1773, loss 0.109392, acc 0.953125\n",
      "2022-01-26T01:55:20.509206: step 1774, loss 0.0374066, acc 0.984375\n",
      "2022-01-26T01:55:20.680348: step 1775, loss 0.197151, acc 0.9375\n",
      "2022-01-26T01:55:20.840544: step 1776, loss 0.0750417, acc 0.96875\n",
      "2022-01-26T01:55:20.997145: step 1777, loss 0.0519011, acc 1\n",
      "2022-01-26T01:55:21.172071: step 1778, loss 0.213639, acc 0.90625\n",
      "2022-01-26T01:55:21.323823: step 1779, loss 0.155088, acc 0.90625\n",
      "2022-01-26T01:55:21.478623: step 1780, loss 0.200263, acc 0.921875\n",
      "2022-01-26T01:55:21.633853: step 1781, loss 0.125313, acc 0.953125\n",
      "2022-01-26T01:55:21.791255: step 1782, loss 0.0642592, acc 0.96875\n",
      "2022-01-26T01:55:21.944804: step 1783, loss 0.0499528, acc 1\n",
      "2022-01-26T01:55:22.099237: step 1784, loss 0.121586, acc 0.9375\n",
      "2022-01-26T01:55:22.263634: step 1785, loss 0.0931396, acc 0.953125\n",
      "2022-01-26T01:55:22.435365: step 1786, loss 0.0880536, acc 0.96875\n",
      "2022-01-26T01:55:22.609071: step 1787, loss 0.0504609, acc 0.96875\n",
      "2022-01-26T01:55:22.773431: step 1788, loss 0.0666727, acc 0.96875\n",
      "2022-01-26T01:55:22.936813: step 1789, loss 0.168686, acc 0.96875\n",
      "2022-01-26T01:55:23.099382: step 1790, loss 0.077361, acc 0.96875\n",
      "2022-01-26T01:55:23.258542: step 1791, loss 0.13631, acc 0.9375\n",
      "2022-01-26T01:55:23.426620: step 1792, loss 0.111717, acc 0.953125\n",
      "2022-01-26T01:55:23.591151: step 1793, loss 0.0960216, acc 0.953125\n",
      "2022-01-26T01:55:23.750673: step 1794, loss 0.130063, acc 0.96875\n",
      "2022-01-26T01:55:23.906008: step 1795, loss 0.155293, acc 0.9375\n",
      "2022-01-26T01:55:24.065743: step 1796, loss 0.092075, acc 0.9375\n",
      "2022-01-26T01:55:24.222160: step 1797, loss 0.123301, acc 0.96875\n",
      "2022-01-26T01:55:24.391882: step 1798, loss 0.0371166, acc 1\n",
      "2022-01-26T01:55:24.556784: step 1799, loss 0.0665947, acc 0.96875\n",
      "2022-01-26T01:55:24.726798: step 1800, loss 0.0611224, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:55:25.335721: step 1800, loss 0.715848, acc 0.7425\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1800 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:55:25.493135 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1800 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1800\n",
      "\n",
      "2022-01-26T01:55:25.714773: step 1801, loss 0.0723499, acc 0.984375\n",
      "2022-01-26T01:55:25.877421: step 1802, loss 0.194914, acc 0.9375\n",
      "2022-01-26T01:55:26.217266: step 1803, loss 0.0804636, acc 0.984375\n",
      "2022-01-26T01:55:26.842514: step 1804, loss 0.12003, acc 0.9375\n",
      "2022-01-26T01:55:27.055650: step 1805, loss 0.116207, acc 0.953125\n",
      "2022-01-26T01:55:27.259035: step 1806, loss 0.0550603, acc 0.984375\n",
      "2022-01-26T01:55:27.443084: step 1807, loss 0.0876165, acc 0.984375\n",
      "2022-01-26T01:55:27.610493: step 1808, loss 0.0769466, acc 0.96875\n",
      "2022-01-26T01:55:27.795379: step 1809, loss 0.0682496, acc 0.984375\n",
      "2022-01-26T01:55:27.988554: step 1810, loss 0.184866, acc 0.9375\n",
      "2022-01-26T01:55:28.177311: step 1811, loss 0.149466, acc 0.9375\n",
      "2022-01-26T01:55:28.360106: step 1812, loss 0.13474, acc 0.953125\n",
      "2022-01-26T01:55:28.558259: step 1813, loss 0.104247, acc 0.953125\n",
      "2022-01-26T01:55:28.751496: step 1814, loss 0.157202, acc 0.9375\n",
      "2022-01-26T01:55:28.936600: step 1815, loss 0.105644, acc 0.9375\n",
      "2022-01-26T01:55:29.147566: step 1816, loss 0.0882621, acc 0.953125\n",
      "2022-01-26T01:55:29.345652: step 1817, loss 0.055133, acc 0.984375\n",
      "2022-01-26T01:55:29.526815: step 1818, loss 0.122013, acc 0.9375\n",
      "2022-01-26T01:55:29.692186: step 1819, loss 0.109153, acc 0.953125\n",
      "2022-01-26T01:55:29.865218: step 1820, loss 0.0925328, acc 0.96875\n",
      "2022-01-26T01:55:30.046595: step 1821, loss 0.0904946, acc 0.953125\n",
      "2022-01-26T01:55:30.220278: step 1822, loss 0.125324, acc 0.96875\n",
      "2022-01-26T01:55:30.402648: step 1823, loss 0.0641644, acc 0.984375\n",
      "2022-01-26T01:55:30.585065: step 1824, loss 0.0751736, acc 0.953125\n",
      "2022-01-26T01:55:30.746434: step 1825, loss 0.12462, acc 0.9375\n",
      "2022-01-26T01:55:30.943041: step 1826, loss 0.0772647, acc 0.96875\n",
      "2022-01-26T01:55:31.109443: step 1827, loss 0.0812754, acc 0.984375\n",
      "2022-01-26T01:55:31.303139: step 1828, loss 0.132521, acc 0.90625\n",
      "2022-01-26T01:55:31.470747: step 1829, loss 0.0530609, acc 0.984375\n",
      "2022-01-26T01:55:31.640551: step 1830, loss 0.0679687, acc 0.984375\n",
      "2022-01-26T01:55:31.807466: step 1831, loss 0.0877436, acc 0.984375\n",
      "2022-01-26T01:55:31.991529: step 1832, loss 0.124389, acc 0.953125\n",
      "2022-01-26T01:55:32.161884: step 1833, loss 0.0740843, acc 0.96875\n",
      "2022-01-26T01:55:32.344764: step 1834, loss 0.0862103, acc 0.96875\n",
      "2022-01-26T01:55:32.504702: step 1835, loss 0.100325, acc 0.953125\n",
      "2022-01-26T01:55:32.678704: step 1836, loss 0.0435967, acc 1\n",
      "2022-01-26T01:55:32.841968: step 1837, loss 0.171991, acc 0.953125\n",
      "2022-01-26T01:55:33.000258: step 1838, loss 0.168635, acc 0.921875\n",
      "2022-01-26T01:55:33.157105: step 1839, loss 0.0693798, acc 0.984375\n",
      "2022-01-26T01:55:33.316590: step 1840, loss 0.0995423, acc 0.9375\n",
      "2022-01-26T01:55:33.475006: step 1841, loss 0.113525, acc 0.96875\n",
      "2022-01-26T01:55:33.633163: step 1842, loss 0.148963, acc 0.953125\n",
      "2022-01-26T01:55:33.784507: step 1843, loss 0.0942752, acc 0.984375\n",
      "2022-01-26T01:55:33.947454: step 1844, loss 0.0795884, acc 0.984375\n",
      "2022-01-26T01:55:34.128071: step 1845, loss 0.0738252, acc 0.984375\n",
      "2022-01-26T01:55:34.270662: step 1846, loss 0.137578, acc 0.921053\n",
      "2022-01-26T01:55:34.432634: step 1847, loss 0.18449, acc 0.921875\n",
      "2022-01-26T01:55:34.592242: step 1848, loss 0.0814251, acc 0.96875\n",
      "2022-01-26T01:55:34.750676: step 1849, loss 0.0913367, acc 0.96875\n",
      "2022-01-26T01:55:34.916902: step 1850, loss 0.0618909, acc 0.953125\n",
      "2022-01-26T01:55:35.079921: step 1851, loss 0.0484444, acc 0.984375\n",
      "2022-01-26T01:55:35.251054: step 1852, loss 0.0631144, acc 0.96875\n",
      "2022-01-26T01:55:35.412139: step 1853, loss 0.0532725, acc 0.96875\n",
      "2022-01-26T01:55:35.579292: step 1854, loss 0.0704552, acc 0.96875\n",
      "2022-01-26T01:55:35.738501: step 1855, loss 0.0758266, acc 0.96875\n",
      "2022-01-26T01:55:35.890375: step 1856, loss 0.089988, acc 0.96875\n",
      "2022-01-26T01:55:36.053251: step 1857, loss 0.0652721, acc 0.984375\n",
      "2022-01-26T01:55:36.218806: step 1858, loss 0.0646095, acc 0.96875\n",
      "2022-01-26T01:55:36.385290: step 1859, loss 0.0729915, acc 0.96875\n",
      "2022-01-26T01:55:36.548976: step 1860, loss 0.0847043, acc 0.96875\n",
      "2022-01-26T01:55:36.704288: step 1861, loss 0.0433202, acc 0.984375\n",
      "2022-01-26T01:55:36.892527: step 1862, loss 0.0269801, acc 1\n",
      "2022-01-26T01:55:37.056568: step 1863, loss 0.0897945, acc 0.96875\n",
      "2022-01-26T01:55:37.232294: step 1864, loss 0.0322841, acc 1\n",
      "2022-01-26T01:55:37.391290: step 1865, loss 0.0745545, acc 0.96875\n",
      "2022-01-26T01:55:37.547253: step 1866, loss 0.047428, acc 0.984375\n",
      "2022-01-26T01:55:37.701291: step 1867, loss 0.0292165, acc 1\n",
      "2022-01-26T01:55:37.860621: step 1868, loss 0.047613, acc 0.984375\n",
      "2022-01-26T01:55:38.015471: step 1869, loss 0.0594354, acc 0.984375\n",
      "2022-01-26T01:55:38.172450: step 1870, loss 0.107497, acc 0.953125\n",
      "2022-01-26T01:55:38.336001: step 1871, loss 0.0326236, acc 1\n",
      "2022-01-26T01:55:38.510691: step 1872, loss 0.0567755, acc 0.96875\n",
      "2022-01-26T01:55:38.682087: step 1873, loss 0.0543848, acc 0.984375\n",
      "2022-01-26T01:55:38.863803: step 1874, loss 0.102928, acc 0.984375\n",
      "2022-01-26T01:55:39.029544: step 1875, loss 0.0711762, acc 0.984375\n",
      "2022-01-26T01:55:39.198010: step 1876, loss 0.120388, acc 0.96875\n",
      "2022-01-26T01:55:39.358587: step 1877, loss 0.0610936, acc 0.984375\n",
      "2022-01-26T01:55:39.530726: step 1878, loss 0.108679, acc 0.953125\n",
      "2022-01-26T01:55:39.699616: step 1879, loss 0.0388956, acc 1\n",
      "2022-01-26T01:55:39.855582: step 1880, loss 0.0992261, acc 0.9375\n",
      "2022-01-26T01:55:40.020326: step 1881, loss 0.0253247, acc 1\n",
      "2022-01-26T01:55:40.181082: step 1882, loss 0.0307421, acc 1\n",
      "2022-01-26T01:55:40.335362: step 1883, loss 0.0484307, acc 0.984375\n",
      "2022-01-26T01:55:40.496701: step 1884, loss 0.0494387, acc 0.984375\n",
      "2022-01-26T01:55:40.646127: step 1885, loss 0.0674434, acc 0.96875\n",
      "2022-01-26T01:55:40.803286: step 1886, loss 0.023014, acc 1\n",
      "2022-01-26T01:55:40.959362: step 1887, loss 0.174896, acc 0.953125\n",
      "2022-01-26T01:55:41.117629: step 1888, loss 0.206562, acc 0.96875\n",
      "2022-01-26T01:55:41.277020: step 1889, loss 0.0919233, acc 0.96875\n",
      "2022-01-26T01:55:41.448162: step 1890, loss 0.0555842, acc 1\n",
      "2022-01-26T01:55:41.601717: step 1891, loss 0.042182, acc 0.984375\n",
      "2022-01-26T01:55:41.757931: step 1892, loss 0.0746626, acc 0.96875\n",
      "2022-01-26T01:55:41.911530: step 1893, loss 0.115895, acc 0.953125\n",
      "2022-01-26T01:55:42.078146: step 1894, loss 0.101262, acc 0.96875\n",
      "2022-01-26T01:55:42.231902: step 1895, loss 0.0461668, acc 0.984375\n",
      "2022-01-26T01:55:42.387033: step 1896, loss 0.0329344, acc 0.984375\n",
      "2022-01-26T01:55:42.545727: step 1897, loss 0.0519528, acc 0.984375\n",
      "2022-01-26T01:55:42.701485: step 1898, loss 0.083224, acc 0.953125\n",
      "2022-01-26T01:55:42.850764: step 1899, loss 0.0837517, acc 0.953125\n",
      "2022-01-26T01:55:43.028298: step 1900, loss 0.075339, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:55:43.646289: step 1900, loss 0.720566, acc 0.7425\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1900 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:55:43.806762 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1900 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-1900\n",
      "\n",
      "2022-01-26T01:55:44.049465: step 1901, loss 0.0460076, acc 1\n",
      "2022-01-26T01:55:44.228874: step 1902, loss 0.0463365, acc 0.984375\n",
      "2022-01-26T01:55:44.424710: step 1903, loss 0.0821136, acc 0.96875\n",
      "2022-01-26T01:55:44.798651: step 1904, loss 0.0995566, acc 0.96875\n",
      "2022-01-26T01:55:45.087051: step 1905, loss 0.0858001, acc 0.96875\n",
      "2022-01-26T01:55:45.327187: step 1906, loss 0.0349453, acc 1\n",
      "2022-01-26T01:55:45.533576: step 1907, loss 0.0568097, acc 0.96875\n",
      "2022-01-26T01:55:45.745369: step 1908, loss 0.0213526, acc 1\n",
      "2022-01-26T01:55:45.981983: step 1909, loss 0.0461354, acc 0.984375\n",
      "2022-01-26T01:55:46.170403: step 1910, loss 0.165708, acc 0.9375\n",
      "2022-01-26T01:55:46.351895: step 1911, loss 0.0520626, acc 0.984375\n",
      "2022-01-26T01:55:46.567997: step 1912, loss 0.0534356, acc 1\n",
      "2022-01-26T01:55:46.756959: step 1913, loss 0.0482097, acc 1\n",
      "2022-01-26T01:55:46.937611: step 1914, loss 0.0820112, acc 0.96875\n",
      "2022-01-26T01:55:47.185142: step 1915, loss 0.14878, acc 0.953125\n",
      "2022-01-26T01:55:47.360525: step 1916, loss 0.0749875, acc 0.96875\n",
      "2022-01-26T01:55:47.594475: step 1917, loss 0.0508292, acc 1\n",
      "2022-01-26T01:55:47.873921: step 1918, loss 0.144286, acc 0.953125\n",
      "2022-01-26T01:55:48.148912: step 1919, loss 0.0869379, acc 0.984375\n",
      "2022-01-26T01:55:48.367523: step 1920, loss 0.0517245, acc 0.96875\n",
      "2022-01-26T01:55:48.549625: step 1921, loss 0.0554774, acc 0.96875\n",
      "2022-01-26T01:55:48.718402: step 1922, loss 0.126316, acc 0.96875\n",
      "2022-01-26T01:55:48.897759: step 1923, loss 0.0226163, acc 1\n",
      "2022-01-26T01:55:49.112719: step 1924, loss 0.0369144, acc 1\n",
      "2022-01-26T01:55:49.308860: step 1925, loss 0.107877, acc 0.953125\n",
      "2022-01-26T01:55:49.489650: step 1926, loss 0.0875857, acc 0.953125\n",
      "2022-01-26T01:55:49.708507: step 1927, loss 0.0444629, acc 0.984375\n",
      "2022-01-26T01:55:49.894072: step 1928, loss 0.0376074, acc 0.984375\n",
      "2022-01-26T01:55:50.075831: step 1929, loss 0.0484478, acc 0.984375\n",
      "2022-01-26T01:55:50.276388: step 1930, loss 0.0696505, acc 0.984375\n",
      "2022-01-26T01:55:50.457582: step 1931, loss 0.0727199, acc 0.984375\n",
      "2022-01-26T01:55:50.620717: step 1932, loss 0.0492639, acc 0.984375\n",
      "2022-01-26T01:55:50.810899: step 1933, loss 0.0613313, acc 0.984375\n",
      "2022-01-26T01:55:50.989834: step 1934, loss 0.100903, acc 0.96875\n",
      "2022-01-26T01:55:51.201350: step 1935, loss 0.113629, acc 0.953125\n",
      "2022-01-26T01:55:51.422676: step 1936, loss 0.0883361, acc 0.96875\n",
      "2022-01-26T01:55:51.589630: step 1937, loss 0.112895, acc 0.953125\n",
      "2022-01-26T01:55:51.762579: step 1938, loss 0.101004, acc 0.96875\n",
      "2022-01-26T01:55:51.924876: step 1939, loss 0.0634461, acc 0.984375\n",
      "2022-01-26T01:55:52.087367: step 1940, loss 0.0943107, acc 0.96875\n",
      "2022-01-26T01:55:52.239369: step 1941, loss 0.0289018, acc 1\n",
      "2022-01-26T01:55:52.400065: step 1942, loss 0.0633244, acc 0.984375\n",
      "2022-01-26T01:55:52.557376: step 1943, loss 0.0643169, acc 0.953125\n",
      "2022-01-26T01:55:52.715217: step 1944, loss 0.124921, acc 0.953125\n",
      "2022-01-26T01:55:52.869414: step 1945, loss 0.0217496, acc 1\n",
      "2022-01-26T01:55:53.027343: step 1946, loss 0.0459607, acc 0.96875\n",
      "2022-01-26T01:55:53.184170: step 1947, loss 0.0784465, acc 0.96875\n",
      "2022-01-26T01:55:53.340687: step 1948, loss 0.0220622, acc 1\n",
      "2022-01-26T01:55:53.499743: step 1949, loss 0.0726318, acc 0.96875\n",
      "2022-01-26T01:55:53.674402: step 1950, loss 0.0717002, acc 0.984375\n",
      "2022-01-26T01:55:53.843680: step 1951, loss 0.126961, acc 0.9375\n",
      "2022-01-26T01:55:54.009353: step 1952, loss 0.0377622, acc 1\n",
      "2022-01-26T01:55:54.169208: step 1953, loss 0.0775089, acc 0.96875\n",
      "2022-01-26T01:55:54.355311: step 1954, loss 0.053066, acc 0.984375\n",
      "2022-01-26T01:55:54.567562: step 1955, loss 0.0898267, acc 0.96875\n",
      "2022-01-26T01:55:54.742197: step 1956, loss 0.107693, acc 0.953125\n",
      "2022-01-26T01:55:54.950586: step 1957, loss 0.101821, acc 0.953125\n",
      "2022-01-26T01:55:55.119581: step 1958, loss 0.0944145, acc 0.953125\n",
      "2022-01-26T01:55:55.281745: step 1959, loss 0.0898137, acc 0.96875\n",
      "2022-01-26T01:55:55.481359: step 1960, loss 0.0669755, acc 0.96875\n",
      "2022-01-26T01:55:55.667043: step 1961, loss 0.222014, acc 0.90625\n",
      "2022-01-26T01:55:55.846851: step 1962, loss 0.0871806, acc 0.953125\n",
      "2022-01-26T01:55:56.034099: step 1963, loss 0.0565608, acc 0.984375\n",
      "2022-01-26T01:55:56.188444: step 1964, loss 0.0352528, acc 0.984375\n",
      "2022-01-26T01:55:56.363523: step 1965, loss 0.0707974, acc 0.96875\n",
      "2022-01-26T01:55:56.546214: step 1966, loss 0.0873937, acc 0.984375\n",
      "2022-01-26T01:55:56.712349: step 1967, loss 0.0745878, acc 0.953125\n",
      "2022-01-26T01:55:56.866601: step 1968, loss 0.0654877, acc 0.984375\n",
      "2022-01-26T01:55:57.052279: step 1969, loss 0.0372418, acc 1\n",
      "2022-01-26T01:55:57.233198: step 1970, loss 0.10266, acc 0.953125\n",
      "2022-01-26T01:55:57.392371: step 1971, loss 0.195263, acc 0.9375\n",
      "2022-01-26T01:55:57.582237: step 1972, loss 0.056153, acc 0.984375\n",
      "2022-01-26T01:55:57.781741: step 1973, loss 0.106762, acc 0.9375\n",
      "2022-01-26T01:55:57.990881: step 1974, loss 0.167093, acc 0.9375\n",
      "2022-01-26T01:55:58.206300: step 1975, loss 0.131409, acc 0.96875\n",
      "2022-01-26T01:55:58.390132: step 1976, loss 0.0538981, acc 0.96875\n",
      "2022-01-26T01:55:58.564969: step 1977, loss 0.0589301, acc 0.984375\n",
      "2022-01-26T01:55:58.748364: step 1978, loss 0.0588496, acc 0.984375\n",
      "2022-01-26T01:55:58.931632: step 1979, loss 0.128941, acc 0.9375\n",
      "2022-01-26T01:55:59.088615: step 1980, loss 0.107057, acc 0.96875\n",
      "2022-01-26T01:55:59.248882: step 1981, loss 0.0268405, acc 1\n",
      "2022-01-26T01:55:59.409246: step 1982, loss 0.112019, acc 0.96875\n",
      "2022-01-26T01:55:59.606362: step 1983, loss 0.0921591, acc 0.96875\n",
      "2022-01-26T01:55:59.769417: step 1984, loss 0.0540142, acc 1\n",
      "2022-01-26T01:55:59.929920: step 1985, loss 0.0380444, acc 1\n",
      "2022-01-26T01:56:00.111298: step 1986, loss 0.100367, acc 0.96875\n",
      "2022-01-26T01:56:00.297030: step 1987, loss 0.0285394, acc 1\n",
      "2022-01-26T01:56:00.429165: step 1988, loss 0.128914, acc 0.947368\n",
      "2022-01-26T01:56:00.619656: step 1989, loss 0.0794655, acc 0.96875\n",
      "2022-01-26T01:56:00.775156: step 1990, loss 0.0550352, acc 0.96875\n",
      "2022-01-26T01:56:00.941405: step 1991, loss 0.0892864, acc 0.953125\n",
      "2022-01-26T01:56:01.131039: step 1992, loss 0.0613275, acc 0.96875\n",
      "2022-01-26T01:56:01.288644: step 1993, loss 0.0523117, acc 0.96875\n",
      "2022-01-26T01:56:01.445994: step 1994, loss 0.0280785, acc 1\n",
      "2022-01-26T01:56:01.634119: step 1995, loss 0.045202, acc 0.984375\n",
      "2022-01-26T01:56:01.821560: step 1996, loss 0.0247052, acc 1\n",
      "2022-01-26T01:56:01.975610: step 1997, loss 0.0666881, acc 0.984375\n",
      "2022-01-26T01:56:02.142444: step 1998, loss 0.11073, acc 0.96875\n",
      "2022-01-26T01:56:02.330970: step 1999, loss 0.0297264, acc 0.984375\n",
      "2022-01-26T01:56:02.487858: step 2000, loss 0.0509184, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:56:03.140662: step 2000, loss 0.757092, acc 0.7425\n",
      "\n",
      "INFO:tensorflow:/Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-2000 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 01:56:03.315430 4454043136 checkpoint_management.py:95] /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-2000 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/allisonliu/Documents/Spring2022/APPM5720/biweekly-report-1-reign/runs/1643186971/checkpoints/model-2000\n",
      "\n",
      "2022-01-26T01:56:03.578851: step 2001, loss 0.134933, acc 0.9375\n",
      "2022-01-26T01:56:03.775584: step 2002, loss 0.044592, acc 1\n",
      "2022-01-26T01:56:03.958809: step 2003, loss 0.0716052, acc 0.96875\n",
      "2022-01-26T01:56:04.233597: step 2004, loss 0.098773, acc 0.96875\n",
      "2022-01-26T01:56:04.483656: step 2005, loss 0.10013, acc 0.953125\n",
      "2022-01-26T01:56:04.763121: step 2006, loss 0.0336026, acc 0.984375\n",
      "2022-01-26T01:56:05.005384: step 2007, loss 0.0255258, acc 1\n",
      "2022-01-26T01:56:05.242869: step 2008, loss 0.0703787, acc 0.96875\n",
      "2022-01-26T01:56:05.442436: step 2009, loss 0.059538, acc 0.96875\n",
      "2022-01-26T01:56:05.719969: step 2010, loss 0.0467266, acc 0.984375\n",
      "2022-01-26T01:56:05.903212: step 2011, loss 0.0560277, acc 0.984375\n",
      "2022-01-26T01:56:06.132286: step 2012, loss 0.064519, acc 0.96875\n",
      "2022-01-26T01:56:06.358670: step 2013, loss 0.065719, acc 0.984375\n",
      "2022-01-26T01:56:06.552942: step 2014, loss 0.0435755, acc 0.984375\n",
      "2022-01-26T01:56:06.719029: step 2015, loss 0.0410561, acc 0.984375\n",
      "2022-01-26T01:56:06.943928: step 2016, loss 0.0957481, acc 0.953125\n",
      "2022-01-26T01:56:07.191855: step 2017, loss 0.0413995, acc 0.984375\n",
      "2022-01-26T01:56:07.406172: step 2018, loss 0.0579391, acc 0.984375\n",
      "2022-01-26T01:56:07.618452: step 2019, loss 0.023054, acc 1\n",
      "2022-01-26T01:56:07.814882: step 2020, loss 0.0249721, acc 1\n",
      "2022-01-26T01:56:08.004953: step 2021, loss 0.0759069, acc 0.953125\n",
      "2022-01-26T01:56:08.180672: step 2022, loss 0.044298, acc 0.984375\n",
      "2022-01-26T01:56:08.368557: step 2023, loss 0.0244822, acc 1\n",
      "2022-01-26T01:56:08.541891: step 2024, loss 0.0625961, acc 0.96875\n",
      "2022-01-26T01:56:08.740757: step 2025, loss 0.0426143, acc 0.984375\n",
      "2022-01-26T01:56:08.944221: step 2026, loss 0.0375575, acc 1\n",
      "2022-01-26T01:56:09.168057: step 2027, loss 0.0324521, acc 1\n",
      "2022-01-26T01:56:09.506450: step 2028, loss 0.0974404, acc 0.9375\n",
      "2022-01-26T01:56:09.729752: step 2029, loss 0.0279578, acc 0.984375\n",
      "2022-01-26T01:56:09.929464: step 2030, loss 0.1587, acc 0.953125\n",
      "2022-01-26T01:56:10.125973: step 2031, loss 0.0745122, acc 0.96875\n",
      "2022-01-26T01:56:10.296420: step 2032, loss 0.0570255, acc 0.984375\n",
      "2022-01-26T01:56:10.469627: step 2033, loss 0.0838402, acc 0.96875\n",
      "2022-01-26T01:56:10.656128: step 2034, loss 0.0936834, acc 0.96875\n",
      "2022-01-26T01:56:10.838193: step 2035, loss 0.0310057, acc 1\n",
      "2022-01-26T01:56:11.021246: step 2036, loss 0.0503601, acc 0.984375\n",
      "2022-01-26T01:56:11.232925: step 2037, loss 0.0268082, acc 1\n",
      "2022-01-26T01:56:11.409764: step 2038, loss 0.0894061, acc 0.984375\n",
      "2022-01-26T01:56:11.575197: step 2039, loss 0.0568594, acc 0.984375\n",
      "2022-01-26T01:56:11.733974: step 2040, loss 0.0425852, acc 1\n",
      "2022-01-26T01:56:11.897123: step 2041, loss 0.0399142, acc 0.984375\n",
      "2022-01-26T01:56:12.055567: step 2042, loss 0.0695319, acc 0.953125\n",
      "2022-01-26T01:56:12.216681: step 2043, loss 0.0197153, acc 1\n",
      "2022-01-26T01:56:12.381621: step 2044, loss 0.110313, acc 0.953125\n",
      "2022-01-26T01:56:12.543968: step 2045, loss 0.044176, acc 0.984375\n",
      "2022-01-26T01:56:12.703831: step 2046, loss 0.0147303, acc 1\n",
      "2022-01-26T01:56:12.867892: step 2047, loss 0.0688449, acc 0.96875\n",
      "2022-01-26T01:56:13.032687: step 2048, loss 0.0517167, acc 0.984375\n",
      "2022-01-26T01:56:13.201318: step 2049, loss 0.0568758, acc 0.984375\n",
      "2022-01-26T01:56:13.379563: step 2050, loss 0.0617014, acc 0.96875\n",
      "2022-01-26T01:56:13.547458: step 2051, loss 0.0532833, acc 0.96875\n",
      "2022-01-26T01:56:13.722888: step 2052, loss 0.043189, acc 0.984375\n",
      "2022-01-26T01:56:13.897940: step 2053, loss 0.0769993, acc 0.96875\n",
      "2022-01-26T01:56:14.075925: step 2054, loss 0.0384225, acc 1\n",
      "2022-01-26T01:56:14.254172: step 2055, loss 0.0611246, acc 0.96875\n",
      "2022-01-26T01:56:14.444746: step 2056, loss 0.117954, acc 0.953125\n",
      "2022-01-26T01:56:14.639661: step 2057, loss 0.086274, acc 0.953125\n",
      "2022-01-26T01:56:14.811209: step 2058, loss 0.0406641, acc 1\n",
      "2022-01-26T01:56:14.994121: step 2059, loss 0.0706867, acc 0.96875\n",
      "2022-01-26T01:56:15.161372: step 2060, loss 0.059836, acc 0.96875\n",
      "2022-01-26T01:56:15.342237: step 2061, loss 0.0794515, acc 0.96875\n",
      "2022-01-26T01:56:15.512663: step 2062, loss 0.0315159, acc 1\n",
      "2022-01-26T01:56:15.690788: step 2063, loss 0.0169919, acc 1\n",
      "2022-01-26T01:56:15.872898: step 2064, loss 0.0972804, acc 0.96875\n",
      "2022-01-26T01:56:16.058290: step 2065, loss 0.0149209, acc 1\n",
      "2022-01-26T01:56:16.262224: step 2066, loss 0.029822, acc 1\n",
      "2022-01-26T01:56:16.467035: step 2067, loss 0.0703852, acc 0.96875\n",
      "2022-01-26T01:56:16.654880: step 2068, loss 0.0506853, acc 0.984375\n",
      "2022-01-26T01:56:16.849930: step 2069, loss 0.0268394, acc 1\n",
      "2022-01-26T01:56:17.031347: step 2070, loss 0.0509592, acc 0.984375\n",
      "2022-01-26T01:56:17.205592: step 2071, loss 0.0191477, acc 1\n",
      "2022-01-26T01:56:17.415241: step 2072, loss 0.0960408, acc 0.9375\n",
      "2022-01-26T01:56:17.691111: step 2073, loss 0.0244853, acc 1\n",
      "2022-01-26T01:56:17.888436: step 2074, loss 0.0297814, acc 1\n",
      "2022-01-26T01:56:18.093403: step 2075, loss 0.0347915, acc 0.984375\n",
      "2022-01-26T01:56:18.276987: step 2076, loss 0.0376289, acc 0.984375\n",
      "2022-01-26T01:56:18.489800: step 2077, loss 0.0140175, acc 1\n",
      "2022-01-26T01:56:18.690511: step 2078, loss 0.0919321, acc 0.96875\n",
      "2022-01-26T01:56:18.926055: step 2079, loss 0.0557352, acc 0.984375\n",
      "2022-01-26T01:56:19.173808: step 2080, loss 0.0677405, acc 0.953125\n",
      "2022-01-26T01:56:19.431317: step 2081, loss 0.0354842, acc 1\n",
      "2022-01-26T01:56:19.634509: step 2082, loss 0.0178566, acc 1\n",
      "2022-01-26T01:56:19.827419: step 2083, loss 0.0461605, acc 0.984375\n",
      "2022-01-26T01:56:20.006783: step 2084, loss 0.0681844, acc 0.96875\n",
      "2022-01-26T01:56:20.196880: step 2085, loss 0.0866473, acc 0.96875\n",
      "2022-01-26T01:56:20.368435: step 2086, loss 0.0609762, acc 0.96875\n",
      "2022-01-26T01:56:20.564252: step 2087, loss 0.083101, acc 0.953125\n",
      "2022-01-26T01:56:20.732168: step 2088, loss 0.0555986, acc 0.984375\n",
      "2022-01-26T01:56:20.916961: step 2089, loss 0.0632841, acc 0.96875\n",
      "2022-01-26T01:56:21.080298: step 2090, loss 0.0370536, acc 1\n",
      "2022-01-26T01:56:21.252778: step 2091, loss 0.0893468, acc 0.953125\n",
      "2022-01-26T01:56:21.427920: step 2092, loss 0.044496, acc 0.984375\n",
      "2022-01-26T01:56:21.594560: step 2093, loss 0.0887467, acc 0.96875\n",
      "2022-01-26T01:56:21.761522: step 2094, loss 0.0585611, acc 0.96875\n",
      "2022-01-26T01:56:21.938121: step 2095, loss 0.0172175, acc 1\n",
      "2022-01-26T01:56:22.104821: step 2096, loss 0.109023, acc 0.953125\n",
      "2022-01-26T01:56:22.279382: step 2097, loss 0.07506, acc 0.96875\n",
      "2022-01-26T01:56:22.444669: step 2098, loss 0.0335455, acc 1\n",
      "2022-01-26T01:56:22.612925: step 2099, loss 0.0517513, acc 1\n",
      "2022-01-26T01:56:22.777861: step 2100, loss 0.0480686, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2022-01-26T01:56:23.410744: step 2100, loss 0.780743, acc 0.7475\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/33/49tykqg9645_cd5gw3v25dfh0000gn/T/ipykernel_21279/291173759.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    310\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/33/49tykqg9645_cd5gw3v25dfh0000gn/T/ipykernel_21279/291173759.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/33/49tykqg9645_cd5gw3v25dfh0000gn/T/ipykernel_21279/2874133935.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, y_train, x_dev, y_dev)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mCHECKPOINT_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model checkpoint to {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1171\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1172\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    train(X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "I learned how to use TensorBoard to view my results. It's a very, cool and useful module that I have never gotten to use before, but outputs a lot of useful metrics in an interactive format hosted on a local server. Version control was horrendous to use this module, but I finally figured out it was because I had two versions of TensorBoard downloaded on my computer.\n",
    "<img src = \"./runs/tensorboard_ex.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy\n",
    "I was able to achieve about 75% accuracy after training for approximately 4.2k timesteps. The accuracy begins decreasing after this point due to the model overfitting. I was pretty pleased with these results, though they were lower than the results presented in the Yoon Kim paper of 81%, because I had only trained for about 6 mins. Using pretrained word2vec vectors might improve my model.\n",
    "\n",
    "<img src = \"./runs/test_data_acc.png\" width=70%>\n",
    "\n",
    "#### Training Accuracy\n",
    "I was able to achieve a training accuracy of 100% after 2.56k timesteps.\n",
    "\n",
    "<img src = \"./runs/train_data_acc.png\" width=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.14.0 at http://Allisons-Macbook.local:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir ./runs/1643162486/summaries/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.14.0 at http://Allisons-Macbook.local:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir ./runs/1643162486/summaries/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Remarks\n",
    "I would love to test the performance of this model on other datasets and compare them to those achieved in the Kim Yoon paper. I look forward to seeing the results of my groupmates and I's combined python notebook where we integrate the three models we each explored more in-depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* Yoon Kim paper: https://arxiv.org/pdf/1408.5882.pdf\n",
    "* Tutorial: https://opendatascience.com/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "* Colab Text Classification: https://www.tensorflow.org/tutorials/keras/text_classification\n",
    "* Understanding CNNs for NLP: https://www.kdnuggets.com/2015/11/understanding-convolutional-neural-networks-nlp.html \n",
    "* Sensitivity Analysis of CNNs for Sentence Classification (Zhang et al. 2015): https://arxiv.org/abs/1510.03820"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5c1b2d3ab4fde7fa35d67abb9152f51fa53e03ce00168c6709dca5dc86869dd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
