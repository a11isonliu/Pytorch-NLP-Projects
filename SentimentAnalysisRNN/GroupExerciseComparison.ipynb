{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0f58a8",
   "metadata": {},
   "source": [
    "# Group Exercise: Implementing Simple BOW, CNN, RNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d75504",
   "metadata": {},
   "source": [
    "After completing our individual notebooks, the team members of Reign are sitting down together to teach eachother the implementation skills that we have picked up for Bag of Words, Convolutional Neural Nets, and Recursive Neural Nets. Since we are teaching eachother, we have decided to keep the implementation dataset small and concise for easier understanding of the model manipulations. https://github.com/graykode/nlp-tutorial was a good tutorial to follow as a backbone to us teaching eachother on these simple models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a016bd",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984363a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/n63q8zy901g99zjtbg4hhpb00000gn/T/ipykernel_31537/3484657818.py:57: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  inputs = torch.LongTensor([np.asarray([word_dict[n] for n in sen.split()]) for sen in sentences])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 loss = 0.002421\n",
      "Epoch: 2000 loss = 0.000486\n",
      "Epoch: 3000 loss = 0.000175\n",
      "Epoch: 4000 loss = 0.000077\n",
      "Epoch: 5000 loss = 0.000038\n",
      "tensor([[0]])\n",
      "sorry hate you is Bad Mean...\n",
      "Confidence:  0.8778114478071617\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.W = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.Weight = nn.Linear(self.num_filters_total, num_classes, bias=False)\n",
    "        self.Bias = nn.Parameter(torch.ones([num_classes]))\n",
    "        self.filter_list = nn.ModuleList([nn.Conv2d(1, num_filters, (size, embedding_size)) for size in filter_sizes])\n",
    "\n",
    "    def forward(self, X):\n",
    "        embedded_chars = self.W(X) # [batch_size, sequence_length, sequence_length]\n",
    "        embedded_chars = embedded_chars.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n",
    "\n",
    "        pooled_outputs = []\n",
    "        for i, conv in enumerate(self.filter_list):\n",
    "            # conv : [input_channel(=1), output_channel(=3), (filter_height, filter_width), bias_option]\n",
    "            h = F.relu(conv(embedded_chars))\n",
    "            # mp : ((filter_height, filter_width))\n",
    "            mp = nn.MaxPool2d((sequence_length - filter_sizes[i] + 1, 1))\n",
    "            # pooled : [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3)]\n",
    "            pooled = mp(h).permute(0, 3, 2, 1)\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "        h_pool = torch.cat(pooled_outputs, len(filter_sizes)) # [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3) * 3]\n",
    "        h_pool_flat = torch.reshape(h_pool, [-1, self.num_filters_total]) # [batch_size(=6), output_height * output_width * (output_channel * 3)]\n",
    "        model = self.Weight(h_pool_flat) + self.Bias # [batch_size, num_classes]\n",
    "        return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embedding_size = 2 # embedding size\n",
    "    sequence_length = 3 # sequence length\n",
    "    num_classes = 2 # number of classes\n",
    "    filter_sizes = [2, 2, 2] # n-gram windows\n",
    "    num_filters = 3 # number of filters\n",
    "\n",
    "    # 3 words sentences (=sequence_length is 3)\n",
    "    sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "    labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    word_list = list(set(word_list))\n",
    "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "    vocab_size = len(word_dict)\n",
    "\n",
    "    model = TextCNN()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    inputs = torch.LongTensor([np.asarray([word_dict[n] for n in sen.split()]) for sen in sentences])\n",
    "    targets = torch.LongTensor([out for out in labels]) # To using Torch Softmax Loss function\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "\n",
    "        # output : [batch_size, num_classes], target_batch : [batch_size] (LongTensor, not one-hot)\n",
    "        loss = criterion(output, targets)\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    test_text = 'sorry hate you'\n",
    "    tests = [np.asarray([word_dict[n] for n in test_text.split()])]\n",
    "    test_batch = torch.LongTensor(tests)\n",
    "\n",
    "    # Predict\n",
    "    predict = model(test_batch).data.max(1, keepdim=True)[1]\n",
    "    print(predict)\n",
    "    if predict[0][0] == 0:\n",
    "        print(test_text,\"is Bad Mean...\")\n",
    "    else:\n",
    "        print(test_text,\"is Good Mean!!\")\n",
    "        \n",
    "    import math as m\n",
    "    print('Confidence: ',model(test_batch).data[0][0].item()/m.sqrt((model(test_batch).data[0][0].item()**2)+(model(test_batch).data[0][1].item()**2)))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7c7bf5",
   "metadata": {},
   "source": [
    "The program with CNN is nearly 85% sure that it has correctly labeled the statement \"sorry hate you\" as a negative statement. This results in the correct conclusion! \n",
    "\n",
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed8e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0, 'love': 1, 'you': 2, 'she': 3, 'likes': 4, 'baseball': 5, 'he': 6, 'loves': 7, 'me': 8, 'sorry': 9, 'for': 10, 'that': 11, 'this': 12, 'is': 13, 'awful': 14, 'hate': 15, 'soccer': 16}\n",
      "Parameter containing:\n",
      "tensor([[-0.0288,  0.1272, -0.1876, -0.2333,  0.0304,  0.0265,  0.1044, -0.2144,\n",
      "         -0.1282, -0.1028,  0.1903,  0.1953,  0.2172, -0.1737, -0.0310, -0.2144,\n",
      "          0.1312],\n",
      "        [-0.2393,  0.1496,  0.2219, -0.0713, -0.0452,  0.1456, -0.0363, -0.0342,\n",
      "          0.2089,  0.1294, -0.0661, -0.1787,  0.1558,  0.1872, -0.0813,  0.1612,\n",
      "         -0.1555]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1117, 0.1758], requires_grad=True)\n",
      "TRAINING...\n",
      "tensor([[-0.4975, -0.9366]])\n",
      "tensor([[-1.3733, -0.2920]])\n",
      "tensor([-0.2144,  0.1612], grad_fn=<SelectBackward0>)\n",
      "Final Loss:  tensor(0.0534, grad_fn=<NllLossBackward0>)\n",
      "TESTING...\n",
      "tensor([[-2.1001, -0.1306]])\n",
      "tensor([[-0.0275, -3.6079]])\n",
      "Classifying: hate you sorry\n",
      "Confidence:  0.5812122573455175\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "#Establish training data\n",
    "data = [(\"i love you\".split(), \"Good\"),\n",
    "        (\"she likes baseball\".split(), \"Good\"),\n",
    "        (\"he loves me\".split(), \"Good\"),\n",
    "        (\"sorry for that\".split(), \"Bad\"),\n",
    "        (\"this is awful\".split(), \"Bad\"),\n",
    "        (\"i hate you\".split(), \"Bad\")]\n",
    "\n",
    "#Test Data\n",
    "test_data = [(\"he likes soccer\".split(), \"Good\"),\n",
    "             (\"sorry hate you\".split(), \"Bad\")]\n",
    "\n",
    "#Add words to our dictionary\n",
    "word_to_ix = {}\n",
    "for sentence, _ in data + test_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2 #Good or Bad\n",
    "\n",
    "##########First Define Model###############\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, num_labels) #linear affine map\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        temp=self.linear(bow_vec)\n",
    "        return F.log_softmax(temp, dim=1) # 1 dimention for prob distribution\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1 #count up how many times we hit the word of interest\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]]) #desired classification\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE) #define model\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "##########Now Training the data############\n",
    "print(\"TRAINING...\")\n",
    "label_to_ix = {\"Bad\": 0, \"Good\": 1} #define categories\n",
    "# Run on test data before we train, just to see a before-and-after\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"hate\"]])\n",
    "\n",
    "\n",
    "loss_function = nn.NLLLoss() #define loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1) #define optimizer\n",
    "\n",
    "#Pass over training set each epoch\n",
    "for epoch in range(100):\n",
    "    count=0\n",
    "    for instance, label in data:\n",
    "        model.zero_grad() # Re-clear all gradients before restarting an epoch\n",
    "        \n",
    "        #make vocab vector and define targets\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "\n",
    "        log_probs = model(bow_vec) #run through model\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target) #compute loss\n",
    "        loss.backward() #read gradients\n",
    "        optimizer.step() #update parameters from new gradients\n",
    "        count=count+1\n",
    "    #print('Epoch: ',epoch)\n",
    "    #print('      Total ',count,' Instances.')\n",
    "\n",
    "print(\"Final Loss: \", loss)\n",
    "#################Finally Test Data#######################\n",
    "print(\"TESTING...\")\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "print(\"Classifying: hate you sorry\")\n",
    "word1=next(model.parameters())[:, word_to_ix[\"sorry\"]]\n",
    "word2=next(model.parameters())[:, word_to_ix[\"hate\"]]\n",
    "word3=next(model.parameters())[:, word_to_ix[\"you\"]]\n",
    "print('Confidence: ', (word1[0].item()+word2[0].item()+word3[0].item())/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b13c0",
   "metadata": {},
   "source": [
    "The confidence of Bag of Words correctly identifying the statement as negative is slightly worse at a value under 80%. From the tutrials Simon followed, this is to be expected because BOW is commonly used as a preface or placeholder model when quickly putting together a NN. CNN and likely RNN below are more advanced and respected models. \n",
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0b16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "#General Imports\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "#visualization imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#All Imports for Pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#All Imports for Tensorflow\n",
    "#import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "#import tensorflow_text as text\n",
    "#from official.nlp import optimization  # to create AdamW optimizer\n",
    "#tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "#Dependencies, Downloads, and other necessary installations\n",
    "#!conda install spacy\n",
    "#!pip install wordcloud\n",
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        #self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.nn.functional.softmax(model(tensor), dim=1)\n",
    "    return prediction[0][0].item()\n",
    "\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2084dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-26 11:42:59--  https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08/movie_data.csv.gz [following]\n",
      "--2022-01-26 11:43:00--  https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08/movie_data.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26521894 (25M) [application/octet-stream]\n",
      "Saving to: ‘movie_data.csv.gz’\n",
      "\n",
      "movie_data.csv.gz   100%[===================>]  25.29M  13.7MB/s    in 1.8s    \n",
      "\n",
      "2022-01-26 11:43:02 (13.7 MB/s) - ‘movie_data.csv.gz’ saved [26521894/26521894]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz\n",
    "!gunzip -f movie_data.csv.gz \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c98da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 40000\n",
      "Num Test: 10000\n",
      "Num Train: 34000\n",
      "Num Validation: 6000\n",
      "{'TEXT_COLUMN_NAME': ['For', 'some', 'unknown', 'reason', ',', '7', 'years', 'ago', ',', 'I', 'watched', 'this', 'movie', 'with', 'my', 'mother', 'and', 'sister', '.', 'I', 'do', \"n't\", 'think', 'I', \"'ve\", 'ever', 'laughed', 'as', 'hard', 'with', 'them', 'before', '.', 'This', 'movie', 'was', 'sooooo', 'bad', '.', 'How', 'sequels', 'were', 'produced', 'is', 'beyond', 'me', '.', 'Its', 'been', 'awhile', 'since', 'I', 'last', 'saw', 'this', '\"', 'movie', '\"', ',', 'but', 'the', 'one', 'impression', 'that', 'it', 'has', 'stuck', 'with', 'me', 'over', 'the', 'years', 'has', 'been', ',', '\"', 'They', 'must', 'have', 'found', 'the', 'script', 'in', 'a', 'dumpster', 'in', 'the', 'backlot', 'of', 'a', 'cheap', 'movie', 'studio', ',', 'made', 'into', 'a', '\"', 'movie', '\"', ',', 'and', 'decided', 'that', 'it', 'did', \"n't\", 'suck', 'enough', ',', 'and', 'made', 'it', 'worse', '.', 'I', \"'m\", 'pretty', 'sure', 'that', 'they', 'spent', 'all', 'the', 'budget', 'on', 'camera', 'work', 'and', 'the', 'so', 'called', '\"', 'special', 'effects', '\"', ',', 'and', 'then', 'had', '13', 'cents', 'left', 'toward', 'the', 'script', 'AND', 'to', 'pay', 'the', '\"', 'actors', '\"', '.'], 'LABEL_COLUMN_NAME': '0'}\n",
      "Vocabulary size: 20002\n",
      "Number of classes: 2\n",
      "[('the', 391539), (',', 370979), ('.', 319449), ('and', 210723), ('a', 210397), ('of', 194700), ('to', 180225), ('is', 145689), ('in', 118828), ('I', 105650), ('it', 103414), ('that', 93700), ('\"', 86160), (\"'s\", 83067), ('this', 81424), ('-', 70840), ('/><br', 68865), ('was', 67523), ('as', 57974), ('with', 57558)]\n",
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
      "defaultdict(None, {'1': 0, '0': 1})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 8\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 16        #128  USING THESE DIMENSIONS MAKES MODEL FAR TOO LARGE FOR ME TO FEASIBLY TRAIN\n",
    "HIDDEN_DIM =  24         #256\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "### Defining the feature processing\n",
    "\n",
    "TEXT = torchtext.legacy.data.Field(\n",
    "    tokenize='spacy', # default splits on whitespace\n",
    "    tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "### Defining the label processing\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.long)\n",
    "\n",
    "fields = [('TEXT_COLUMN_NAME', TEXT), ('LABEL_COLUMN_NAME', LABEL)]\n",
    "\n",
    "dataset = torchtext.legacy.data.TabularDataset(\n",
    "    path='movie_data.csv', format='csv',\n",
    "    skip_header=True, fields=fields)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "train_data, test_data = dataset.split(\n",
    "    split_ratio=[0.8, 0.2],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Test: {len(test_data)}')\n",
    "\n",
    "train_data, valid_data = train_data.split(\n",
    "    split_ratio=[0.85, 0.15],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Validation: {len(valid_data)}')\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "print(TEXT.vocab.itos[:10]) # itos = integer-to-string\n",
    "print(LABEL.vocab.stoi)\n",
    "LABEL.vocab.freqs\n",
    "\n",
    "train_loader, valid_loader, test_loader = \\\n",
    "    torchtext.legacy.data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "         batch_size=BATCH_SIZE,\n",
    "         sort_within_batch=False,\n",
    "         sort_key=lambda x: len(x.TEXT_COLUMN_NAME),\n",
    "         device=DEVICE\n",
    "    )\n",
    "\n",
    "#MAYYYYBE TRAIN STUFF HERE\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(input_dim=len(TEXT.vocab),\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=NUM_CLASSES # could use 1 for binary classification\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./bestRNNweights\"))\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85d471",
   "metadata": {},
   "source": [
    "## Ultimate RNN Transfer Learning Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b415f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RNN transfer learning model is:0.9627192541956902 confident that the review above would be negative!\n"
     ]
    }
   ],
   "source": [
    "val = 1-predict_sentiment(model, \"sorry hate you is Bad Mean...\")\n",
    "print(f'The RNN transfer learning model is:{val} confident that the review above would be negative!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05eb83",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The final order of confidance in final classification was Transfer Learning RNN>CNN>BOW.  We recognize that the CNN and BOW datasets here were trained using FAR less data than the transfer learning RNN. Thus our group hopes to do more analysis in a future week to make a more fair comparison.  From above we did solidify that CNN did better on a very small dataset than BOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed89c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
