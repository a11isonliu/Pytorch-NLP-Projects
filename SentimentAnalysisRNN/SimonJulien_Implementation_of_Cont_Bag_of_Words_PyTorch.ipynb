{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simon Initial Investigation & Implementation of Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Pytorch: Tutorial\n",
    "As of this point, I have nearly exclusively worked with TensorFlow. With this being the first week of the class, I have decided I would like to try out using Pytorch to get a feel and develop a preference. Before I dive deeper into an application to the concepts we have learned in class, I have first followed a tutorial (https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html) to get started with PyTorch. Below I have performed some of the concepts from the tutorial, and have also layed out and labeled the main concepts for furture reference (somewhat like and encyclopedia). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7ff95926b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals\n",
    "#### Initializing Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n",
      "tensor([[0.6614, 0.2669],\n",
      "        [0.0617, 0.6213]])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "vec=torch.tensor([1.,2.,3.]) #vector\n",
    "print(vec)\n",
    "mat=torch.tensor([[1.,2.,3.],[4.,5.,6.]]) #matrix\n",
    "print(mat)\n",
    "ten3D=torch.tensor([[[1., 2.], [3., 4.]],[[5., 6.], [7., 8.]]]) #3D tensor\n",
    "print(ten3D)\n",
    "randomV=torch.randn(2, 2) #random tensor\n",
    "print(randomV)\n",
    "zeros=torch.zeros(2,3,4) #zero tensor\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "tensor([1., 2., 3.])\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(vec[0])\n",
    "print(vec[0].item()) #vec\n",
    "print(mat[0])\n",
    "print(ten3D[0,1,1].item())#order of indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([ 4, 10, 18])\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor(21)\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2,3])\n",
    "y=torch.tensor([4,5,6])\n",
    "\n",
    "print(x+y) #addition\n",
    "\n",
    "print(x*y) #Elementwise multiplicaiton\n",
    "\n",
    "temp=torch.cat([x,y]) #concatination\n",
    "print(temp)\n",
    "\n",
    "print(sum(temp)) #summation of elements\n",
    "\n",
    "print(temp.view(2,3)) #reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation Graphs and Automatic Differentiation\n",
    "PyTorch can perform automatic backpropogation gradients as long as the data is specified in how it is connected so gradients can be computed. To flag that we want the data to be followed for gradient determination and backpropogation, we specify \"requires_grad=True\". Notice how the specified grad_fn understands what operation is being processed in the previous computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n",
      "tensor(5., grad_fn=<SelectBackward0>)\n",
      "5.0\n",
      "<AddBackward0 object at 0x7f7fd81dcb50>\n",
      "<SumBackward0 object at 0x7f7fd81dcb50>\n",
      "tensor([1., 1., 1.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1.,2.,3.], requires_grad=True)\n",
    "y=torch.tensor([4.,5.,6.], requires_grad=True)\n",
    "z=x+y\n",
    "\n",
    "print(z) #Now we have information about the gradient funciton for backprop\n",
    "print(z[0])\n",
    "print(z[0].item())\n",
    "print(z.grad_fn) #Add backward\n",
    "s=z.sum()\n",
    "print(s.grad_fn) #Sum backward\n",
    "\n",
    "s.backward() #backprop\n",
    "print(x.grad)\n",
    "\n",
    "z=z.detach() #Detach from history\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Standard Deep Learning Mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7ff95926b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine Maping: Multilayered Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1948,  0.0250, -0.7627,  1.3969, -0.3245],\n",
      "        [ 0.2879,  1.0579,  0.9621,  0.3935,  1.1322]])\n",
      "tensor([[ 0.1755, -0.3268, -0.5069],\n",
      "        [-0.6602,  0.2260,  0.1089]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0000, 0.0250, 0.0000, 1.3969, 0.0000],\n",
      "        [0.2879, 1.0579, 0.9621, 0.3935, 1.1322]])\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3) # Net from layer of dim 5 to dim 3\n",
    "\n",
    "data = torch.randn(2, 5) \n",
    "print(data)\n",
    "print(lin(data))  # linearly mapped to next layer\n",
    "\n",
    "print(F.relu(data)) #non-linearity-- in this case ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax: Turning Tensor into probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5404, -2.2102,  2.1130, -0.0040,  1.3800])\n",
      "tensor([0.0418, 0.0079, 0.5936, 0.0715, 0.2852])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(5)\n",
    "print(data)\n",
    "print(F.softmax(data, dim=0)) #Next layer becomes prob distribution\n",
    "print(F.softmax(data, dim=0).sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tutorial Example: Bag-of-Words Spanish or English (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n",
      "Parameter containing:\n",
      "tensor([[-0.1885, -0.0935,  0.1064, -0.0477,  0.1953,  0.1572, -0.0092, -0.1309,\n",
      "          0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,\n",
      "         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,\n",
      "          0.0644,  0.0431],\n",
      "        [ 0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,  0.0119, -0.0334,\n",
      "          0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,\n",
      "          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,\n",
      "          0.1796, -0.0363]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1106, 0.0849], requires_grad=True)\n",
      "TRAINING...\n",
      "tensor([[-0.6866, -0.6998]])\n",
      "tensor([[-0.8991, -0.5225]])\n",
      "tensor([-0.1268, -0.1743], grad_fn=<SelectBackward0>)\n",
      "TESTING...\n",
      "tensor([[-0.1496, -1.9739]])\n",
      "tensor([[-3.0060, -0.0508]])\n",
      "Classifying: creo\n",
      "tensor([ 0.2876, -0.5887], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Establish training data\n",
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "#Test Data\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "#Add words to our dictionary\n",
    "word_to_ix = {}\n",
    "for sentence, _ in data + test_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2 #Spanish or English\n",
    "\n",
    "##########First Define Model###############\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, num_labels) #linear affine map\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        temp=self.linear(bow_vec)\n",
    "        return F.log_softmax(temp, dim=1) # 1 dimention for prob distribution\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1 #count up how many times we hit the word of interest\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]]) #desired classification\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE) #define model\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "##########Now Training the data############\n",
    "print(\"TRAINING...\")\n",
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1} #define categories\n",
    "# Run on test data before we train, just to see a before-and-after\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "\n",
    "loss_function = nn.NLLLoss() #define loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1) #define optimizer\n",
    "\n",
    "#Pass over training set each epoch\n",
    "for epoch in range(100):\n",
    "    count=0\n",
    "    for instance, label in data:\n",
    "        model.zero_grad() # Re-clear all gradients before restarting an epoch\n",
    "        \n",
    "        #make vocab vector and define targets\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "\n",
    "        log_probs = model(bow_vec) #run through model\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target) #compute loss\n",
    "        loss.backward() #read gradients\n",
    "        optimizer.step() #update parameters from new gradients\n",
    "        count=count+1\n",
    "    #print('Epoch: ',epoch)\n",
    "    #print('      Total ',count,' Instances.')\n",
    "\n",
    "#################Finally Test Data#######################\n",
    "print(\"TESTING...\")\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "print(\"Classifying: creo\")\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above exercise. We can see that the final probability of confidence index increased for spanish once we trained the data! The probability is still low because we have used a very simple model (linear reg), a very elementary loss function function (negative log likelihood), and a low level optimizer (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Continuous Bag of Words: Changing loss functions\n",
    "After following the tutorial and implementing the simple NN above, I want to round out my skills and realizations by implementing another NN and comparing what happens when I change optimizers using PyTorch. To do this I will be implementing the Continuous Bag of Words (CBOW) model from class that predicts words given the context of a few words before and after the target word. Often times, CBOW is used to quickly train word embeddings as initializaitons of more complex embeddings (pretraining). By \"embeddings\" I mean the computer interpretation (by some vector of numbers) to our target words and likely their trained meaning and context. The function used in CBOW is show below where $q_w$ is the embedding for word w. $$-log[p(w_{i}|C)] = -log [\\text{Softmax}(A(\\sum_{w\\in C}q_{w})+b)]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Implementation\n",
    "The following code is a solution to the problem prompted by the last example in https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['are', 'We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n",
      "Raw Text: We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\n",
      "\n",
      "Context: ['conjure', 'the', 'of', 'the']\n",
      "\n",
      "Prediction: spirits\n",
      "Confidence: 75%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix): #make vector of surrounding words\n",
    "    idxs = [word_to_ix[w] for w in context] \n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "EMDEDDING_DIM = 100\n",
    "\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "vocab = set(raw_text) #Make vocab dictionary as subset of full raw text\n",
    "vocab_size = len(vocab)\n",
    "#print(len(raw_text))\n",
    "#print(len(vocab))\n",
    "\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)} #associate word with index of vocab vec\n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "#create training data\n",
    "data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1) #make linear vector\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)\n",
    "\n",
    "\n",
    "model = CBOW(vocab_size, EMDEDDING_DIM) #Model is above using linear and ReLU like I previously practiced in PyTorch\n",
    "loss_function = nn.NLLLoss() #Negative log likelihood loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001) #SGD Optimizer\n",
    "\n",
    "#TRAINING\n",
    "for epoch in range(50):\n",
    "    total_loss = 0 #count up total loss on the epoch\n",
    "\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)  \n",
    "        log_probs = model(context_vector)\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "\n",
    "    optimizer.zero_grad() #reset gradient\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#TESTING\n",
    "#context = ['People','create','to', 'direct']\n",
    "#context=['other','abstract','called','data.']\n",
    "context=['conjure','the','of','the']\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "\n",
    "#Print result\n",
    "print(f'Raw Text: {\" \".join(raw_text)}\\n')\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')\n",
    "conf=100*m.exp(max(a[0]))\n",
    "print(f'Confidence: {m.trunc(conf)}%')\n",
    "\n",
    "#PLOT\n",
    "wordVec=[]\n",
    "for i in range(len(a[0])):\n",
    "    wordVec.append(i)\n",
    "plotVec=a.detach().numpy()\n",
    "plt.bar(wordVec,100*np.exp(plotVec[0]))\n",
    "plt.title('Probability Distribution')\n",
    "plt.xlabel('Embedded word in vocabulary')\n",
    "plt.ylabel('Probability (Confidence)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully building this NN model, training, and testing the results, I am now interested in what changing the different components of the NN I built. This is done below by replacing the loss function and optimizers by ones I know to be generally more desireable.\n",
    "\n",
    "### Replacing SGD with Adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['are', 'We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n",
      "Raw Text: We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\n",
      "\n",
      "Context: ['conjure', 'the', 'of', 'the']\n",
      "\n",
      "Prediction: spirits\n",
      "Confidence: 94%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math as m\n",
    "\n",
    "def make_context_vector(context, word_to_ix): \n",
    "    idxs = [word_to_ix[w] for w in context] \n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "CONTEXT_SIZE = 2 \n",
    "EMDEDDING_DIM = 100\n",
    "\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "#print(len(raw_text))\n",
    "#print(len(vocab))\n",
    "\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)} \n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "#create training data\n",
    "data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1) \n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)\n",
    "\n",
    "\n",
    "model = CBOW(vocab_size, EMDEDDING_DIM) \n",
    "loss_function = nn.NLLLoss() #\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #Adam Optimizer\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0 \n",
    "\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)  \n",
    "        log_probs = model(context_vector)\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "#context = ['People','create','to', 'direct']\n",
    "#context=['other','abstract','called','data.']\n",
    "context=['conjure','the','of','the']\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "\n",
    "#Print result\n",
    "print(f'Raw Text: {\" \".join(raw_text)}\\n')\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')\n",
    "conf=100*m.exp(max(a[0]))\n",
    "print(f'Confidence: {m.trunc(conf)}%')\n",
    "\n",
    "wordVec=[]\n",
    "for i in range(len(a[0])):\n",
    "    wordVec.append(i)\n",
    "plotVec=a.detach().numpy()\n",
    "plt.bar(wordVec,100*np.exp(plotVec[0]))\n",
    "plt.title('Probability Distribution')\n",
    "plt.xlabel('Embedded word in vocabulary')\n",
    "plt.ylabel('Probability (Confidence)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the use of Adam increased our confidence about 20%, which was expected.\n",
    "### Replacing SGD with RMSProp optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['are', 'We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n",
      "Raw Text: We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\n",
      "\n",
      "Context: ['conjure', 'the', 'of', 'the']\n",
      "\n",
      "Prediction: spirits\n",
      "Confidence: 99%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeyUlEQVR4nO3dd7RcVd3/8fcnhZpQQkKMEAhgaKIUQ1FAqSoKBHxEQdCAKAvkEfxZMLhUiqKxoQKiRin5AQooIGh+jxoDkSLCkwQEQjGUEEpMLkUgVIHv74+9B+Zc5s6dW2bOzZ3Pa61Zc84+7bvPXXe+c/Y5s7ciAjMzs4ohZQdgZmYDixODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgx2IAkKSS9pZfbLpK0VxfLdpV0T611JX1F0i97F3Gv4txA0nJJQ/tpfz+T9LU8vZukh/tjv3l/hfNmg5sTg/Wb/CH7fP6wWyrpPEkjyo6rWkRcFxGbdbHsWxHxKQBJE3JyGtab40g6XNIr+Vwsl/RAPh+bVh1vcUSMiIhXGtjX9d0dMyKOjohv9CbeGscsJOZ6580GHycG62/7RcQIYDtge+CrnVfo7YftCujGfC7WBPYCngfmSdqqvw/UX1cdZuDEYE0SEY8A/wNsBa99Az1W0kJgYS77tKR7JT0h6SpJb+60mw9Iul/SY5K+J2lI3m4TSVdLejwvu0jSWp223V7SnZKezN/UV8nbdtnEIulkSRfm2Wvz+7/zN/735DjfVrX+uvkKaUw35+KViLgvIj4D/BU4OW9fuCrJVwb3S3omX2EcKmkL4GfAO3Mc/87rni/pp5L+n6Rngd1z2Tc71ekr+RwtknRoVfkcSZ+qmn/tqkRSpe7/yMf8aOfzJmmLvI9/S1ogaf+qZedL+omkmbkuN0napN45soHFicGaQtJ44APALVXFBwA7AltK2gP4NvARYBzwIHBxp90cCEwiXX1MBj5Z2X3e9s3AFsB48odtlUOB9wGbAJtS48qlG+/O72vl5p6/5vgOq1rnEOAvEdHRg/1eDuzauVDS6sAZwD4RMRJ4F3BrRNwFHE2++oiI6gT4MeA0YCRQq6npTcBoYD1gCjBdUrfNQRFRqfvW+ZiXdIp1OPB74M/AusBngYs67fsQ4BRgbeDeHKetIJwYrL/9Ln+rvZ707fhbVcu+HRFPRMTzpA/ucyNifkS8CJxI+lY8oWr97+T1FwM/In3YEBH3RsSsiHgxfyifDrynUxxnRcRDEfEE6UPpkH6o2wzgY5UrF+DjwAU93MejwKgulr0KbCVp1YhYEhELutnXlRFxQ0S8GhEvdLHO1/J5+iswk5SI+2onYAQwLSJeioirgT9QPMeXR8TNEfEycBGwTT8c11rEicH62wERsVZEbBgRn8lJoOKhquk3k64SAIiI5cDjpG+3tdZ/MG9TacK5WNIjkp4GLiR9M6a7bfsiIm4CngXeI2lz4C3AVT3czXrAEzX2/SzwUdLVwZLcDLN5N/t6qJvlT+b9VvTLecj7eCgiXu207+q/3b+qpp8jJRJbQTgxWCtVd+X7KLBhZSY3pawDPFK1zviq6Q3yNpCakQJ4e0SsQWreUadjdbVtb2KtNiMf7+PAb+t8U+/KgcB1NQ8Y8aeI2JvUtHY38ItuYumua+S183mtqD4PzwKrVS17Uzf7qvYoML7qyqmy70e6WN9WME4MVpZfAUdI2kbSyqQmp5siYlHVOl+StHa+X3E8UGnrHgksJ90YXg/4Uo39HytpfUmjgK9UbduoDlLTzsadyi8gfbgfBvzfRnYkaaikjSSdCexGanvvvM5YSfvnD/IXSfWrPMa6FFhf0ko9rAPAKZJWkrQrsC/wm1x+K/AhSavlx1KP7LTdUt5Y94rKldMJkoZL2g3YjzfeI7IVlBODlSIiZgNfAy4DlpBuEh/cabUrgXmkD7GZwDm5/BTSDemncvnlNQ7xK9LN0fvz65s11qkX33OkexM35CdvdsrlDwPzSd/Wa37zr/JOScuBp4E5wBrA9hFxe411hwBfIH0bf4J0z+QzednVwALgX5Ie60E1/gU8mfd5EXB0RNydl/0QeImUAGbk5dVOBmbkuhfuS0TES8D+wD7AY8DZwCeq9m0rOHmgHrOekXQu8GhE9PRJJ7MVQrv80MisX+Snpj4EbFtuJGbN46YkswZJ+gZwB/C9iHig7HjMmsVNSWZmVuArBjMzK1ih7zGMHj06JkyYUHYYZmYrlHnz5j0WEV328bVCJ4YJEyYwd+7cssMwM1uhSHqw3nI3JZmZWYETg5mZFTgxmJlZgRODmZkVNC0xSDpX0jJJd1SVjZI0S9LC/L521bITlUbzukfS+5oVl5mZ1dfMK4bzgfd3KpsKzI6IicDsPI+kLUkdqL01b3O2PIatmVkpmpYYIuJa3jggyWRST47k9wOqyi/OI009QBoKcIdmxWZmZl1r9T2GsRGxBCC/r5vL16M4GtXDFEeDeo2koyTNlTS3o6MnQ+2amVkjBsrN586jb0EXo1NFxPSImBQRk8aM6fKHe2Zm1kut/uXzUknjImKJpHHAslz+MMWhGNen50MxmpkNeBOmznxD2aJpHywhkq61+orhKmBKnp5CGqGrUn6wpJUlbQRMBG5ucWxmZkYTrxgk/Zo0vu1oSQ8DJwHTgEslHQksBg4CiIgFki4F7gReBo6NiFdq7tjMzJqqaYkhIg7pYtGeXax/GmmMXTMzK9FAuflsZmYDhBODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVlBKYlB0v+RtEDSHZJ+LWkVSaMkzZK0ML+vXUZsZmbtruWJQdJ6wHHApIjYChgKHAxMBWZHxERgdp43M7MWK6spaRiwqqRhwGrAo8BkYEZePgM4oKTYzMzaWsOJQdLqkob29YAR8QjwfWAxsAR4KiL+DIyNiCV5nSXAul3EcZSkuZLmdnR09DUcMzPrpMvEIGmIpI9JmilpGXA3sCTfG/iepIm9OWC+dzAZ2Ah4M7C6pMMa3T4ipkfEpIiYNGbMmN6EYGZmddS7YrgG2AQ4EXhTRIyPiHWBXYG/A9N68oFeZS/ggYjoiIj/AJcD7wKWShoHkN+X9WLfZmbWR8PqLNsrf3AXRMQTwGXAZZKG9+KYi4GdJK0GPA/sCcwFngWmANPy+5W92LeZmfVRl4mhOilI2gWYGBHnSRoDjIiIB2olju5ExE2SfgvMB14GbgGmAyOASyUdSUoeB/V032Zm1nf1rhgAkHQSMAnYDDgPGA5cCOzc24NGxEnASZ2KXyRdPZiZWYkaeSrpQGB/UlMPEfEoMLKZQZmZWXkaSQwvRUQAAemx1eaGZGZmZWokMVwq6efAWpI+DfwF+EVzwzIzs7J0e48hIr4vaW/gadJ9hq9HxKymR2ZmZqVo5ObzRsB1lWQgaVVJEyJiUbODMzOz1mukKek3wKtV86/kMjMzG4QaSQzDIuKlykyeXql5IZmZWZkaSQwdkvavzEiaDDzWvJDMzKxM3d5jAI4GLpJ0FiDgIeATTY3KzMxK08hTSfeR+jYaASginml+WGZmVpZGnkpaGfgvYAIwTBIAEXFqUyMzM7NSNNKUdCXwFDCP1J+RmZkNYo0khvUj4v1Nj8TMzAaERp5K+puktzU9EjMzGxAauWLYBThc0gOkpiQBERFvb2pkZmZWikYSwz5Nj8LMzAaMbpuSIuJBYDywR55+rpHtzMxsxdTtB3wewe3LwIm5qDKCm5mZDUIewc3MzAo8gpuZmRV4BDczMyvwCG5mZlbQyOOq5ETgZGBm1ga6TAySniHfV6glItZoSkRmZlaqLhNDRIwEkHQq8C/gAtKvng/FTyWZmQ1ajdx8fl9EnB0Rz0TE0xHxU1I33GZmNgg1khhekXSopKGShkg6FHil2YGZmVk5GkkMHwM+AizNr4NymZmZDUKNPK66CJjc/FDMzGwgqPdU0gkR8V1JZ1Lj6aSIOK6pkZmZWSnqXTHcmd/ntiIQMzMbGOolho8CfwDWiogftygeMzMrWb2bz++QtCHwSUlrSxpV/erLQSWtJem3ku6WdJekd+b9zpK0ML+v3ZdjmJlZ79RLDD8D/ghsDszr9Opr89KPgT9GxObA1sBdwFRgdkRMBGbneTMza7EuE0NEnBERWwDnRsTGEbFR1Wvj3h5Q0hrAu4Fz8nFeioh/k558mpFXmwEc0NtjmJlZ7zXyuOoxkoYCY6vXj4jFvTzmxkAHcJ6krUlXIMcDYyNiSd73Eknr9nL/ZmbWB40M7fnfpB+2zQJm5tcf+nDMYcB2wE8jYlvSyHANNxtJOkrSXElzOzo6+hCGmZnV0sgvnz8HbBYRb42It+XX2/twzIeBhyPipjz/W1KiWCppHEB+X1Zr44iYHhGTImLSmDFj+hCGmZnV0khieAh4qr8OGBH/Ah6StFku2pP0m4mrgCm5bApwZX8d08zMGtfIQD33A3MkzQRerBRGxOl9OO5ngYskrZT3fwQpSV0q6UhgMalPJjMza7FGEsPi/Fopv/osIm4FJtVYtGd/7N/MzHqvkaeSTgGQNDLNxvKmR2VmZqVp5KmkrSTdAtwBLJA0T9Jbmx+amZmVoZGbz9OBz0fEhhGxIfAF4BfNDcvMzMrSSGJYPSKuqcxExBxg9aZFZGZmpWroqSRJXwMuyPOHAQ80LyQzMytTI1cMnwTGAJfn12jS46VmZjYI1RvBbRVgZER0AMdVlY8Fnm9BbGZmVoJ6VwxnALvWKN8L+GFzwjEzs7LVSwy7RMTlnQsj4iJSt9lmZjYI1UsM6uV2Zma2Aqv3Ab9M0g6dCyVtTxpPwczMBqF6j6t+idSp3fmkwXQg9W/0CeDgJsdlZmYlqTe0583ADqQmpcPzS8COVWMpmJnZIFP3B24RsQw4qUWxmJnZANDlFYOk30vaT9LwGss2lnSqpE82NzwzM2u1elcMnwY+D/xI0hOkG86rABOA+4CzIsKjrJmZDTJdJoY8BOcJwAmSJgDjSL94/mdEPNeS6MzMrOUa6USPiFgELGpqJGZmNiD4h2pmZlbgxGBmZgWNDO25ryQnEDOzNtHIB/7BwEJJ35W0RbMDMjOzcnWbGCLiMGBb0iOq50m6UdJRkkY2PTozM2u5hpqIIuJp4DLgYtJjqwcC8yV9tomxmZlZCRq5x7C/pCuAq4HhwA4RsQ+wNfDFJsdnZmYt1sjvGD4M/DAirq0ujIjn3CWGmdng00hT0pLOSUHSdwAiYnZTojIzs9I0khj2rlG2T38HYmZmA0OXTUmSjgE+A2wi6baqRSOBG5odmJmZlaPePYZfAf8DfBuYWlX+TEQ80dSozMysNPUSQ0TEIknHdl4gaZSTg5nZ4NTdFcO+pPGegzSsZ0UAGzcxLjMzK0m98Rj2ze8btS4cMzMrW72bz9vV2zAi5vflwJKGAnOBRyJiX0mjgEtII8QtAj4SEU/25RhmZtZz9ZqSflBnWQB79PHYxwN3AWvk+anA7IiYJmlqnv9yH49hZmY9VK8pafdmHVTS+sAHgdNI40oDTAZ2y9MzgDk4MZiZtVy9pqQ9IuJqSR+qtTwiLu/DcX9EGk+6uofWsRGxJO97iaR1u4jrKOAogA022KAPIZiZWS31mpLeQ+o4b78aywLoVWKQtC+wLCLmSdqtp9tHxHRgOsCkSZOiNzGYmVnX6jUlnZTfj+jnY+4M7C/pA8AqwBqSLgSWShqXrxbGAcv6+bhmZtaARrrdXkfSGZLmS5on6ceS1untASPixIhYPyImkEaHuzoPBnQVMCWvNgW4srfHMDOz3mukE72LgQ7gv0hdcHeQHivtb9OAvSUtJHXcN60JxzAzs240Mh7DqIj4RtX8NyUd0B8Hj4g5pKePiIjHgT37Y79mZtZ7jVwxXCPpYElD8usjwMxmB2ZmZuWo97jqM7zeR9LngQvzoiHAcuCkpkdnZmYtV++ppJFdLTMzs8GrkXsMSFobmEh6vBSAzsN9mpnZ4NBtYpD0KVK/RusDtwI7ATfS976SzMxsAGrk5vPxwPbAg7n/pG1Jj6yamdkg1EhieCEiXgCQtHJE3A1s1tywzMysLI3cY3hY0lrA74BZkp4EHm1uWGZmVpZuE0NEHJgnT5Z0DbAm8MemRmVmZqVp9Kmk7YBdSL9ruCEiXmpqVGZmVppGOtH7OmngnHWA0cB5kr7a7MDMzKwcjVwxHAJsW3UDehowH/hmMwMzM7NyNPJU0iKqftgGrAzc15RozMysdPX6SjqTdE/hRWCBpFl5fm/g+taEZ2ZmrVavKWlufp8HXFFVPqdp0ZiZWenqdaI3ozItaSVg0zx7T0T8p9mBmZlZORrpK2k30lNJi0hdcI+XNMWd6JmZDU6NPJX0A+C9EXEPgKRNgV8D72hmYGZmVo5GnkoaXkkKABHxT2B480IyM7MyNXLFME/SOcAFef5Q0g1pMzMbhBpJDEcDxwLHke4xXAuc3cygzMysPHUTg6QhwLyI2Ao4vTUhmZlZmereY4iIV4F/SNqgRfGYmVnJGmlKGkf65fPNwLOVwojYv2lRmZlZaRpJDKc0PQozMxsw6vWVtArpxvNbgNuBcyLi5VYFZmZm5ah3j2EGMImUFPYh/dDNzMwGuXpNSVtGxNsA8u8Ybm5NSGZmVqZ6VwyvdZTnJiQzs/ZR74pha0lP52kBq+Z5ARERazQ9OjMza7l63W4PbWUgZmY2MDTSiZ6ZmbURJwYzMytoeWKQNF7SNZLukrRA0vG5fJSkWZIW5ve1Wx2bmZmVc8XwMvCFiNgC2Ak4VtKWwFRgdkRMBGbneTMza7GWJ4aIWBIR8/P0M8BdwHrAZNKP6sjvB7Q6NjMzK/keg6QJwLbATcDYiFgCKXkA63axzVGS5kqa29HR0apQzczaRmmJQdII4DLgcxHxdHfrV0TE9IiYFBGTxowZ07wAzczaVCmJQdJwUlK4KCIuz8VLJY3Ly8cBy8qIzcys3ZXxVJKAc4C7IqJ6VLirgCl5egpwZatjMzOzxsZj6G87Ax8Hbpd0ay77CjANuFTSkcBi4KASYjMza3stTwwRcT2pv6Va9mxlLGZm9kb+5bOZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRU4MZiZWYETg5mZFQy4xCDp/ZLukXSvpKllx2Nm1m4GVGKQNBT4CbAPsCVwiKQty43KzKy9DCs7gE52AO6NiPsBJF0MTAbuLDUq69KEqTPfULZo2gdLiKR9+Jxbsw20xLAe8FDV/MPAjtUrSDoKOCrPLpd0Tz8cdzTwWD/sZ0XTlHrrO/29x6YYVH/zHpzzQVXvHhqwdW/y/0ytem9Yb4OBlhhUoywKMxHTgen9elBpbkRM6s99rgjatd7QvnVv13pD+9a9N/UeUPcYSFcI46vm1wceLSkWM7O2NNASw/8CEyVtJGkl4GDgqpJjMjNrKwOqKSkiXpb038CfgKHAuRGxoAWH7temqRVIu9Yb2rfu7VpvaN+697jeioju1zIzs7Yx0JqSzMysZE4MZmZW0NaJoZ2635B0rqRlku6oKhslaZakhfl97TJjbAZJ4yVdI+kuSQskHZ/L26Huq0i6WdI/ct1PyeWDvu6QelKQdIukP+T5QV9vSYsk3S7pVklzc1mP6922iaENu984H3h/p7KpwOyImAjMzvODzcvAFyJiC2An4Nj8d26Hur8I7BERWwPbAO+XtBPtUXeA44G7qubbpd67R8Q2Vb9d6HG92zYxUNX9RkS8BFS63xiUIuJa4IlOxZOBGXl6BnBAS4NqgYhYEhHz8/QzpA+K9WiPukdELM+zw/MraIO6S1of+CDwy6riQV/vLvS43u2cGGp1v7FeSbGUZWxELIH0AQqsW3I8TSVpArAtcBNtUvfcnHIrsAyYFRHtUvcfAScAr1aVtUO9A/izpHm5+yDoRb0H1O8YWqzb7jds8JA0ArgM+FxEPC3V+vMPPhHxCrCNpLWAKyRtVXZMzSZpX2BZRMyTtFvZ8bTYzhHxqKR1gVmS7u7NTtr5isHdb8BSSeMA8vuykuNpCknDSUnhooi4PBe3Rd0rIuLfwBzSfabBXvedgf0lLSI1Ee8h6UIGf72JiEfz+zLgClKTeY/r3c6Jwd1vpPpOydNTgCtLjKUplC4NzgHuiojTqxa1Q93H5CsFJK0K7AXczSCve0ScGBHrR8QE0v/11RFxGIO83pJWlzSyMg28F7iDXtS7rX/5LOkDpLbISvcbp5UcUtNI+jWwG6kL3qXAScDvgEuBDYDFwEER0fkG9QpN0i7AdcDtvN7e/BXSfYbBXve3k242DiV9Cbw0Ik6VtA6DvO4VuSnpixGx72Cvt6SNSVcJkG4T/CoiTutNvds6MZiZ2Ru1c1OSmZnV4MRgZmYFTgxmZlbgxGBmZgVODGZmVuDEYD0i6ZXcc2Pl1XBHZJJ2q/R02ctjd7l97lVydA/2dbiks3obS09JmiPpDQOyS/rlQOq8UdLy7tcqrN+nv6kNTO3cJYb1zvMRsU3ZQQxkkoZFxMuNrBsRn2p2PANJT86NlcdXDNYv8jf2b0m6UdJcSdtJ+pOk+yQdXbXqGpKukHSnpJ9JGpK3f2/edr6k3+S+jSpjZtwt6XrgQ1XHW0fSn3N/+z+nqu8rSYflcQhulfTz3MU6ko6Q9E9JfyV1m1CrHrdLWkvJ45I+kcsvkLSX0hgH5+X1bpG0e15+eI7796ROzFaVdLGk2yRdAqzaxfFeu5KQtFzSaUrjJ/xd0thO6w7J53mtqrJ7JY2VtKGk2fl4syVtkJePzef7H/n1rlz+O6WO1hbo9c7WKvv8Qf47zJY0pkaco5W6m+hclx0k/S2fl79J2qyLc3OBpMlV210kaf9a58dKEhF++dXwC3gFuLXq9dFcvgg4Jk//ELgNGAmMIXVoBumX1y8AG5N+jTsL+DDp19jXAqvn9b4MfB1YhdQD7kTSB/+lwB/yOmcAX8/THyR1gDga2AL4PTA8Lzsb+AQwjvSrzzHASsANwFk16vezvL+tSN2m/CKXLwRGAF8Azstlm+d9rgIcTup/a1Re9nnSr+kB3k4aF2JSjePNqZTnOuyXp78LfLXG+j8GjsjTOwJ/ydO/B6bk6U8Cv8vTl5A6DiSf8zXzdCXOVUndJqxTFcOhefrrlXPUKc7RwKKqv2nlb7IGMCxP7wVclqc7n5v3VMW3JvBAZTu/BsbLTUnWU/Wakip9Td0OjIg0/sEzkl6o+pZ7c0TcD69107ELKVlsCdyg1OvpSsCNpA/eByJiYV7/QqDy7fbd5CuIiJgp6clcvifwDuB/875WJXUatiMwJyI68r4uATatUYfr8r4fBH4KHCVpPeCJiFiu1MXGmfm4d0t6sGo/s+L1rgbeTUpeRMRtkm7r4pxVewmotNfPA/ausc4lpA/s80j9AF2Sy9/J61dUF5ASC8AepMRIpJ5Wn8rlx0k6ME+PJyXfx0ndhlT2eSFQ6XSwEWsCMyRNJCWY4VXLXjs3EfFXST9R6gH0Q6QE4ualAcRNSdafXszvr1ZNV+YrX0I698ESpKuBWZFGndomIraMiCO7WL/ztp0JmFG1r80i4uQG9lVxLbBrfs0BOkhXNddV7b8rzzYQXz3/ifw1mnRlVuuL243AW3ITzwF0/cHd5bGV+g/aC3hnpNHdbiFd9dTbz8u8/nnR1brfAK6JiK2A/Tqt1/ncXAAcChxBSnI2gDgxWKvtoNSj7RDgo8D1wN+BnSW9BUDSapI2JfUEupGkTfK2h1Tt51rSBwuS9gEq49jOBj6cv41WxrvdkNRp3m753sRw4KBawUXEQ6Smkon5yuZ64Iu8nhiqj7spqWOye2rsqnq9rUjNSX2WE8cVwOmkHmMfz4v+RrqCIB/3+jw9GzgmxzFU0hqkb/ZPRsRzkjYnDXlaMYSUCAE+VrWfRaQrMaqWd7Ym8EiePrybqpwPfC7XaUE361qLOTFYT62q4uOq03q4/Y3ANFK79gPAFbl553Dg17nJ5e/A5hHxAqnpaGa++fxg1X5OAd4taT6pe+HFABFxJ/BV0k3O20j3McZFGrnq5Hz8vwDz68R4E/DPPH0daWS/ygfk2cBQSbeTmlwOj4gX37gLfgqMyDGcANzc/alp2CXAYbze5ANwHHBEPt7HSeMdk993z/HOA94K/BEYltf9Bul8VzwLvFXSPFIz1Km5/PvAMZL+RkqctXwX+LakG0j3M7oUEUtJw6z6amEAcu+qZtZyklYj3YvaLiKe6m59ay1fMZhZS0mqDBh0ppPCwOQrBjMzK/AVg5mZFTgxmJlZgRODmZkVODGYmVmBE4OZmRX8fxLQtp1r0kblAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math as m\n",
    "\n",
    "def make_context_vector(context, word_to_ix): \n",
    "    idxs = [word_to_ix[w] for w in context] \n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "CONTEXT_SIZE = 2 \n",
    "EMDEDDING_DIM = 100\n",
    "\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "#print(len(raw_text))\n",
    "#print(len(vocab))\n",
    "\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)} \n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "#create training data\n",
    "data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1) \n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)\n",
    "\n",
    "\n",
    "model = CBOW(vocab_size, EMDEDDING_DIM) \n",
    "loss_function = nn.NLLLoss() \n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001) #RMSProp\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0 \n",
    "\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)  \n",
    "        log_probs = model(context_vector)\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "#context = ['People','create','to', 'direct']\n",
    "#context=['other','abstract','called','data.']\n",
    "context=['conjure','the','of','the']\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "\n",
    "#Print result\n",
    "print(f'Raw Text: {\" \".join(raw_text)}\\n')\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')\n",
    "conf=100*m.exp(max(a[0]))\n",
    "print(f'Confidence: {m.trunc(conf)}%')\n",
    "\n",
    "\n",
    "wordVec=[]\n",
    "for i in range(len(a[0])):\n",
    "    wordVec.append(i)\n",
    "plotVec=a.detach().numpy()\n",
    "plt.bar(wordVec,100*np.exp(plotVec[0]))\n",
    "plt.title('Probability Distribution')\n",
    "plt.xlabel('Embedded word in vocabulary')\n",
    "plt.ylabel('Probability (Confidence)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMS Prop was even more confident!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Now that I have thuroughly investigated the fundamentals of PyTorch and Continuous Bag of Words, I will bring my skills into collaboration with the whole Reign Team on our collaborative portion of the project. With my knowledge of \"pretraining\" data with CBOW, I can integrate CBOW with a CNN or RNN and see how this improves the training efficiency and accuracy of the models we create. Another cool application that I did not get to implemenet this week is comparing CBOW to Skip-Gram, which essentially follows the opposite process. I prioritized CNN and RNN integrations here because I know from experience these are the more important concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
