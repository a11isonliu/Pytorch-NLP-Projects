{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23d6c4a",
   "metadata": {},
   "source": [
    "# Note that Everything in this notebook until the markdown cell that says \"Part 1: Training RNN With Varying Hyperparameters (Altering Hidden Layer Sizes Etc.)\" is repeated from the other notebook and doesn't need close attention. \n",
    "\n",
    "# However after this part 1 cell is where I have tried different hyperparameter combinations to get a converging RNN model for the dataset.  At the end of this notebook I successfully save my best models weights to be used in the other jupyter notebook without training a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a8dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575556e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 16        #128\n",
    "HIDDEN_DIM =  24         #256\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a539a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-26 00:09:41--  https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08/movie_data.csv.gz [following]\n",
      "--2022-01-26 00:09:42--  https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08/movie_data.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26521894 (25M) [application/octet-stream]\n",
      "Saving to: ‘movie_data.csv.gz’\n",
      "\n",
      "movie_data.csv.gz   100%[===================>]  25.29M  13.8MB/s    in 1.8s    \n",
      "\n",
      "2022-01-26 00:09:44 (13.8 MB/s) - ‘movie_data.csv.gz’ saved [26521894/26521894]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a5c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -f movie_data.csv.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96751ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "0      In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
      "1      OK... so... I really like Kris Kristofferson a...          0\n",
      "2      ***SPOILER*** Do not read this, if you think a...          0\n",
      "3      hi for all the people who have seen this wonde...          1\n",
      "4      I recently bought the DVD, forgetting just how...          0\n",
      "...                                                  ...        ...\n",
      "49995  OK, lets start with the best. the building. al...          0\n",
      "49996  The British 'heritage film' industry is out of...          0\n",
      "49997  I don't even know where to begin on this one. ...          0\n",
      "49998  Richard Tyler is a little boy who is scared of...          0\n",
      "49999  I waited long to watch this movie. Also becaus...          1\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "print(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e4ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the feature processing\n",
    "\n",
    "TEXT = torchtext.legacy.data.Field(\n",
    "    tokenize='spacy', # default splits on whitespace\n",
    "    tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "### Defining the label processing\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f321f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('TEXT_COLUMN_NAME', TEXT), ('LABEL_COLUMN_NAME', LABEL)]\n",
    "\n",
    "dataset = torchtext.legacy.data.TabularDataset(\n",
    "    path='movie_data.csv', format='csv',\n",
    "    skip_header=True, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77bfe295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 40000\n",
      "Num Test: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = dataset.split(\n",
    "    split_ratio=[0.8, 0.2],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e2bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 34000\n",
      "Num Validation: 6000\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = train_data.split(\n",
    "    split_ratio=[0.85, 0.15],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Validation: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "772fbb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94832978",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = \\\n",
    "    torchtext.legacy.data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "         batch_size=BATCH_SIZE,\n",
    "         sort_within_batch=False,\n",
    "         sort_key=lambda x: len(x.TEXT_COLUMN_NAME),\n",
    "         device=DEVICE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7177674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([1136, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([55, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([52, 128])\n",
      "Target vector size: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n",
    "    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n",
    "    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n",
    "    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d2100fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        #self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c823d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(input_dim=len(TEXT.vocab),\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=NUM_CLASSES # could use 1 for binary classification\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e2a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03f1df",
   "metadata": {},
   "source": [
    "## Part 1: Training RNN With Varying Hyperparameters (Altering Hidden Layer Sizes Etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f2f3d",
   "metadata": {},
   "source": [
    "## UGGGGHH Model below still doesn't converge.  Perhaps it is still too small for much knowledge to be encoded during training.  I will try increasing the EMBEDDING_DIM and HIDDEN_DIM below to see if it helps it converge.  (However there is a balance because if the network is too large then it takes an unreasonable amount of time to train!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7ce0335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/266 | Loss: 0.6880\n",
      "Epoch: 001/003 | Batch 001/266 | Loss: 0.6881\n",
      "Epoch: 001/003 | Batch 002/266 | Loss: 0.7042\n",
      "Epoch: 001/003 | Batch 003/266 | Loss: 0.6912\n",
      "Epoch: 001/003 | Batch 004/266 | Loss: 0.6937\n",
      "Epoch: 001/003 | Batch 005/266 | Loss: 0.6992\n",
      "Epoch: 001/003 | Batch 006/266 | Loss: 0.6954\n",
      "Epoch: 001/003 | Batch 007/266 | Loss: 0.6914\n",
      "Epoch: 001/003 | Batch 008/266 | Loss: 0.6864\n",
      "Epoch: 001/003 | Batch 009/266 | Loss: 0.6901\n",
      "Epoch: 001/003 | Batch 010/266 | Loss: 0.6900\n",
      "Epoch: 001/003 | Batch 011/266 | Loss: 0.6962\n",
      "Epoch: 001/003 | Batch 012/266 | Loss: 0.6867\n",
      "Epoch: 001/003 | Batch 013/266 | Loss: 0.6911\n",
      "Epoch: 001/003 | Batch 014/266 | Loss: 0.6932\n",
      "Epoch: 001/003 | Batch 015/266 | Loss: 0.6898\n",
      "Epoch: 001/003 | Batch 016/266 | Loss: 0.6990\n",
      "Epoch: 001/003 | Batch 017/266 | Loss: 0.6911\n",
      "Epoch: 001/003 | Batch 018/266 | Loss: 0.6919\n",
      "Epoch: 001/003 | Batch 019/266 | Loss: 0.6957\n",
      "Epoch: 001/003 | Batch 020/266 | Loss: 0.7032\n",
      "Epoch: 001/003 | Batch 021/266 | Loss: 0.6973\n",
      "Epoch: 001/003 | Batch 022/266 | Loss: 0.6952\n",
      "Epoch: 001/003 | Batch 023/266 | Loss: 0.6975\n",
      "Epoch: 001/003 | Batch 024/266 | Loss: 0.6895\n",
      "Epoch: 001/003 | Batch 025/266 | Loss: 0.6912\n",
      "Epoch: 001/003 | Batch 026/266 | Loss: 0.6979\n",
      "Epoch: 001/003 | Batch 027/266 | Loss: 0.6919\n",
      "Epoch: 001/003 | Batch 028/266 | Loss: 0.6887\n",
      "Epoch: 001/003 | Batch 029/266 | Loss: 0.6953\n",
      "Epoch: 001/003 | Batch 030/266 | Loss: 0.6910\n",
      "Epoch: 001/003 | Batch 031/266 | Loss: 0.6882\n",
      "Epoch: 001/003 | Batch 032/266 | Loss: 0.6895\n",
      "Epoch: 001/003 | Batch 033/266 | Loss: 0.6899\n",
      "Epoch: 001/003 | Batch 034/266 | Loss: 0.6897\n",
      "Epoch: 001/003 | Batch 035/266 | Loss: 0.6939\n",
      "Epoch: 001/003 | Batch 036/266 | Loss: 0.6969\n",
      "Epoch: 001/003 | Batch 037/266 | Loss: 0.6942\n",
      "Epoch: 001/003 | Batch 038/266 | Loss: 0.6880\n",
      "Epoch: 001/003 | Batch 039/266 | Loss: 0.6875\n",
      "Epoch: 001/003 | Batch 040/266 | Loss: 0.6913\n",
      "Epoch: 001/003 | Batch 041/266 | Loss: 0.6896\n",
      "Epoch: 001/003 | Batch 042/266 | Loss: 0.6886\n",
      "Epoch: 001/003 | Batch 043/266 | Loss: 0.6879\n",
      "Epoch: 001/003 | Batch 044/266 | Loss: 0.6895\n",
      "Epoch: 001/003 | Batch 045/266 | Loss: 0.6881\n",
      "Epoch: 001/003 | Batch 046/266 | Loss: 0.6951\n",
      "Epoch: 001/003 | Batch 047/266 | Loss: 0.6875\n",
      "Epoch: 001/003 | Batch 048/266 | Loss: 0.6954\n",
      "Epoch: 001/003 | Batch 049/266 | Loss: 0.6920\n",
      "Epoch: 001/003 | Batch 050/266 | Loss: 0.6916\n",
      "Epoch: 001/003 | Batch 051/266 | Loss: 0.6924\n",
      "Epoch: 001/003 | Batch 052/266 | Loss: 0.6932\n",
      "Epoch: 001/003 | Batch 053/266 | Loss: 0.6900\n",
      "Epoch: 001/003 | Batch 054/266 | Loss: 0.6892\n",
      "Epoch: 001/003 | Batch 055/266 | Loss: 0.6885\n",
      "Epoch: 001/003 | Batch 056/266 | Loss: 0.6931\n",
      "Epoch: 001/003 | Batch 057/266 | Loss: 0.6891\n",
      "Epoch: 001/003 | Batch 058/266 | Loss: 0.6904\n",
      "Epoch: 001/003 | Batch 059/266 | Loss: 0.6888\n",
      "Epoch: 001/003 | Batch 060/266 | Loss: 0.6885\n",
      "Epoch: 001/003 | Batch 061/266 | Loss: 0.6882\n",
      "Epoch: 001/003 | Batch 062/266 | Loss: 0.6888\n",
      "Epoch: 001/003 | Batch 063/266 | Loss: 0.6883\n",
      "Epoch: 001/003 | Batch 064/266 | Loss: 0.6882\n",
      "Epoch: 001/003 | Batch 065/266 | Loss: 0.6948\n",
      "Epoch: 001/003 | Batch 066/266 | Loss: 0.6883\n",
      "Epoch: 001/003 | Batch 067/266 | Loss: 0.6887\n",
      "Epoch: 001/003 | Batch 068/266 | Loss: 0.6932\n",
      "Epoch: 001/003 | Batch 069/266 | Loss: 0.6957\n",
      "Epoch: 001/003 | Batch 070/266 | Loss: 0.6877\n",
      "Epoch: 001/003 | Batch 071/266 | Loss: 0.6899\n",
      "Epoch: 001/003 | Batch 072/266 | Loss: 0.6861\n",
      "Epoch: 001/003 | Batch 073/266 | Loss: 0.6893\n",
      "Epoch: 001/003 | Batch 074/266 | Loss: 0.6950\n",
      "Epoch: 001/003 | Batch 075/266 | Loss: 0.6873\n",
      "Epoch: 001/003 | Batch 076/266 | Loss: 0.7017\n",
      "Epoch: 001/003 | Batch 077/266 | Loss: 0.6916\n",
      "Epoch: 001/003 | Batch 078/266 | Loss: 0.6908\n",
      "Epoch: 001/003 | Batch 079/266 | Loss: 0.6952\n",
      "Epoch: 001/003 | Batch 080/266 | Loss: 0.6937\n",
      "Epoch: 001/003 | Batch 081/266 | Loss: 0.6896\n",
      "Epoch: 001/003 | Batch 082/266 | Loss: 0.6882\n",
      "Epoch: 001/003 | Batch 083/266 | Loss: 0.6863\n",
      "Epoch: 001/003 | Batch 084/266 | Loss: 0.6875\n",
      "Epoch: 001/003 | Batch 085/266 | Loss: 0.6888\n",
      "Epoch: 001/003 | Batch 086/266 | Loss: 0.6908\n",
      "Epoch: 001/003 | Batch 087/266 | Loss: 0.6905\n",
      "Epoch: 001/003 | Batch 088/266 | Loss: 0.6890\n",
      "Epoch: 001/003 | Batch 089/266 | Loss: 0.6874\n",
      "Epoch: 001/003 | Batch 090/266 | Loss: 0.6950\n",
      "Epoch: 001/003 | Batch 091/266 | Loss: 0.6902\n",
      "Epoch: 001/003 | Batch 092/266 | Loss: 0.6965\n",
      "Epoch: 001/003 | Batch 093/266 | Loss: 0.6925\n",
      "Epoch: 001/003 | Batch 094/266 | Loss: 0.6886\n",
      "Epoch: 001/003 | Batch 095/266 | Loss: 0.6880\n",
      "Epoch: 001/003 | Batch 096/266 | Loss: 0.7108\n",
      "Epoch: 001/003 | Batch 097/266 | Loss: 0.6890\n",
      "Epoch: 001/003 | Batch 098/266 | Loss: 0.6906\n",
      "Epoch: 001/003 | Batch 099/266 | Loss: 0.6874\n",
      "Epoch: 001/003 | Batch 100/266 | Loss: 0.6894\n",
      "Epoch: 001/003 | Batch 101/266 | Loss: 0.6913\n",
      "Epoch: 001/003 | Batch 102/266 | Loss: 0.6888\n",
      "Epoch: 001/003 | Batch 103/266 | Loss: 0.6880\n",
      "Epoch: 001/003 | Batch 104/266 | Loss: 0.6881\n",
      "Epoch: 001/003 | Batch 105/266 | Loss: 0.6896\n",
      "Epoch: 001/003 | Batch 106/266 | Loss: 0.6869\n",
      "Epoch: 001/003 | Batch 107/266 | Loss: 0.6863\n",
      "Epoch: 001/003 | Batch 108/266 | Loss: 0.6976\n",
      "Epoch: 001/003 | Batch 109/266 | Loss: 0.6936\n",
      "Epoch: 001/003 | Batch 110/266 | Loss: 0.7054\n",
      "Epoch: 001/003 | Batch 111/266 | Loss: 0.6892\n",
      "Epoch: 001/003 | Batch 112/266 | Loss: 0.6903\n",
      "Epoch: 001/003 | Batch 113/266 | Loss: 0.6947\n",
      "Epoch: 001/003 | Batch 114/266 | Loss: 0.6919\n",
      "Epoch: 001/003 | Batch 115/266 | Loss: 0.6859\n",
      "Epoch: 001/003 | Batch 116/266 | Loss: 0.6837\n",
      "Epoch: 001/003 | Batch 117/266 | Loss: 0.6982\n",
      "Epoch: 001/003 | Batch 118/266 | Loss: 0.6877\n",
      "Epoch: 001/003 | Batch 119/266 | Loss: 0.6912\n",
      "Epoch: 001/003 | Batch 120/266 | Loss: 0.6924\n",
      "Epoch: 001/003 | Batch 121/266 | Loss: 0.6933\n",
      "Epoch: 001/003 | Batch 122/266 | Loss: 0.6950\n",
      "Epoch: 001/003 | Batch 123/266 | Loss: 0.6896\n",
      "Epoch: 001/003 | Batch 124/266 | Loss: 0.6900\n",
      "Epoch: 001/003 | Batch 125/266 | Loss: 0.6917\n",
      "Epoch: 001/003 | Batch 126/266 | Loss: 0.6914\n",
      "Epoch: 001/003 | Batch 127/266 | Loss: 0.6952\n",
      "Epoch: 001/003 | Batch 128/266 | Loss: 0.6855\n",
      "Epoch: 001/003 | Batch 129/266 | Loss: 0.6866\n",
      "Epoch: 001/003 | Batch 130/266 | Loss: 0.6940\n",
      "Epoch: 001/003 | Batch 131/266 | Loss: 0.6888\n",
      "Epoch: 001/003 | Batch 132/266 | Loss: 0.6865\n",
      "Epoch: 001/003 | Batch 133/266 | Loss: 0.6997\n",
      "Epoch: 001/003 | Batch 134/266 | Loss: 0.6934\n",
      "Epoch: 001/003 | Batch 135/266 | Loss: 0.6878\n",
      "Epoch: 001/003 | Batch 136/266 | Loss: 0.6939\n",
      "Epoch: 001/003 | Batch 137/266 | Loss: 0.6899\n",
      "Epoch: 001/003 | Batch 138/266 | Loss: 0.6881\n",
      "Epoch: 001/003 | Batch 139/266 | Loss: 0.6882\n",
      "Epoch: 001/003 | Batch 140/266 | Loss: 0.6891\n",
      "Epoch: 001/003 | Batch 141/266 | Loss: 0.6920\n",
      "Epoch: 001/003 | Batch 142/266 | Loss: 0.6932\n",
      "Epoch: 001/003 | Batch 143/266 | Loss: 0.6934\n",
      "Epoch: 001/003 | Batch 144/266 | Loss: 0.6986\n",
      "Epoch: 001/003 | Batch 145/266 | Loss: 0.6886\n",
      "Epoch: 001/003 | Batch 146/266 | Loss: 0.6877\n",
      "Epoch: 001/003 | Batch 147/266 | Loss: 0.6873\n",
      "Epoch: 001/003 | Batch 148/266 | Loss: 0.6921\n",
      "Epoch: 001/003 | Batch 149/266 | Loss: 0.6902\n",
      "Epoch: 001/003 | Batch 150/266 | Loss: 0.6880\n",
      "Epoch: 001/003 | Batch 151/266 | Loss: 0.6884\n",
      "Epoch: 001/003 | Batch 152/266 | Loss: 0.6981\n",
      "Epoch: 001/003 | Batch 153/266 | Loss: 0.6911\n",
      "Epoch: 001/003 | Batch 154/266 | Loss: 0.6875\n",
      "Epoch: 001/003 | Batch 155/266 | Loss: 0.6885\n",
      "Epoch: 001/003 | Batch 156/266 | Loss: 0.6881\n",
      "Epoch: 001/003 | Batch 157/266 | Loss: 0.6927\n",
      "Epoch: 001/003 | Batch 158/266 | Loss: 0.6911\n",
      "Epoch: 001/003 | Batch 159/266 | Loss: 0.6884\n",
      "Epoch: 001/003 | Batch 160/266 | Loss: 0.6884\n",
      "Epoch: 001/003 | Batch 161/266 | Loss: 0.6922\n",
      "Epoch: 001/003 | Batch 162/266 | Loss: 0.6876\n",
      "Epoch: 001/003 | Batch 163/266 | Loss: 0.6880\n",
      "Epoch: 001/003 | Batch 164/266 | Loss: 0.6906\n",
      "Epoch: 001/003 | Batch 165/266 | Loss: 0.6916\n",
      "Epoch: 001/003 | Batch 166/266 | Loss: 0.6923\n",
      "Epoch: 001/003 | Batch 167/266 | Loss: 0.6896\n",
      "Epoch: 001/003 | Batch 168/266 | Loss: 0.6886\n",
      "Epoch: 001/003 | Batch 169/266 | Loss: 0.6919\n",
      "Epoch: 001/003 | Batch 170/266 | Loss: 0.6889\n",
      "Epoch: 001/003 | Batch 171/266 | Loss: 0.6887\n",
      "Epoch: 001/003 | Batch 172/266 | Loss: 0.6912\n",
      "Epoch: 001/003 | Batch 173/266 | Loss: 0.6871\n",
      "Epoch: 001/003 | Batch 174/266 | Loss: 0.6937\n",
      "Epoch: 001/003 | Batch 175/266 | Loss: 0.6893\n",
      "Epoch: 001/003 | Batch 176/266 | Loss: 0.6902\n",
      "Epoch: 001/003 | Batch 177/266 | Loss: 0.6872\n",
      "Epoch: 001/003 | Batch 178/266 | Loss: 0.6937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 179/266 | Loss: 0.7011\n",
      "Epoch: 001/003 | Batch 180/266 | Loss: 0.6967\n",
      "Epoch: 001/003 | Batch 181/266 | Loss: 0.6877\n",
      "Epoch: 001/003 | Batch 182/266 | Loss: 0.7068\n",
      "Epoch: 001/003 | Batch 183/266 | Loss: 0.6887\n",
      "Epoch: 001/003 | Batch 184/266 | Loss: 0.6880\n",
      "Epoch: 001/003 | Batch 185/266 | Loss: 0.6877\n",
      "Epoch: 001/003 | Batch 186/266 | Loss: 0.6967\n",
      "Epoch: 001/003 | Batch 187/266 | Loss: 0.6883\n",
      "Epoch: 001/003 | Batch 188/266 | Loss: 0.6957\n",
      "Epoch: 001/003 | Batch 189/266 | Loss: 0.6866\n",
      "Epoch: 001/003 | Batch 190/266 | Loss: 0.6889\n",
      "Epoch: 001/003 | Batch 191/266 | Loss: 0.6889\n",
      "Epoch: 001/003 | Batch 192/266 | Loss: 0.6875\n",
      "Epoch: 001/003 | Batch 193/266 | Loss: 0.6986\n",
      "Epoch: 001/003 | Batch 194/266 | Loss: 0.6899\n",
      "Epoch: 001/003 | Batch 195/266 | Loss: 0.6889\n",
      "Epoch: 001/003 | Batch 196/266 | Loss: 0.7052\n",
      "Epoch: 001/003 | Batch 197/266 | Loss: 0.6865\n",
      "Epoch: 001/003 | Batch 198/266 | Loss: 0.6924\n",
      "Epoch: 001/003 | Batch 199/266 | Loss: 0.6993\n",
      "Epoch: 001/003 | Batch 200/266 | Loss: 0.6914\n",
      "Epoch: 001/003 | Batch 201/266 | Loss: 0.6879\n",
      "Epoch: 001/003 | Batch 202/266 | Loss: 0.6913\n",
      "Epoch: 001/003 | Batch 203/266 | Loss: 0.6986\n",
      "Epoch: 001/003 | Batch 204/266 | Loss: 0.6877\n",
      "Epoch: 001/003 | Batch 205/266 | Loss: 0.6874\n",
      "Epoch: 001/003 | Batch 206/266 | Loss: 0.6889\n",
      "Epoch: 001/003 | Batch 207/266 | Loss: 0.6864\n",
      "Epoch: 001/003 | Batch 208/266 | Loss: 0.6909\n",
      "Epoch: 001/003 | Batch 209/266 | Loss: 0.6906\n",
      "Epoch: 001/003 | Batch 210/266 | Loss: 0.6952\n",
      "Epoch: 001/003 | Batch 211/266 | Loss: 0.6903\n",
      "Epoch: 001/003 | Batch 212/266 | Loss: 0.6883\n",
      "Epoch: 001/003 | Batch 213/266 | Loss: 0.6883\n",
      "Epoch: 001/003 | Batch 214/266 | Loss: 0.6873\n",
      "Epoch: 001/003 | Batch 215/266 | Loss: 0.6950\n",
      "Epoch: 001/003 | Batch 216/266 | Loss: 0.6864\n",
      "Epoch: 001/003 | Batch 217/266 | Loss: 0.6936\n",
      "Epoch: 001/003 | Batch 218/266 | Loss: 0.7001\n",
      "Epoch: 001/003 | Batch 219/266 | Loss: 0.6949\n",
      "Epoch: 001/003 | Batch 220/266 | Loss: 0.6894\n",
      "Epoch: 001/003 | Batch 221/266 | Loss: 0.6884\n",
      "Epoch: 001/003 | Batch 222/266 | Loss: 0.6877\n",
      "Epoch: 001/003 | Batch 223/266 | Loss: 0.6878\n",
      "Epoch: 001/003 | Batch 224/266 | Loss: 0.6931\n",
      "Epoch: 001/003 | Batch 225/266 | Loss: 0.6870\n",
      "Epoch: 001/003 | Batch 226/266 | Loss: 0.6890\n",
      "Epoch: 001/003 | Batch 227/266 | Loss: 0.6899\n",
      "Epoch: 001/003 | Batch 228/266 | Loss: 0.6894\n",
      "Epoch: 001/003 | Batch 229/266 | Loss: 0.6904\n",
      "Epoch: 001/003 | Batch 230/266 | Loss: 0.6893\n",
      "Epoch: 001/003 | Batch 231/266 | Loss: 0.6968\n",
      "Epoch: 001/003 | Batch 232/266 | Loss: 0.6914\n",
      "Epoch: 001/003 | Batch 233/266 | Loss: 0.6888\n",
      "Epoch: 001/003 | Batch 234/266 | Loss: 0.6904\n",
      "Epoch: 001/003 | Batch 235/266 | Loss: 0.6896\n",
      "Epoch: 001/003 | Batch 236/266 | Loss: 0.6874\n",
      "Epoch: 001/003 | Batch 237/266 | Loss: 0.6968\n",
      "Epoch: 001/003 | Batch 238/266 | Loss: 0.6972\n",
      "Epoch: 001/003 | Batch 239/266 | Loss: 0.6898\n",
      "Epoch: 001/003 | Batch 240/266 | Loss: 0.6896\n",
      "Epoch: 001/003 | Batch 241/266 | Loss: 0.6887\n",
      "Epoch: 001/003 | Batch 242/266 | Loss: 0.6878\n",
      "Epoch: 001/003 | Batch 243/266 | Loss: 0.6921\n",
      "Epoch: 001/003 | Batch 244/266 | Loss: 0.7035\n",
      "Epoch: 001/003 | Batch 245/266 | Loss: 0.6973\n",
      "Epoch: 001/003 | Batch 246/266 | Loss: 0.6885\n",
      "Epoch: 001/003 | Batch 247/266 | Loss: 0.6887\n",
      "Epoch: 001/003 | Batch 248/266 | Loss: 0.6890\n",
      "Epoch: 001/003 | Batch 249/266 | Loss: 0.6907\n",
      "Epoch: 001/003 | Batch 250/266 | Loss: 0.6952\n",
      "Epoch: 001/003 | Batch 251/266 | Loss: 0.6948\n",
      "Epoch: 001/003 | Batch 252/266 | Loss: 0.6898\n",
      "Epoch: 001/003 | Batch 253/266 | Loss: 0.6887\n",
      "Epoch: 001/003 | Batch 254/266 | Loss: 0.6986\n",
      "Epoch: 001/003 | Batch 255/266 | Loss: 0.7002\n",
      "Epoch: 001/003 | Batch 256/266 | Loss: 0.6875\n",
      "Epoch: 001/003 | Batch 257/266 | Loss: 0.6890\n",
      "Epoch: 001/003 | Batch 258/266 | Loss: 0.6927\n",
      "Epoch: 001/003 | Batch 259/266 | Loss: 0.6889\n",
      "Epoch: 001/003 | Batch 260/266 | Loss: 0.6874\n",
      "Epoch: 001/003 | Batch 261/266 | Loss: 0.6897\n",
      "Epoch: 001/003 | Batch 262/266 | Loss: 0.6856\n",
      "Epoch: 001/003 | Batch 263/266 | Loss: 0.6882\n",
      "Epoch: 001/003 | Batch 264/266 | Loss: 0.6924\n",
      "Epoch: 001/003 | Batch 265/266 | Loss: 0.6823\n",
      "training accuracy: 50.22%\n",
      "valid accuracy: 50.72%\n",
      "Time elapsed: 3.15 min\n",
      "Epoch: 002/003 | Batch 000/266 | Loss: 0.6876\n",
      "Epoch: 002/003 | Batch 001/266 | Loss: 0.6864\n",
      "Epoch: 002/003 | Batch 002/266 | Loss: 0.6890\n",
      "Epoch: 002/003 | Batch 003/266 | Loss: 0.6922\n",
      "Epoch: 002/003 | Batch 004/266 | Loss: 0.6930\n",
      "Epoch: 002/003 | Batch 005/266 | Loss: 0.6929\n",
      "Epoch: 002/003 | Batch 006/266 | Loss: 0.6849\n",
      "Epoch: 002/003 | Batch 007/266 | Loss: 0.6965\n",
      "Epoch: 002/003 | Batch 008/266 | Loss: 0.6941\n",
      "Epoch: 002/003 | Batch 009/266 | Loss: 0.6872\n",
      "Epoch: 002/003 | Batch 010/266 | Loss: 0.6895\n",
      "Epoch: 002/003 | Batch 011/266 | Loss: 0.6967\n",
      "Epoch: 002/003 | Batch 012/266 | Loss: 0.6899\n",
      "Epoch: 002/003 | Batch 013/266 | Loss: 0.7023\n",
      "Epoch: 002/003 | Batch 014/266 | Loss: 0.6911\n",
      "Epoch: 002/003 | Batch 015/266 | Loss: 0.6896\n",
      "Epoch: 002/003 | Batch 016/266 | Loss: 0.7016\n",
      "Epoch: 002/003 | Batch 017/266 | Loss: 0.6905\n",
      "Epoch: 002/003 | Batch 018/266 | Loss: 0.6892\n",
      "Epoch: 002/003 | Batch 019/266 | Loss: 0.6852\n",
      "Epoch: 002/003 | Batch 020/266 | Loss: 0.6872\n",
      "Epoch: 002/003 | Batch 021/266 | Loss: 0.6914\n",
      "Epoch: 002/003 | Batch 022/266 | Loss: 0.6948\n",
      "Epoch: 002/003 | Batch 023/266 | Loss: 0.6893\n",
      "Epoch: 002/003 | Batch 024/266 | Loss: 0.6877\n",
      "Epoch: 002/003 | Batch 025/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 026/266 | Loss: 0.6874\n",
      "Epoch: 002/003 | Batch 027/266 | Loss: 0.6875\n",
      "Epoch: 002/003 | Batch 028/266 | Loss: 0.7053\n",
      "Epoch: 002/003 | Batch 029/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 030/266 | Loss: 0.6881\n",
      "Epoch: 002/003 | Batch 031/266 | Loss: 0.6892\n",
      "Epoch: 002/003 | Batch 032/266 | Loss: 0.6916\n",
      "Epoch: 002/003 | Batch 033/266 | Loss: 0.6890\n",
      "Epoch: 002/003 | Batch 034/266 | Loss: 0.6910\n",
      "Epoch: 002/003 | Batch 035/266 | Loss: 0.6875\n",
      "Epoch: 002/003 | Batch 036/266 | Loss: 0.6948\n",
      "Epoch: 002/003 | Batch 037/266 | Loss: 0.6904\n",
      "Epoch: 002/003 | Batch 038/266 | Loss: 0.6882\n",
      "Epoch: 002/003 | Batch 039/266 | Loss: 0.6913\n",
      "Epoch: 002/003 | Batch 040/266 | Loss: 0.6882\n",
      "Epoch: 002/003 | Batch 041/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 042/266 | Loss: 0.6899\n",
      "Epoch: 002/003 | Batch 043/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 044/266 | Loss: 0.6873\n",
      "Epoch: 002/003 | Batch 045/266 | Loss: 0.6882\n",
      "Epoch: 002/003 | Batch 046/266 | Loss: 0.6896\n",
      "Epoch: 002/003 | Batch 047/266 | Loss: 0.6869\n",
      "Epoch: 002/003 | Batch 048/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 049/266 | Loss: 0.6889\n",
      "Epoch: 002/003 | Batch 050/266 | Loss: 0.6892\n",
      "Epoch: 002/003 | Batch 051/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 052/266 | Loss: 0.6881\n",
      "Epoch: 002/003 | Batch 053/266 | Loss: 0.6870\n",
      "Epoch: 002/003 | Batch 054/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 055/266 | Loss: 0.6894\n",
      "Epoch: 002/003 | Batch 056/266 | Loss: 0.6873\n",
      "Epoch: 002/003 | Batch 057/266 | Loss: 0.6906\n",
      "Epoch: 002/003 | Batch 058/266 | Loss: 0.6884\n",
      "Epoch: 002/003 | Batch 059/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 060/266 | Loss: 0.6879\n",
      "Epoch: 002/003 | Batch 061/266 | Loss: 0.6921\n",
      "Epoch: 002/003 | Batch 062/266 | Loss: 0.6868\n",
      "Epoch: 002/003 | Batch 063/266 | Loss: 0.7070\n",
      "Epoch: 002/003 | Batch 064/266 | Loss: 0.6911\n",
      "Epoch: 002/003 | Batch 065/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 066/266 | Loss: 0.6925\n",
      "Epoch: 002/003 | Batch 067/266 | Loss: 0.6912\n",
      "Epoch: 002/003 | Batch 068/266 | Loss: 0.6896\n",
      "Epoch: 002/003 | Batch 069/266 | Loss: 0.6898\n",
      "Epoch: 002/003 | Batch 070/266 | Loss: 0.6927\n",
      "Epoch: 002/003 | Batch 071/266 | Loss: 0.6940\n",
      "Epoch: 002/003 | Batch 072/266 | Loss: 0.6879\n",
      "Epoch: 002/003 | Batch 073/266 | Loss: 0.7007\n",
      "Epoch: 002/003 | Batch 074/266 | Loss: 0.6923\n",
      "Epoch: 002/003 | Batch 075/266 | Loss: 0.7165\n",
      "Epoch: 002/003 | Batch 076/266 | Loss: 0.6920\n",
      "Epoch: 002/003 | Batch 077/266 | Loss: 0.6890\n",
      "Epoch: 002/003 | Batch 078/266 | Loss: 0.6899\n",
      "Epoch: 002/003 | Batch 079/266 | Loss: 0.6889\n",
      "Epoch: 002/003 | Batch 080/266 | Loss: 0.6886\n",
      "Epoch: 002/003 | Batch 081/266 | Loss: 0.6883\n",
      "Epoch: 002/003 | Batch 082/266 | Loss: 0.6887\n",
      "Epoch: 002/003 | Batch 083/266 | Loss: 0.6912\n",
      "Epoch: 002/003 | Batch 084/266 | Loss: 0.6903\n",
      "Epoch: 002/003 | Batch 085/266 | Loss: 0.6931\n",
      "Epoch: 002/003 | Batch 086/266 | Loss: 0.6880\n",
      "Epoch: 002/003 | Batch 087/266 | Loss: 0.6875\n",
      "Epoch: 002/003 | Batch 088/266 | Loss: 0.6893\n",
      "Epoch: 002/003 | Batch 089/266 | Loss: 0.6908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002/003 | Batch 090/266 | Loss: 0.6898\n",
      "Epoch: 002/003 | Batch 091/266 | Loss: 0.7024\n",
      "Epoch: 002/003 | Batch 092/266 | Loss: 0.6900\n",
      "Epoch: 002/003 | Batch 093/266 | Loss: 0.6890\n",
      "Epoch: 002/003 | Batch 094/266 | Loss: 0.6898\n",
      "Epoch: 002/003 | Batch 095/266 | Loss: 0.6907\n",
      "Epoch: 002/003 | Batch 096/266 | Loss: 0.6876\n",
      "Epoch: 002/003 | Batch 097/266 | Loss: 0.6915\n",
      "Epoch: 002/003 | Batch 098/266 | Loss: 0.6881\n",
      "Epoch: 002/003 | Batch 099/266 | Loss: 0.6975\n",
      "Epoch: 002/003 | Batch 100/266 | Loss: 0.6884\n",
      "Epoch: 002/003 | Batch 101/266 | Loss: 0.6895\n",
      "Epoch: 002/003 | Batch 102/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 103/266 | Loss: 0.6898\n",
      "Epoch: 002/003 | Batch 104/266 | Loss: 0.6887\n",
      "Epoch: 002/003 | Batch 105/266 | Loss: 0.7025\n",
      "Epoch: 002/003 | Batch 106/266 | Loss: 0.6902\n",
      "Epoch: 002/003 | Batch 107/266 | Loss: 0.6908\n",
      "Epoch: 002/003 | Batch 108/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 109/266 | Loss: 0.6887\n",
      "Epoch: 002/003 | Batch 110/266 | Loss: 0.6879\n",
      "Epoch: 002/003 | Batch 111/266 | Loss: 0.7061\n",
      "Epoch: 002/003 | Batch 112/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 113/266 | Loss: 0.7032\n",
      "Epoch: 002/003 | Batch 114/266 | Loss: 0.6968\n",
      "Epoch: 002/003 | Batch 115/266 | Loss: 0.6874\n",
      "Epoch: 002/003 | Batch 116/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 117/266 | Loss: 0.6980\n",
      "Epoch: 002/003 | Batch 118/266 | Loss: 0.6892\n",
      "Epoch: 002/003 | Batch 119/266 | Loss: 0.6883\n",
      "Epoch: 002/003 | Batch 120/266 | Loss: 0.6880\n",
      "Epoch: 002/003 | Batch 121/266 | Loss: 0.6891\n",
      "Epoch: 002/003 | Batch 122/266 | Loss: 0.6909\n",
      "Epoch: 002/003 | Batch 123/266 | Loss: 0.6880\n",
      "Epoch: 002/003 | Batch 124/266 | Loss: 0.6917\n",
      "Epoch: 002/003 | Batch 125/266 | Loss: 0.7030\n",
      "Epoch: 002/003 | Batch 126/266 | Loss: 0.6878\n",
      "Epoch: 002/003 | Batch 127/266 | Loss: 0.6961\n",
      "Epoch: 002/003 | Batch 128/266 | Loss: 0.6882\n",
      "Epoch: 002/003 | Batch 129/266 | Loss: 0.6890\n",
      "Epoch: 002/003 | Batch 130/266 | Loss: 0.6896\n",
      "Epoch: 002/003 | Batch 131/266 | Loss: 0.6897\n",
      "Epoch: 002/003 | Batch 132/266 | Loss: 0.6879\n",
      "Epoch: 002/003 | Batch 133/266 | Loss: 0.6884\n",
      "Epoch: 002/003 | Batch 134/266 | Loss: 0.6882\n",
      "Epoch: 002/003 | Batch 135/266 | Loss: 0.6938\n",
      "Epoch: 002/003 | Batch 136/266 | Loss: 0.6882\n",
      "Epoch: 002/003 | Batch 137/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 138/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 139/266 | Loss: 0.6898\n",
      "Epoch: 002/003 | Batch 140/266 | Loss: 0.6873\n",
      "Epoch: 002/003 | Batch 141/266 | Loss: 0.6934\n",
      "Epoch: 002/003 | Batch 142/266 | Loss: 0.6930\n",
      "Epoch: 002/003 | Batch 143/266 | Loss: 0.6938\n",
      "Epoch: 002/003 | Batch 144/266 | Loss: 0.7054\n",
      "Epoch: 002/003 | Batch 145/266 | Loss: 0.6892\n",
      "Epoch: 002/003 | Batch 146/266 | Loss: 0.6916\n",
      "Epoch: 002/003 | Batch 147/266 | Loss: 0.6892\n",
      "Epoch: 002/003 | Batch 148/266 | Loss: 0.6892\n",
      "Epoch: 002/003 | Batch 149/266 | Loss: 0.6921\n",
      "Epoch: 002/003 | Batch 150/266 | Loss: 0.6891\n",
      "Epoch: 002/003 | Batch 151/266 | Loss: 0.6959\n",
      "Epoch: 002/003 | Batch 152/266 | Loss: 0.6899\n",
      "Epoch: 002/003 | Batch 153/266 | Loss: 0.6898\n",
      "Epoch: 002/003 | Batch 154/266 | Loss: 0.6887\n",
      "Epoch: 002/003 | Batch 155/266 | Loss: 0.6867\n",
      "Epoch: 002/003 | Batch 156/266 | Loss: 0.6938\n",
      "Epoch: 002/003 | Batch 157/266 | Loss: 0.6930\n",
      "Epoch: 002/003 | Batch 158/266 | Loss: 0.6956\n",
      "Epoch: 002/003 | Batch 159/266 | Loss: 0.6975\n",
      "Epoch: 002/003 | Batch 160/266 | Loss: 0.6886\n",
      "Epoch: 002/003 | Batch 161/266 | Loss: 0.6881\n",
      "Epoch: 002/003 | Batch 162/266 | Loss: 0.6917\n",
      "Epoch: 002/003 | Batch 163/266 | Loss: 0.6933\n",
      "Epoch: 002/003 | Batch 164/266 | Loss: 0.6913\n",
      "Epoch: 002/003 | Batch 165/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 166/266 | Loss: 0.6889\n",
      "Epoch: 002/003 | Batch 167/266 | Loss: 0.6878\n",
      "Epoch: 002/003 | Batch 168/266 | Loss: 0.6979\n",
      "Epoch: 002/003 | Batch 169/266 | Loss: 0.6906\n",
      "Epoch: 002/003 | Batch 170/266 | Loss: 0.6890\n",
      "Epoch: 002/003 | Batch 171/266 | Loss: 0.6894\n",
      "Epoch: 002/003 | Batch 172/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 173/266 | Loss: 0.6908\n",
      "Epoch: 002/003 | Batch 174/266 | Loss: 0.6890\n",
      "Epoch: 002/003 | Batch 175/266 | Loss: 0.6897\n",
      "Epoch: 002/003 | Batch 176/266 | Loss: 0.6886\n",
      "Epoch: 002/003 | Batch 177/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 178/266 | Loss: 0.6877\n",
      "Epoch: 002/003 | Batch 179/266 | Loss: 0.6890\n",
      "Epoch: 002/003 | Batch 180/266 | Loss: 0.6927\n",
      "Epoch: 002/003 | Batch 181/266 | Loss: 0.6910\n",
      "Epoch: 002/003 | Batch 182/266 | Loss: 0.6881\n",
      "Epoch: 002/003 | Batch 183/266 | Loss: 0.6900\n",
      "Epoch: 002/003 | Batch 184/266 | Loss: 0.6882\n",
      "Epoch: 002/003 | Batch 185/266 | Loss: 0.6879\n",
      "Epoch: 002/003 | Batch 186/266 | Loss: 0.6887\n",
      "Epoch: 002/003 | Batch 187/266 | Loss: 0.6918\n",
      "Epoch: 002/003 | Batch 188/266 | Loss: 0.6912\n",
      "Epoch: 002/003 | Batch 189/266 | Loss: 0.6883\n",
      "Epoch: 002/003 | Batch 190/266 | Loss: 0.6881\n",
      "Epoch: 002/003 | Batch 191/266 | Loss: 0.6981\n",
      "Epoch: 002/003 | Batch 192/266 | Loss: 0.6898\n",
      "Epoch: 002/003 | Batch 193/266 | Loss: 0.6909\n",
      "Epoch: 002/003 | Batch 194/266 | Loss: 0.6893\n",
      "Epoch: 002/003 | Batch 195/266 | Loss: 0.6909\n",
      "Epoch: 002/003 | Batch 196/266 | Loss: 0.6966\n",
      "Epoch: 002/003 | Batch 197/266 | Loss: 0.6912\n",
      "Epoch: 002/003 | Batch 198/266 | Loss: 0.6876\n",
      "Epoch: 002/003 | Batch 199/266 | Loss: 0.6968\n",
      "Epoch: 002/003 | Batch 200/266 | Loss: 0.6900\n",
      "Epoch: 002/003 | Batch 201/266 | Loss: 0.6897\n",
      "Epoch: 002/003 | Batch 202/266 | Loss: 0.6881\n",
      "Epoch: 002/003 | Batch 203/266 | Loss: 0.6982\n",
      "Epoch: 002/003 | Batch 204/266 | Loss: 0.6952\n",
      "Epoch: 002/003 | Batch 205/266 | Loss: 0.6880\n",
      "Epoch: 002/003 | Batch 206/266 | Loss: 0.6878\n",
      "Epoch: 002/003 | Batch 207/266 | Loss: 0.6882\n",
      "Epoch: 002/003 | Batch 208/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 209/266 | Loss: 0.6891\n",
      "Epoch: 002/003 | Batch 210/266 | Loss: 0.6976\n",
      "Epoch: 002/003 | Batch 211/266 | Loss: 0.7077\n",
      "Epoch: 002/003 | Batch 212/266 | Loss: 0.6892\n",
      "Epoch: 002/003 | Batch 213/266 | Loss: 0.6895\n",
      "Epoch: 002/003 | Batch 214/266 | Loss: 0.7102\n",
      "Epoch: 002/003 | Batch 215/266 | Loss: 0.6919\n",
      "Epoch: 002/003 | Batch 216/266 | Loss: 0.6886\n",
      "Epoch: 002/003 | Batch 217/266 | Loss: 0.6879\n",
      "Epoch: 002/003 | Batch 218/266 | Loss: 0.6896\n",
      "Epoch: 002/003 | Batch 219/266 | Loss: 0.6879\n",
      "Epoch: 002/003 | Batch 220/266 | Loss: 0.6886\n",
      "Epoch: 002/003 | Batch 221/266 | Loss: 0.6906\n",
      "Epoch: 002/003 | Batch 222/266 | Loss: 0.6884\n",
      "Epoch: 002/003 | Batch 223/266 | Loss: 0.6880\n",
      "Epoch: 002/003 | Batch 224/266 | Loss: 0.6882\n",
      "Epoch: 002/003 | Batch 225/266 | Loss: 0.6911\n",
      "Epoch: 002/003 | Batch 226/266 | Loss: 0.6884\n",
      "Epoch: 002/003 | Batch 227/266 | Loss: 0.6934\n",
      "Epoch: 002/003 | Batch 228/266 | Loss: 0.7051\n",
      "Epoch: 002/003 | Batch 229/266 | Loss: 0.6992\n",
      "Epoch: 002/003 | Batch 230/266 | Loss: 0.6891\n",
      "Epoch: 002/003 | Batch 231/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 232/266 | Loss: 0.6932\n",
      "Epoch: 002/003 | Batch 233/266 | Loss: 0.6935\n",
      "Epoch: 002/003 | Batch 234/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 235/266 | Loss: 0.6886\n",
      "Epoch: 002/003 | Batch 236/266 | Loss: 0.6918\n",
      "Epoch: 002/003 | Batch 237/266 | Loss: 0.7013\n",
      "Epoch: 002/003 | Batch 238/266 | Loss: 0.6879\n",
      "Epoch: 002/003 | Batch 239/266 | Loss: 0.6891\n",
      "Epoch: 002/003 | Batch 240/266 | Loss: 0.6889\n",
      "Epoch: 002/003 | Batch 241/266 | Loss: 0.6931\n",
      "Epoch: 002/003 | Batch 242/266 | Loss: 0.6893\n",
      "Epoch: 002/003 | Batch 243/266 | Loss: 0.6898\n",
      "Epoch: 002/003 | Batch 244/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 245/266 | Loss: 0.6896\n",
      "Epoch: 002/003 | Batch 246/266 | Loss: 0.6934\n",
      "Epoch: 002/003 | Batch 247/266 | Loss: 0.6881\n",
      "Epoch: 002/003 | Batch 248/266 | Loss: 0.6883\n",
      "Epoch: 002/003 | Batch 249/266 | Loss: 0.6860\n",
      "Epoch: 002/003 | Batch 250/266 | Loss: 0.6895\n",
      "Epoch: 002/003 | Batch 251/266 | Loss: 0.6888\n",
      "Epoch: 002/003 | Batch 252/266 | Loss: 0.6967\n",
      "Epoch: 002/003 | Batch 253/266 | Loss: 0.6884\n",
      "Epoch: 002/003 | Batch 254/266 | Loss: 0.7040\n",
      "Epoch: 002/003 | Batch 255/266 | Loss: 0.6883\n",
      "Epoch: 002/003 | Batch 256/266 | Loss: 0.6883\n",
      "Epoch: 002/003 | Batch 257/266 | Loss: 0.6883\n",
      "Epoch: 002/003 | Batch 258/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 259/266 | Loss: 0.6891\n",
      "Epoch: 002/003 | Batch 260/266 | Loss: 0.6883\n",
      "Epoch: 002/003 | Batch 261/266 | Loss: 0.6879\n",
      "Epoch: 002/003 | Batch 262/266 | Loss: 0.6885\n",
      "Epoch: 002/003 | Batch 263/266 | Loss: 0.6897\n",
      "Epoch: 002/003 | Batch 264/266 | Loss: 0.6881\n",
      "Epoch: 002/003 | Batch 265/266 | Loss: 0.6871\n",
      "training accuracy: 50.31%\n",
      "valid accuracy: 49.30%\n",
      "Time elapsed: 6.69 min\n",
      "Epoch: 003/003 | Batch 000/266 | Loss: 0.6886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/003 | Batch 001/266 | Loss: 0.6920\n",
      "Epoch: 003/003 | Batch 002/266 | Loss: 0.6870\n",
      "Epoch: 003/003 | Batch 003/266 | Loss: 0.6879\n",
      "Epoch: 003/003 | Batch 004/266 | Loss: 0.6874\n",
      "Epoch: 003/003 | Batch 005/266 | Loss: 0.6932\n",
      "Epoch: 003/003 | Batch 006/266 | Loss: 0.6890\n",
      "Epoch: 003/003 | Batch 007/266 | Loss: 0.6902\n",
      "Epoch: 003/003 | Batch 008/266 | Loss: 0.6918\n",
      "Epoch: 003/003 | Batch 009/266 | Loss: 0.6904\n",
      "Epoch: 003/003 | Batch 010/266 | Loss: 0.6890\n",
      "Epoch: 003/003 | Batch 011/266 | Loss: 0.6895\n",
      "Epoch: 003/003 | Batch 012/266 | Loss: 0.7047\n",
      "Epoch: 003/003 | Batch 013/266 | Loss: 0.6938\n",
      "Epoch: 003/003 | Batch 014/266 | Loss: 0.6902\n",
      "Epoch: 003/003 | Batch 015/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 016/266 | Loss: 0.6878\n",
      "Epoch: 003/003 | Batch 017/266 | Loss: 0.6868\n",
      "Epoch: 003/003 | Batch 018/266 | Loss: 0.6928\n",
      "Epoch: 003/003 | Batch 019/266 | Loss: 0.6876\n",
      "Epoch: 003/003 | Batch 020/266 | Loss: 0.6903\n",
      "Epoch: 003/003 | Batch 021/266 | Loss: 0.6880\n",
      "Epoch: 003/003 | Batch 022/266 | Loss: 0.7015\n",
      "Epoch: 003/003 | Batch 023/266 | Loss: 0.6858\n",
      "Epoch: 003/003 | Batch 024/266 | Loss: 0.6876\n",
      "Epoch: 003/003 | Batch 025/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 026/266 | Loss: 0.6914\n",
      "Epoch: 003/003 | Batch 027/266 | Loss: 0.6898\n",
      "Epoch: 003/003 | Batch 028/266 | Loss: 0.6868\n",
      "Epoch: 003/003 | Batch 029/266 | Loss: 0.6868\n",
      "Epoch: 003/003 | Batch 030/266 | Loss: 0.6879\n",
      "Epoch: 003/003 | Batch 031/266 | Loss: 0.6869\n",
      "Epoch: 003/003 | Batch 032/266 | Loss: 0.6866\n",
      "Epoch: 003/003 | Batch 033/266 | Loss: 0.6856\n",
      "Epoch: 003/003 | Batch 034/266 | Loss: 0.6843\n",
      "Epoch: 003/003 | Batch 035/266 | Loss: 0.6916\n",
      "Epoch: 003/003 | Batch 036/266 | Loss: 0.6891\n",
      "Epoch: 003/003 | Batch 037/266 | Loss: 0.6967\n",
      "Epoch: 003/003 | Batch 038/266 | Loss: 0.6871\n",
      "Epoch: 003/003 | Batch 039/266 | Loss: 0.7153\n",
      "Epoch: 003/003 | Batch 040/266 | Loss: 0.6958\n",
      "Epoch: 003/003 | Batch 041/266 | Loss: 0.6880\n",
      "Epoch: 003/003 | Batch 042/266 | Loss: 0.6887\n",
      "Epoch: 003/003 | Batch 043/266 | Loss: 0.6893\n",
      "Epoch: 003/003 | Batch 044/266 | Loss: 0.6867\n",
      "Epoch: 003/003 | Batch 045/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 046/266 | Loss: 0.6869\n",
      "Epoch: 003/003 | Batch 047/266 | Loss: 0.6911\n",
      "Epoch: 003/003 | Batch 048/266 | Loss: 0.6991\n",
      "Epoch: 003/003 | Batch 049/266 | Loss: 0.6973\n",
      "Epoch: 003/003 | Batch 050/266 | Loss: 0.6908\n",
      "Epoch: 003/003 | Batch 051/266 | Loss: 0.7027\n",
      "Epoch: 003/003 | Batch 052/266 | Loss: 0.7047\n",
      "Epoch: 003/003 | Batch 053/266 | Loss: 0.6901\n",
      "Epoch: 003/003 | Batch 054/266 | Loss: 0.6886\n",
      "Epoch: 003/003 | Batch 055/266 | Loss: 0.6900\n",
      "Epoch: 003/003 | Batch 056/266 | Loss: 0.6869\n",
      "Epoch: 003/003 | Batch 057/266 | Loss: 0.6873\n",
      "Epoch: 003/003 | Batch 058/266 | Loss: 0.6921\n",
      "Epoch: 003/003 | Batch 059/266 | Loss: 0.6897\n",
      "Epoch: 003/003 | Batch 060/266 | Loss: 0.7091\n",
      "Epoch: 003/003 | Batch 061/266 | Loss: 0.6894\n",
      "Epoch: 003/003 | Batch 062/266 | Loss: 0.6922\n",
      "Epoch: 003/003 | Batch 063/266 | Loss: 0.6893\n",
      "Epoch: 003/003 | Batch 064/266 | Loss: 0.6879\n",
      "Epoch: 003/003 | Batch 065/266 | Loss: 0.6894\n",
      "Epoch: 003/003 | Batch 066/266 | Loss: 0.7132\n",
      "Epoch: 003/003 | Batch 067/266 | Loss: 0.6962\n",
      "Epoch: 003/003 | Batch 068/266 | Loss: 0.6945\n",
      "Epoch: 003/003 | Batch 069/266 | Loss: 0.6871\n",
      "Epoch: 003/003 | Batch 070/266 | Loss: 0.6876\n",
      "Epoch: 003/003 | Batch 071/266 | Loss: 0.6873\n",
      "Epoch: 003/003 | Batch 072/266 | Loss: 0.6874\n",
      "Epoch: 003/003 | Batch 073/266 | Loss: 0.6888\n",
      "Epoch: 003/003 | Batch 074/266 | Loss: 0.6880\n",
      "Epoch: 003/003 | Batch 075/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 076/266 | Loss: 0.6899\n",
      "Epoch: 003/003 | Batch 077/266 | Loss: 0.6978\n",
      "Epoch: 003/003 | Batch 078/266 | Loss: 0.6858\n",
      "Epoch: 003/003 | Batch 079/266 | Loss: 0.6885\n",
      "Epoch: 003/003 | Batch 080/266 | Loss: 0.6930\n",
      "Epoch: 003/003 | Batch 081/266 | Loss: 0.6905\n",
      "Epoch: 003/003 | Batch 082/266 | Loss: 0.6862\n",
      "Epoch: 003/003 | Batch 083/266 | Loss: 0.6926\n",
      "Epoch: 003/003 | Batch 084/266 | Loss: 0.6907\n",
      "Epoch: 003/003 | Batch 085/266 | Loss: 0.6933\n",
      "Epoch: 003/003 | Batch 086/266 | Loss: 0.7004\n",
      "Epoch: 003/003 | Batch 087/266 | Loss: 0.6909\n",
      "Epoch: 003/003 | Batch 088/266 | Loss: 0.7025\n",
      "Epoch: 003/003 | Batch 089/266 | Loss: 0.6888\n",
      "Epoch: 003/003 | Batch 090/266 | Loss: 0.6894\n",
      "Epoch: 003/003 | Batch 091/266 | Loss: 0.6898\n",
      "Epoch: 003/003 | Batch 092/266 | Loss: 0.6900\n",
      "Epoch: 003/003 | Batch 093/266 | Loss: 0.6901\n",
      "Epoch: 003/003 | Batch 094/266 | Loss: 0.6881\n",
      "Epoch: 003/003 | Batch 095/266 | Loss: 0.6980\n",
      "Epoch: 003/003 | Batch 096/266 | Loss: 0.6944\n",
      "Epoch: 003/003 | Batch 097/266 | Loss: 0.6907\n",
      "Epoch: 003/003 | Batch 098/266 | Loss: 0.6901\n",
      "Epoch: 003/003 | Batch 099/266 | Loss: 0.6887\n",
      "Epoch: 003/003 | Batch 100/266 | Loss: 0.6944\n",
      "Epoch: 003/003 | Batch 101/266 | Loss: 0.6892\n",
      "Epoch: 003/003 | Batch 102/266 | Loss: 0.6893\n",
      "Epoch: 003/003 | Batch 103/266 | Loss: 0.6863\n",
      "Epoch: 003/003 | Batch 104/266 | Loss: 0.6946\n",
      "Epoch: 003/003 | Batch 105/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 106/266 | Loss: 0.6956\n",
      "Epoch: 003/003 | Batch 107/266 | Loss: 0.6894\n",
      "Epoch: 003/003 | Batch 108/266 | Loss: 0.6952\n",
      "Epoch: 003/003 | Batch 109/266 | Loss: 0.6905\n",
      "Epoch: 003/003 | Batch 110/266 | Loss: 0.6911\n",
      "Epoch: 003/003 | Batch 111/266 | Loss: 0.6897\n",
      "Epoch: 003/003 | Batch 112/266 | Loss: 0.6870\n",
      "Epoch: 003/003 | Batch 113/266 | Loss: 0.6868\n",
      "Epoch: 003/003 | Batch 114/266 | Loss: 0.6881\n",
      "Epoch: 003/003 | Batch 115/266 | Loss: 0.6832\n",
      "Epoch: 003/003 | Batch 116/266 | Loss: 0.6865\n",
      "Epoch: 003/003 | Batch 117/266 | Loss: 0.6851\n",
      "Epoch: 003/003 | Batch 118/266 | Loss: 0.6928\n",
      "Epoch: 003/003 | Batch 119/266 | Loss: 0.6933\n",
      "Epoch: 003/003 | Batch 120/266 | Loss: 0.6904\n",
      "Epoch: 003/003 | Batch 121/266 | Loss: 0.6962\n",
      "Epoch: 003/003 | Batch 122/266 | Loss: 0.6906\n",
      "Epoch: 003/003 | Batch 123/266 | Loss: 0.6884\n",
      "Epoch: 003/003 | Batch 124/266 | Loss: 0.6918\n",
      "Epoch: 003/003 | Batch 125/266 | Loss: 0.6899\n",
      "Epoch: 003/003 | Batch 126/266 | Loss: 0.6897\n",
      "Epoch: 003/003 | Batch 127/266 | Loss: 0.6924\n",
      "Epoch: 003/003 | Batch 128/266 | Loss: 0.6917\n",
      "Epoch: 003/003 | Batch 129/266 | Loss: 0.7010\n",
      "Epoch: 003/003 | Batch 130/266 | Loss: 0.6876\n",
      "Epoch: 003/003 | Batch 131/266 | Loss: 0.6926\n",
      "Epoch: 003/003 | Batch 132/266 | Loss: 0.7053\n",
      "Epoch: 003/003 | Batch 133/266 | Loss: 0.6889\n",
      "Epoch: 003/003 | Batch 134/266 | Loss: 0.6911\n",
      "Epoch: 003/003 | Batch 135/266 | Loss: 0.6898\n",
      "Epoch: 003/003 | Batch 136/266 | Loss: 0.6878\n",
      "Epoch: 003/003 | Batch 137/266 | Loss: 0.6881\n",
      "Epoch: 003/003 | Batch 138/266 | Loss: 0.6879\n",
      "Epoch: 003/003 | Batch 139/266 | Loss: 0.7146\n",
      "Epoch: 003/003 | Batch 140/266 | Loss: 0.6885\n",
      "Epoch: 003/003 | Batch 141/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 142/266 | Loss: 0.6922\n",
      "Epoch: 003/003 | Batch 143/266 | Loss: 0.6898\n",
      "Epoch: 003/003 | Batch 144/266 | Loss: 0.6877\n",
      "Epoch: 003/003 | Batch 145/266 | Loss: 0.6891\n",
      "Epoch: 003/003 | Batch 146/266 | Loss: 0.6941\n",
      "Epoch: 003/003 | Batch 147/266 | Loss: 0.7031\n",
      "Epoch: 003/003 | Batch 148/266 | Loss: 0.6885\n",
      "Epoch: 003/003 | Batch 149/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 150/266 | Loss: 0.6952\n",
      "Epoch: 003/003 | Batch 151/266 | Loss: 0.6930\n",
      "Epoch: 003/003 | Batch 152/266 | Loss: 0.6887\n",
      "Epoch: 003/003 | Batch 153/266 | Loss: 0.6936\n",
      "Epoch: 003/003 | Batch 154/266 | Loss: 0.6916\n",
      "Epoch: 003/003 | Batch 155/266 | Loss: 0.6878\n",
      "Epoch: 003/003 | Batch 156/266 | Loss: 0.6912\n",
      "Epoch: 003/003 | Batch 157/266 | Loss: 0.6881\n",
      "Epoch: 003/003 | Batch 158/266 | Loss: 0.6877\n",
      "Epoch: 003/003 | Batch 159/266 | Loss: 0.6886\n",
      "Epoch: 003/003 | Batch 160/266 | Loss: 0.6888\n",
      "Epoch: 003/003 | Batch 161/266 | Loss: 0.6883\n",
      "Epoch: 003/003 | Batch 162/266 | Loss: 0.6857\n",
      "Epoch: 003/003 | Batch 163/266 | Loss: 0.6896\n",
      "Epoch: 003/003 | Batch 164/266 | Loss: 0.7009\n",
      "Epoch: 003/003 | Batch 165/266 | Loss: 0.6933\n",
      "Epoch: 003/003 | Batch 166/266 | Loss: 0.6942\n",
      "Epoch: 003/003 | Batch 167/266 | Loss: 0.6881\n",
      "Epoch: 003/003 | Batch 168/266 | Loss: 0.6893\n",
      "Epoch: 003/003 | Batch 169/266 | Loss: 0.6893\n",
      "Epoch: 003/003 | Batch 170/266 | Loss: 0.6913\n",
      "Epoch: 003/003 | Batch 171/266 | Loss: 0.6849\n",
      "Epoch: 003/003 | Batch 172/266 | Loss: 0.6912\n",
      "Epoch: 003/003 | Batch 173/266 | Loss: 0.6901\n",
      "Epoch: 003/003 | Batch 174/266 | Loss: 0.6848\n",
      "Epoch: 003/003 | Batch 175/266 | Loss: 0.6893\n",
      "Epoch: 003/003 | Batch 176/266 | Loss: 0.6928\n",
      "Epoch: 003/003 | Batch 177/266 | Loss: 0.6915\n",
      "Epoch: 003/003 | Batch 178/266 | Loss: 0.6895\n",
      "Epoch: 003/003 | Batch 179/266 | Loss: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/003 | Batch 180/266 | Loss: 0.6891\n",
      "Epoch: 003/003 | Batch 181/266 | Loss: 0.6926\n",
      "Epoch: 003/003 | Batch 182/266 | Loss: 0.6893\n",
      "Epoch: 003/003 | Batch 183/266 | Loss: 0.6885\n",
      "Epoch: 003/003 | Batch 184/266 | Loss: 0.6912\n",
      "Epoch: 003/003 | Batch 185/266 | Loss: 0.6898\n",
      "Epoch: 003/003 | Batch 186/266 | Loss: 0.6884\n",
      "Epoch: 003/003 | Batch 187/266 | Loss: 0.6909\n",
      "Epoch: 003/003 | Batch 188/266 | Loss: 0.7082\n",
      "Epoch: 003/003 | Batch 189/266 | Loss: 0.6891\n",
      "Epoch: 003/003 | Batch 190/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 191/266 | Loss: 0.6884\n",
      "Epoch: 003/003 | Batch 192/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 193/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 194/266 | Loss: 0.6884\n",
      "Epoch: 003/003 | Batch 195/266 | Loss: 0.6884\n",
      "Epoch: 003/003 | Batch 196/266 | Loss: 0.6883\n",
      "Epoch: 003/003 | Batch 197/266 | Loss: 0.6893\n",
      "Epoch: 003/003 | Batch 198/266 | Loss: 0.6880\n",
      "Epoch: 003/003 | Batch 199/266 | Loss: 0.7042\n",
      "Epoch: 003/003 | Batch 200/266 | Loss: 0.6883\n",
      "Epoch: 003/003 | Batch 201/266 | Loss: 0.6888\n",
      "Epoch: 003/003 | Batch 202/266 | Loss: 0.6883\n",
      "Epoch: 003/003 | Batch 203/266 | Loss: 0.6884\n",
      "Epoch: 003/003 | Batch 204/266 | Loss: 0.6879\n",
      "Epoch: 003/003 | Batch 205/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 206/266 | Loss: 0.6884\n",
      "Epoch: 003/003 | Batch 207/266 | Loss: 0.6891\n",
      "Epoch: 003/003 | Batch 208/266 | Loss: 0.6886\n",
      "Epoch: 003/003 | Batch 209/266 | Loss: 0.6886\n",
      "Epoch: 003/003 | Batch 210/266 | Loss: 0.6894\n",
      "Epoch: 003/003 | Batch 211/266 | Loss: 0.6879\n",
      "Epoch: 003/003 | Batch 212/266 | Loss: 0.6883\n",
      "Epoch: 003/003 | Batch 213/266 | Loss: 0.6880\n",
      "Epoch: 003/003 | Batch 214/266 | Loss: 0.6956\n",
      "Epoch: 003/003 | Batch 215/266 | Loss: 0.6878\n",
      "Epoch: 003/003 | Batch 216/266 | Loss: 0.6885\n",
      "Epoch: 003/003 | Batch 217/266 | Loss: 0.6999\n",
      "Epoch: 003/003 | Batch 218/266 | Loss: 0.6908\n",
      "Epoch: 003/003 | Batch 219/266 | Loss: 0.6866\n",
      "Epoch: 003/003 | Batch 220/266 | Loss: 0.6892\n",
      "Epoch: 003/003 | Batch 221/266 | Loss: 0.7083\n",
      "Epoch: 003/003 | Batch 222/266 | Loss: 0.6890\n",
      "Epoch: 003/003 | Batch 223/266 | Loss: 0.6867\n",
      "Epoch: 003/003 | Batch 224/266 | Loss: 0.6875\n",
      "Epoch: 003/003 | Batch 225/266 | Loss: 0.6897\n",
      "Epoch: 003/003 | Batch 226/266 | Loss: 0.6892\n",
      "Epoch: 003/003 | Batch 227/266 | Loss: 0.6897\n",
      "Epoch: 003/003 | Batch 228/266 | Loss: 0.6908\n",
      "Epoch: 003/003 | Batch 229/266 | Loss: 0.6868\n",
      "Epoch: 003/003 | Batch 230/266 | Loss: 0.6903\n",
      "Epoch: 003/003 | Batch 231/266 | Loss: 0.6920\n",
      "Epoch: 003/003 | Batch 232/266 | Loss: 0.6922\n",
      "Epoch: 003/003 | Batch 233/266 | Loss: 0.6965\n",
      "Epoch: 003/003 | Batch 234/266 | Loss: 0.6869\n",
      "Epoch: 003/003 | Batch 235/266 | Loss: 0.6871\n",
      "Epoch: 003/003 | Batch 236/266 | Loss: 0.6889\n",
      "Epoch: 003/003 | Batch 237/266 | Loss: 0.6878\n",
      "Epoch: 003/003 | Batch 238/266 | Loss: 0.6911\n",
      "Epoch: 003/003 | Batch 239/266 | Loss: 0.6885\n",
      "Epoch: 003/003 | Batch 240/266 | Loss: 0.6893\n",
      "Epoch: 003/003 | Batch 241/266 | Loss: 0.6881\n",
      "Epoch: 003/003 | Batch 242/266 | Loss: 0.6974\n",
      "Epoch: 003/003 | Batch 243/266 | Loss: 0.6988\n",
      "Epoch: 003/003 | Batch 244/266 | Loss: 0.6883\n",
      "Epoch: 003/003 | Batch 245/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 246/266 | Loss: 0.6911\n",
      "Epoch: 003/003 | Batch 247/266 | Loss: 0.6892\n",
      "Epoch: 003/003 | Batch 248/266 | Loss: 0.6867\n",
      "Epoch: 003/003 | Batch 249/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 250/266 | Loss: 0.6907\n",
      "Epoch: 003/003 | Batch 251/266 | Loss: 0.7026\n",
      "Epoch: 003/003 | Batch 252/266 | Loss: 0.6887\n",
      "Epoch: 003/003 | Batch 253/266 | Loss: 0.6980\n",
      "Epoch: 003/003 | Batch 254/266 | Loss: 0.6855\n",
      "Epoch: 003/003 | Batch 255/266 | Loss: 0.6882\n",
      "Epoch: 003/003 | Batch 256/266 | Loss: 0.6925\n",
      "Epoch: 003/003 | Batch 257/266 | Loss: 0.6863\n",
      "Epoch: 003/003 | Batch 258/266 | Loss: 0.6894\n",
      "Epoch: 003/003 | Batch 259/266 | Loss: 0.6938\n",
      "Epoch: 003/003 | Batch 260/266 | Loss: 0.6912\n",
      "Epoch: 003/003 | Batch 261/266 | Loss: 0.7038\n",
      "Epoch: 003/003 | Batch 262/266 | Loss: 0.6880\n",
      "Epoch: 003/003 | Batch 263/266 | Loss: 0.7007\n",
      "Epoch: 003/003 | Batch 264/266 | Loss: 0.6916\n",
      "Epoch: 003/003 | Batch 265/266 | Loss: 0.6882\n",
      "training accuracy: 50.23%\n",
      "valid accuracy: 50.93%\n",
      "Time elapsed: 9.92 min\n",
      "Total Training Time: 9.92 min\n",
      "Test accuracy: 50.16%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 24\n",
    "EMBEDDING_DIM = 8        #128\n",
    "HIDDEN_DIM =  16         #256\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text = batch_data.TEXT_COLUMN_NAME.to(DEVICE)\n",
    "        labels = batch_data.LABEL_COLUMN_NAME.to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "               f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "               f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9649f5",
   "metadata": {},
   "source": [
    "## HOORAYYY THE MODEL BELOW CONVERGED REASONABLY!! THIS WAS THE BEST RNN MODEL I WAS ABLE TO GET BY VARYING THE HYPERPARAMETERS.  THUS THIS WILL BE THE MODEL I SAVE TO LOAD IN THE MAIN JUPYTER NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90c4aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/008 | Batch 000/266 | Loss: 0.6944\n",
      "Epoch: 001/008 | Batch 001/266 | Loss: 0.6880\n",
      "Epoch: 001/008 | Batch 002/266 | Loss: 0.6885\n",
      "Epoch: 001/008 | Batch 003/266 | Loss: 0.6895\n",
      "Epoch: 001/008 | Batch 004/266 | Loss: 0.6884\n",
      "Epoch: 001/008 | Batch 005/266 | Loss: 0.6885\n",
      "Epoch: 001/008 | Batch 006/266 | Loss: 0.6886\n",
      "Epoch: 001/008 | Batch 007/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 008/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 009/266 | Loss: 0.6879\n",
      "Epoch: 001/008 | Batch 010/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 011/266 | Loss: 0.6842\n",
      "Epoch: 001/008 | Batch 012/266 | Loss: 0.7145\n",
      "Epoch: 001/008 | Batch 013/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 014/266 | Loss: 0.6963\n",
      "Epoch: 001/008 | Batch 015/266 | Loss: 0.6877\n",
      "Epoch: 001/008 | Batch 016/266 | Loss: 0.6956\n",
      "Epoch: 001/008 | Batch 017/266 | Loss: 0.7093\n",
      "Epoch: 001/008 | Batch 018/266 | Loss: 0.6908\n",
      "Epoch: 001/008 | Batch 019/266 | Loss: 0.6893\n",
      "Epoch: 001/008 | Batch 020/266 | Loss: 0.6867\n",
      "Epoch: 001/008 | Batch 021/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 022/266 | Loss: 0.6894\n",
      "Epoch: 001/008 | Batch 023/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 024/266 | Loss: 0.6867\n",
      "Epoch: 001/008 | Batch 025/266 | Loss: 0.6801\n",
      "Epoch: 001/008 | Batch 026/266 | Loss: 0.6901\n",
      "Epoch: 001/008 | Batch 027/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 028/266 | Loss: 0.6873\n",
      "Epoch: 001/008 | Batch 029/266 | Loss: 0.6866\n",
      "Epoch: 001/008 | Batch 030/266 | Loss: 0.6933\n",
      "Epoch: 001/008 | Batch 031/266 | Loss: 0.6868\n",
      "Epoch: 001/008 | Batch 032/266 | Loss: 0.6960\n",
      "Epoch: 001/008 | Batch 033/266 | Loss: 0.6865\n",
      "Epoch: 001/008 | Batch 034/266 | Loss: 0.6839\n",
      "Epoch: 001/008 | Batch 035/266 | Loss: 0.6915\n",
      "Epoch: 001/008 | Batch 036/266 | Loss: 0.6877\n",
      "Epoch: 001/008 | Batch 037/266 | Loss: 0.6888\n",
      "Epoch: 001/008 | Batch 038/266 | Loss: 0.6914\n",
      "Epoch: 001/008 | Batch 039/266 | Loss: 0.6861\n",
      "Epoch: 001/008 | Batch 040/266 | Loss: 0.6861\n",
      "Epoch: 001/008 | Batch 041/266 | Loss: 0.6859\n",
      "Epoch: 001/008 | Batch 042/266 | Loss: 0.6911\n",
      "Epoch: 001/008 | Batch 043/266 | Loss: 0.6908\n",
      "Epoch: 001/008 | Batch 044/266 | Loss: 0.6873\n",
      "Epoch: 001/008 | Batch 045/266 | Loss: 0.6853\n",
      "Epoch: 001/008 | Batch 046/266 | Loss: 0.6882\n",
      "Epoch: 001/008 | Batch 047/266 | Loss: 0.6908\n",
      "Epoch: 001/008 | Batch 048/266 | Loss: 0.6970\n",
      "Epoch: 001/008 | Batch 049/266 | Loss: 0.6876\n",
      "Epoch: 001/008 | Batch 050/266 | Loss: 0.6832\n",
      "Epoch: 001/008 | Batch 051/266 | Loss: 0.6866\n",
      "Epoch: 001/008 | Batch 052/266 | Loss: 0.6875\n",
      "Epoch: 001/008 | Batch 053/266 | Loss: 0.6852\n",
      "Epoch: 001/008 | Batch 054/266 | Loss: 0.6963\n",
      "Epoch: 001/008 | Batch 055/266 | Loss: 0.6925\n",
      "Epoch: 001/008 | Batch 056/266 | Loss: 0.6860\n",
      "Epoch: 001/008 | Batch 057/266 | Loss: 0.6851\n",
      "Epoch: 001/008 | Batch 058/266 | Loss: 0.6918\n",
      "Epoch: 001/008 | Batch 059/266 | Loss: 0.6906\n",
      "Epoch: 001/008 | Batch 060/266 | Loss: 0.6897\n",
      "Epoch: 001/008 | Batch 061/266 | Loss: 0.6898\n",
      "Epoch: 001/008 | Batch 062/266 | Loss: 0.6874\n",
      "Epoch: 001/008 | Batch 063/266 | Loss: 0.6887\n",
      "Epoch: 001/008 | Batch 064/266 | Loss: 0.6920\n",
      "Epoch: 001/008 | Batch 065/266 | Loss: 0.6979\n",
      "Epoch: 001/008 | Batch 066/266 | Loss: 0.6879\n",
      "Epoch: 001/008 | Batch 067/266 | Loss: 0.7040\n",
      "Epoch: 001/008 | Batch 068/266 | Loss: 0.6861\n",
      "Epoch: 001/008 | Batch 069/266 | Loss: 0.6890\n",
      "Epoch: 001/008 | Batch 070/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 071/266 | Loss: 0.6966\n",
      "Epoch: 001/008 | Batch 072/266 | Loss: 0.6867\n",
      "Epoch: 001/008 | Batch 073/266 | Loss: 0.6867\n",
      "Epoch: 001/008 | Batch 074/266 | Loss: 0.6890\n",
      "Epoch: 001/008 | Batch 075/266 | Loss: 0.6879\n",
      "Epoch: 001/008 | Batch 076/266 | Loss: 0.6924\n",
      "Epoch: 001/008 | Batch 077/266 | Loss: 0.6855\n",
      "Epoch: 001/008 | Batch 078/266 | Loss: 0.6893\n",
      "Epoch: 001/008 | Batch 079/266 | Loss: 0.6931\n",
      "Epoch: 001/008 | Batch 080/266 | Loss: 0.6888\n",
      "Epoch: 001/008 | Batch 081/266 | Loss: 0.6917\n",
      "Epoch: 001/008 | Batch 082/266 | Loss: 0.6920\n",
      "Epoch: 001/008 | Batch 083/266 | Loss: 0.6891\n",
      "Epoch: 001/008 | Batch 084/266 | Loss: 0.6877\n",
      "Epoch: 001/008 | Batch 085/266 | Loss: 0.6869\n",
      "Epoch: 001/008 | Batch 086/266 | Loss: 0.6910\n",
      "Epoch: 001/008 | Batch 087/266 | Loss: 0.6900\n",
      "Epoch: 001/008 | Batch 088/266 | Loss: 0.6887\n",
      "Epoch: 001/008 | Batch 089/266 | Loss: 0.6885\n",
      "Epoch: 001/008 | Batch 090/266 | Loss: 0.7047\n",
      "Epoch: 001/008 | Batch 091/266 | Loss: 0.6903\n",
      "Epoch: 001/008 | Batch 092/266 | Loss: 0.6880\n",
      "Epoch: 001/008 | Batch 093/266 | Loss: 0.6879\n",
      "Epoch: 001/008 | Batch 094/266 | Loss: 0.6880\n",
      "Epoch: 001/008 | Batch 095/266 | Loss: 0.6876\n",
      "Epoch: 001/008 | Batch 096/266 | Loss: 0.6891\n",
      "Epoch: 001/008 | Batch 097/266 | Loss: 0.7082\n",
      "Epoch: 001/008 | Batch 098/266 | Loss: 0.6871\n",
      "Epoch: 001/008 | Batch 099/266 | Loss: 0.7010\n",
      "Epoch: 001/008 | Batch 100/266 | Loss: 0.6869\n",
      "Epoch: 001/008 | Batch 101/266 | Loss: 0.6884\n",
      "Epoch: 001/008 | Batch 102/266 | Loss: 0.6894\n",
      "Epoch: 001/008 | Batch 103/266 | Loss: 0.6960\n",
      "Epoch: 001/008 | Batch 104/266 | Loss: 0.6882\n",
      "Epoch: 001/008 | Batch 105/266 | Loss: 0.6862\n",
      "Epoch: 001/008 | Batch 106/266 | Loss: 0.6871\n",
      "Epoch: 001/008 | Batch 107/266 | Loss: 0.6899\n",
      "Epoch: 001/008 | Batch 108/266 | Loss: 0.6898\n",
      "Epoch: 001/008 | Batch 109/266 | Loss: 0.6880\n",
      "Epoch: 001/008 | Batch 110/266 | Loss: 0.6866\n",
      "Epoch: 001/008 | Batch 111/266 | Loss: 0.6898\n",
      "Epoch: 001/008 | Batch 112/266 | Loss: 0.6873\n",
      "Epoch: 001/008 | Batch 113/266 | Loss: 0.6919\n",
      "Epoch: 001/008 | Batch 114/266 | Loss: 0.6882\n",
      "Epoch: 001/008 | Batch 115/266 | Loss: 0.7199\n",
      "Epoch: 001/008 | Batch 116/266 | Loss: 0.7113\n",
      "Epoch: 001/008 | Batch 117/266 | Loss: 0.6871\n",
      "Epoch: 001/008 | Batch 118/266 | Loss: 0.6852\n",
      "Epoch: 001/008 | Batch 119/266 | Loss: 0.6932\n",
      "Epoch: 001/008 | Batch 120/266 | Loss: 0.6854\n",
      "Epoch: 001/008 | Batch 121/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 122/266 | Loss: 0.6865\n",
      "Epoch: 001/008 | Batch 123/266 | Loss: 0.7084\n",
      "Epoch: 001/008 | Batch 124/266 | Loss: 0.6862\n",
      "Epoch: 001/008 | Batch 125/266 | Loss: 0.6988\n",
      "Epoch: 001/008 | Batch 126/266 | Loss: 0.6931\n",
      "Epoch: 001/008 | Batch 127/266 | Loss: 0.6839\n",
      "Epoch: 001/008 | Batch 128/266 | Loss: 0.6858\n",
      "Epoch: 001/008 | Batch 129/266 | Loss: 0.6885\n",
      "Epoch: 001/008 | Batch 130/266 | Loss: 0.6948\n",
      "Epoch: 001/008 | Batch 131/266 | Loss: 0.6889\n",
      "Epoch: 001/008 | Batch 132/266 | Loss: 0.6866\n",
      "Epoch: 001/008 | Batch 133/266 | Loss: 0.6920\n",
      "Epoch: 001/008 | Batch 134/266 | Loss: 0.6968\n",
      "Epoch: 001/008 | Batch 135/266 | Loss: 0.6856\n",
      "Epoch: 001/008 | Batch 136/266 | Loss: 0.6903\n",
      "Epoch: 001/008 | Batch 137/266 | Loss: 0.6902\n",
      "Epoch: 001/008 | Batch 138/266 | Loss: 0.6905\n",
      "Epoch: 001/008 | Batch 139/266 | Loss: 0.6856\n",
      "Epoch: 001/008 | Batch 140/266 | Loss: 0.6894\n",
      "Epoch: 001/008 | Batch 141/266 | Loss: 0.6923\n",
      "Epoch: 001/008 | Batch 142/266 | Loss: 0.6930\n",
      "Epoch: 001/008 | Batch 143/266 | Loss: 0.6944\n",
      "Epoch: 001/008 | Batch 144/266 | Loss: 0.6957\n",
      "Epoch: 001/008 | Batch 145/266 | Loss: 0.6897\n",
      "Epoch: 001/008 | Batch 146/266 | Loss: 0.6899\n",
      "Epoch: 001/008 | Batch 147/266 | Loss: 0.6880\n",
      "Epoch: 001/008 | Batch 148/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 149/266 | Loss: 0.6875\n",
      "Epoch: 001/008 | Batch 150/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 151/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 152/266 | Loss: 0.6888\n",
      "Epoch: 001/008 | Batch 153/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 154/266 | Loss: 0.6883\n",
      "Epoch: 001/008 | Batch 155/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 156/266 | Loss: 0.6897\n",
      "Epoch: 001/008 | Batch 157/266 | Loss: 0.6889\n",
      "Epoch: 001/008 | Batch 158/266 | Loss: 0.6873\n",
      "Epoch: 001/008 | Batch 159/266 | Loss: 0.6867\n",
      "Epoch: 001/008 | Batch 160/266 | Loss: 0.6936\n",
      "Epoch: 001/008 | Batch 161/266 | Loss: 0.6882\n",
      "Epoch: 001/008 | Batch 162/266 | Loss: 0.6875\n",
      "Epoch: 001/008 | Batch 163/266 | Loss: 0.6873\n",
      "Epoch: 001/008 | Batch 164/266 | Loss: 0.6879\n",
      "Epoch: 001/008 | Batch 165/266 | Loss: 0.6904\n",
      "Epoch: 001/008 | Batch 166/266 | Loss: 0.6937\n",
      "Epoch: 001/008 | Batch 167/266 | Loss: 0.6908\n",
      "Epoch: 001/008 | Batch 168/266 | Loss: 0.6871\n",
      "Epoch: 001/008 | Batch 169/266 | Loss: 0.6888\n",
      "Epoch: 001/008 | Batch 170/266 | Loss: 0.6897\n",
      "Epoch: 001/008 | Batch 171/266 | Loss: 0.6875\n",
      "Epoch: 001/008 | Batch 172/266 | Loss: 0.6884\n",
      "Epoch: 001/008 | Batch 173/266 | Loss: 0.6888\n",
      "Epoch: 001/008 | Batch 174/266 | Loss: 0.6886\n",
      "Epoch: 001/008 | Batch 175/266 | Loss: 0.6974\n",
      "Epoch: 001/008 | Batch 176/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 177/266 | Loss: 0.6883\n",
      "Epoch: 001/008 | Batch 178/266 | Loss: 0.6909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/008 | Batch 179/266 | Loss: 0.6907\n",
      "Epoch: 001/008 | Batch 180/266 | Loss: 0.6897\n",
      "Epoch: 001/008 | Batch 181/266 | Loss: 0.6905\n",
      "Epoch: 001/008 | Batch 182/266 | Loss: 0.6944\n",
      "Epoch: 001/008 | Batch 183/266 | Loss: 0.6875\n",
      "Epoch: 001/008 | Batch 184/266 | Loss: 0.6884\n",
      "Epoch: 001/008 | Batch 185/266 | Loss: 0.6919\n",
      "Epoch: 001/008 | Batch 186/266 | Loss: 0.6888\n",
      "Epoch: 001/008 | Batch 187/266 | Loss: 0.6871\n",
      "Epoch: 001/008 | Batch 188/266 | Loss: 0.6887\n",
      "Epoch: 001/008 | Batch 189/266 | Loss: 0.6875\n",
      "Epoch: 001/008 | Batch 190/266 | Loss: 0.6873\n",
      "Epoch: 001/008 | Batch 191/266 | Loss: 0.6847\n",
      "Epoch: 001/008 | Batch 192/266 | Loss: 0.6877\n",
      "Epoch: 001/008 | Batch 193/266 | Loss: 0.6831\n",
      "Epoch: 001/008 | Batch 194/266 | Loss: 0.6876\n",
      "Epoch: 001/008 | Batch 195/266 | Loss: 0.6901\n",
      "Epoch: 001/008 | Batch 196/266 | Loss: 0.6918\n",
      "Epoch: 001/008 | Batch 197/266 | Loss: 0.6847\n",
      "Epoch: 001/008 | Batch 198/266 | Loss: 0.6925\n",
      "Epoch: 001/008 | Batch 199/266 | Loss: 0.6905\n",
      "Epoch: 001/008 | Batch 200/266 | Loss: 0.6897\n",
      "Epoch: 001/008 | Batch 201/266 | Loss: 0.6876\n",
      "Epoch: 001/008 | Batch 202/266 | Loss: 0.7046\n",
      "Epoch: 001/008 | Batch 203/266 | Loss: 0.6914\n",
      "Epoch: 001/008 | Batch 204/266 | Loss: 0.6898\n",
      "Epoch: 001/008 | Batch 205/266 | Loss: 0.6936\n",
      "Epoch: 001/008 | Batch 206/266 | Loss: 0.6903\n",
      "Epoch: 001/008 | Batch 207/266 | Loss: 0.6903\n",
      "Epoch: 001/008 | Batch 208/266 | Loss: 0.6901\n",
      "Epoch: 001/008 | Batch 209/266 | Loss: 0.6897\n",
      "Epoch: 001/008 | Batch 210/266 | Loss: 0.6875\n",
      "Epoch: 001/008 | Batch 211/266 | Loss: 0.6876\n",
      "Epoch: 001/008 | Batch 212/266 | Loss: 0.6871\n",
      "Epoch: 001/008 | Batch 213/266 | Loss: 0.6875\n",
      "Epoch: 001/008 | Batch 214/266 | Loss: 0.6885\n",
      "Epoch: 001/008 | Batch 215/266 | Loss: 0.6869\n",
      "Epoch: 001/008 | Batch 216/266 | Loss: 0.6873\n",
      "Epoch: 001/008 | Batch 217/266 | Loss: 0.7047\n",
      "Epoch: 001/008 | Batch 218/266 | Loss: 0.6880\n",
      "Epoch: 001/008 | Batch 219/266 | Loss: 0.6902\n",
      "Epoch: 001/008 | Batch 220/266 | Loss: 0.6945\n",
      "Epoch: 001/008 | Batch 221/266 | Loss: 0.6889\n",
      "Epoch: 001/008 | Batch 222/266 | Loss: 0.6889\n",
      "Epoch: 001/008 | Batch 223/266 | Loss: 0.6990\n",
      "Epoch: 001/008 | Batch 224/266 | Loss: 0.6884\n",
      "Epoch: 001/008 | Batch 225/266 | Loss: 0.6877\n",
      "Epoch: 001/008 | Batch 226/266 | Loss: 0.6877\n",
      "Epoch: 001/008 | Batch 227/266 | Loss: 0.6912\n",
      "Epoch: 001/008 | Batch 228/266 | Loss: 0.7110\n",
      "Epoch: 001/008 | Batch 229/266 | Loss: 0.6884\n",
      "Epoch: 001/008 | Batch 230/266 | Loss: 0.6894\n",
      "Epoch: 001/008 | Batch 231/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 232/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 233/266 | Loss: 0.6879\n",
      "Epoch: 001/008 | Batch 234/266 | Loss: 0.6884\n",
      "Epoch: 001/008 | Batch 235/266 | Loss: 0.7010\n",
      "Epoch: 001/008 | Batch 236/266 | Loss: 0.6888\n",
      "Epoch: 001/008 | Batch 237/266 | Loss: 0.6885\n",
      "Epoch: 001/008 | Batch 238/266 | Loss: 0.6882\n",
      "Epoch: 001/008 | Batch 239/266 | Loss: 0.6876\n",
      "Epoch: 001/008 | Batch 240/266 | Loss: 0.6903\n",
      "Epoch: 001/008 | Batch 241/266 | Loss: 0.6891\n",
      "Epoch: 001/008 | Batch 242/266 | Loss: 0.6883\n",
      "Epoch: 001/008 | Batch 243/266 | Loss: 0.6884\n",
      "Epoch: 001/008 | Batch 244/266 | Loss: 0.6879\n",
      "Epoch: 001/008 | Batch 245/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 246/266 | Loss: 0.6883\n",
      "Epoch: 001/008 | Batch 247/266 | Loss: 0.6887\n",
      "Epoch: 001/008 | Batch 248/266 | Loss: 0.6889\n",
      "Epoch: 001/008 | Batch 249/266 | Loss: 0.6886\n",
      "Epoch: 001/008 | Batch 250/266 | Loss: 0.6901\n",
      "Epoch: 001/008 | Batch 251/266 | Loss: 0.6877\n",
      "Epoch: 001/008 | Batch 252/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 253/266 | Loss: 0.6906\n",
      "Epoch: 001/008 | Batch 254/266 | Loss: 0.6875\n",
      "Epoch: 001/008 | Batch 255/266 | Loss: 0.6832\n",
      "Epoch: 001/008 | Batch 256/266 | Loss: 0.6903\n",
      "Epoch: 001/008 | Batch 257/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 258/266 | Loss: 0.6894\n",
      "Epoch: 001/008 | Batch 259/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 260/266 | Loss: 0.6878\n",
      "Epoch: 001/008 | Batch 261/266 | Loss: 0.6846\n",
      "Epoch: 001/008 | Batch 262/266 | Loss: 0.6912\n",
      "Epoch: 001/008 | Batch 263/266 | Loss: 0.6914\n",
      "Epoch: 001/008 | Batch 264/266 | Loss: 0.6881\n",
      "Epoch: 001/008 | Batch 265/266 | Loss: 0.6877\n",
      "training accuracy: 50.37%\n",
      "valid accuracy: 49.82%\n",
      "Time elapsed: 4.49 min\n",
      "Epoch: 002/008 | Batch 000/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 001/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 002/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 003/266 | Loss: 0.6927\n",
      "Epoch: 002/008 | Batch 004/266 | Loss: 0.6937\n",
      "Epoch: 002/008 | Batch 005/266 | Loss: 0.6887\n",
      "Epoch: 002/008 | Batch 006/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 007/266 | Loss: 0.6882\n",
      "Epoch: 002/008 | Batch 008/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 009/266 | Loss: 0.6884\n",
      "Epoch: 002/008 | Batch 010/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 011/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 012/266 | Loss: 0.6909\n",
      "Epoch: 002/008 | Batch 013/266 | Loss: 0.6877\n",
      "Epoch: 002/008 | Batch 014/266 | Loss: 0.6884\n",
      "Epoch: 002/008 | Batch 015/266 | Loss: 0.6950\n",
      "Epoch: 002/008 | Batch 016/266 | Loss: 0.6974\n",
      "Epoch: 002/008 | Batch 017/266 | Loss: 0.6889\n",
      "Epoch: 002/008 | Batch 018/266 | Loss: 0.6897\n",
      "Epoch: 002/008 | Batch 019/266 | Loss: 0.6872\n",
      "Epoch: 002/008 | Batch 020/266 | Loss: 0.6885\n",
      "Epoch: 002/008 | Batch 021/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 022/266 | Loss: 0.7075\n",
      "Epoch: 002/008 | Batch 023/266 | Loss: 0.6890\n",
      "Epoch: 002/008 | Batch 024/266 | Loss: 0.6886\n",
      "Epoch: 002/008 | Batch 025/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 026/266 | Loss: 0.6913\n",
      "Epoch: 002/008 | Batch 027/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 028/266 | Loss: 0.6880\n",
      "Epoch: 002/008 | Batch 029/266 | Loss: 0.6893\n",
      "Epoch: 002/008 | Batch 030/266 | Loss: 0.6884\n",
      "Epoch: 002/008 | Batch 031/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 032/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 033/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 034/266 | Loss: 0.6897\n",
      "Epoch: 002/008 | Batch 035/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 036/266 | Loss: 0.6890\n",
      "Epoch: 002/008 | Batch 037/266 | Loss: 0.6880\n",
      "Epoch: 002/008 | Batch 038/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 039/266 | Loss: 0.6880\n",
      "Epoch: 002/008 | Batch 040/266 | Loss: 0.6890\n",
      "Epoch: 002/008 | Batch 041/266 | Loss: 0.6886\n",
      "Epoch: 002/008 | Batch 042/266 | Loss: 0.6891\n",
      "Epoch: 002/008 | Batch 043/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 044/266 | Loss: 0.6871\n",
      "Epoch: 002/008 | Batch 045/266 | Loss: 0.6976\n",
      "Epoch: 002/008 | Batch 046/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 047/266 | Loss: 0.6877\n",
      "Epoch: 002/008 | Batch 048/266 | Loss: 0.6892\n",
      "Epoch: 002/008 | Batch 049/266 | Loss: 0.6874\n",
      "Epoch: 002/008 | Batch 050/266 | Loss: 0.6918\n",
      "Epoch: 002/008 | Batch 051/266 | Loss: 0.6867\n",
      "Epoch: 002/008 | Batch 052/266 | Loss: 0.6917\n",
      "Epoch: 002/008 | Batch 053/266 | Loss: 0.6897\n",
      "Epoch: 002/008 | Batch 054/266 | Loss: 0.6859\n",
      "Epoch: 002/008 | Batch 055/266 | Loss: 0.6893\n",
      "Epoch: 002/008 | Batch 056/266 | Loss: 0.6906\n",
      "Epoch: 002/008 | Batch 057/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 058/266 | Loss: 0.6859\n",
      "Epoch: 002/008 | Batch 059/266 | Loss: 0.6877\n",
      "Epoch: 002/008 | Batch 060/266 | Loss: 0.6894\n",
      "Epoch: 002/008 | Batch 061/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 062/266 | Loss: 0.6882\n",
      "Epoch: 002/008 | Batch 063/266 | Loss: 0.6886\n",
      "Epoch: 002/008 | Batch 064/266 | Loss: 0.6891\n",
      "Epoch: 002/008 | Batch 065/266 | Loss: 0.7032\n",
      "Epoch: 002/008 | Batch 066/266 | Loss: 0.6899\n",
      "Epoch: 002/008 | Batch 067/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 068/266 | Loss: 0.6882\n",
      "Epoch: 002/008 | Batch 069/266 | Loss: 0.6865\n",
      "Epoch: 002/008 | Batch 070/266 | Loss: 0.6871\n",
      "Epoch: 002/008 | Batch 071/266 | Loss: 0.6892\n",
      "Epoch: 002/008 | Batch 072/266 | Loss: 0.6873\n",
      "Epoch: 002/008 | Batch 073/266 | Loss: 0.6897\n",
      "Epoch: 002/008 | Batch 074/266 | Loss: 0.7225\n",
      "Epoch: 002/008 | Batch 075/266 | Loss: 0.6887\n",
      "Epoch: 002/008 | Batch 076/266 | Loss: 0.6889\n",
      "Epoch: 002/008 | Batch 077/266 | Loss: 0.6902\n",
      "Epoch: 002/008 | Batch 078/266 | Loss: 0.6885\n",
      "Epoch: 002/008 | Batch 079/266 | Loss: 0.6926\n",
      "Epoch: 002/008 | Batch 080/266 | Loss: 0.6890\n",
      "Epoch: 002/008 | Batch 081/266 | Loss: 0.6888\n",
      "Epoch: 002/008 | Batch 082/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 083/266 | Loss: 0.6939\n",
      "Epoch: 002/008 | Batch 084/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 085/266 | Loss: 0.6877\n",
      "Epoch: 002/008 | Batch 086/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 087/266 | Loss: 0.7080\n",
      "Epoch: 002/008 | Batch 088/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 089/266 | Loss: 0.7135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002/008 | Batch 090/266 | Loss: 0.6882\n",
      "Epoch: 002/008 | Batch 091/266 | Loss: 0.6877\n",
      "Epoch: 002/008 | Batch 092/266 | Loss: 0.6872\n",
      "Epoch: 002/008 | Batch 093/266 | Loss: 0.6947\n",
      "Epoch: 002/008 | Batch 094/266 | Loss: 0.6915\n",
      "Epoch: 002/008 | Batch 095/266 | Loss: 0.6920\n",
      "Epoch: 002/008 | Batch 096/266 | Loss: 0.6860\n",
      "Epoch: 002/008 | Batch 097/266 | Loss: 0.6889\n",
      "Epoch: 002/008 | Batch 098/266 | Loss: 0.6867\n",
      "Epoch: 002/008 | Batch 099/266 | Loss: 0.6863\n",
      "Epoch: 002/008 | Batch 100/266 | Loss: 0.6870\n",
      "Epoch: 002/008 | Batch 101/266 | Loss: 0.6909\n",
      "Epoch: 002/008 | Batch 102/266 | Loss: 0.6853\n",
      "Epoch: 002/008 | Batch 103/266 | Loss: 0.6877\n",
      "Epoch: 002/008 | Batch 104/266 | Loss: 0.6927\n",
      "Epoch: 002/008 | Batch 105/266 | Loss: 0.6912\n",
      "Epoch: 002/008 | Batch 106/266 | Loss: 0.6941\n",
      "Epoch: 002/008 | Batch 107/266 | Loss: 0.6903\n",
      "Epoch: 002/008 | Batch 108/266 | Loss: 0.6896\n",
      "Epoch: 002/008 | Batch 109/266 | Loss: 0.6886\n",
      "Epoch: 002/008 | Batch 110/266 | Loss: 0.6896\n",
      "Epoch: 002/008 | Batch 111/266 | Loss: 0.6897\n",
      "Epoch: 002/008 | Batch 112/266 | Loss: 0.7196\n",
      "Epoch: 002/008 | Batch 113/266 | Loss: 0.6887\n",
      "Epoch: 002/008 | Batch 114/266 | Loss: 0.6889\n",
      "Epoch: 002/008 | Batch 115/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 116/266 | Loss: 0.6882\n",
      "Epoch: 002/008 | Batch 117/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 118/266 | Loss: 0.6970\n",
      "Epoch: 002/008 | Batch 119/266 | Loss: 0.6876\n",
      "Epoch: 002/008 | Batch 120/266 | Loss: 0.6874\n",
      "Epoch: 002/008 | Batch 121/266 | Loss: 0.6902\n",
      "Epoch: 002/008 | Batch 122/266 | Loss: 0.6923\n",
      "Epoch: 002/008 | Batch 123/266 | Loss: 0.6903\n",
      "Epoch: 002/008 | Batch 124/266 | Loss: 0.6963\n",
      "Epoch: 002/008 | Batch 125/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 126/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 127/266 | Loss: 0.6891\n",
      "Epoch: 002/008 | Batch 128/266 | Loss: 0.6917\n",
      "Epoch: 002/008 | Batch 129/266 | Loss: 0.6916\n",
      "Epoch: 002/008 | Batch 130/266 | Loss: 0.6884\n",
      "Epoch: 002/008 | Batch 131/266 | Loss: 0.6860\n",
      "Epoch: 002/008 | Batch 132/266 | Loss: 0.6911\n",
      "Epoch: 002/008 | Batch 133/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 134/266 | Loss: 0.6913\n",
      "Epoch: 002/008 | Batch 135/266 | Loss: 0.7111\n",
      "Epoch: 002/008 | Batch 136/266 | Loss: 0.6896\n",
      "Epoch: 002/008 | Batch 137/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 138/266 | Loss: 0.6884\n",
      "Epoch: 002/008 | Batch 139/266 | Loss: 0.6869\n",
      "Epoch: 002/008 | Batch 140/266 | Loss: 0.6880\n",
      "Epoch: 002/008 | Batch 141/266 | Loss: 0.6906\n",
      "Epoch: 002/008 | Batch 142/266 | Loss: 0.6899\n",
      "Epoch: 002/008 | Batch 143/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 144/266 | Loss: 0.6876\n",
      "Epoch: 002/008 | Batch 145/266 | Loss: 0.6885\n",
      "Epoch: 002/008 | Batch 146/266 | Loss: 0.6891\n",
      "Epoch: 002/008 | Batch 147/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 148/266 | Loss: 0.6884\n",
      "Epoch: 002/008 | Batch 149/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 150/266 | Loss: 0.6903\n",
      "Epoch: 002/008 | Batch 151/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 152/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 153/266 | Loss: 0.6871\n",
      "Epoch: 002/008 | Batch 154/266 | Loss: 0.6876\n",
      "Epoch: 002/008 | Batch 155/266 | Loss: 0.6932\n",
      "Epoch: 002/008 | Batch 156/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 157/266 | Loss: 0.6892\n",
      "Epoch: 002/008 | Batch 158/266 | Loss: 0.6862\n",
      "Epoch: 002/008 | Batch 159/266 | Loss: 0.6999\n",
      "Epoch: 002/008 | Batch 160/266 | Loss: 0.6885\n",
      "Epoch: 002/008 | Batch 161/266 | Loss: 0.6872\n",
      "Epoch: 002/008 | Batch 162/266 | Loss: 0.6908\n",
      "Epoch: 002/008 | Batch 163/266 | Loss: 0.6845\n",
      "Epoch: 002/008 | Batch 164/266 | Loss: 0.6908\n",
      "Epoch: 002/008 | Batch 165/266 | Loss: 0.6840\n",
      "Epoch: 002/008 | Batch 166/266 | Loss: 0.6850\n",
      "Epoch: 002/008 | Batch 167/266 | Loss: 0.6960\n",
      "Epoch: 002/008 | Batch 168/266 | Loss: 0.6922\n",
      "Epoch: 002/008 | Batch 169/266 | Loss: 0.6887\n",
      "Epoch: 002/008 | Batch 170/266 | Loss: 0.6849\n",
      "Epoch: 002/008 | Batch 171/266 | Loss: 0.6860\n",
      "Epoch: 002/008 | Batch 172/266 | Loss: 0.6834\n",
      "Epoch: 002/008 | Batch 173/266 | Loss: 0.6916\n",
      "Epoch: 002/008 | Batch 174/266 | Loss: 0.6925\n",
      "Epoch: 002/008 | Batch 175/266 | Loss: 0.6885\n",
      "Epoch: 002/008 | Batch 176/266 | Loss: 0.6893\n",
      "Epoch: 002/008 | Batch 177/266 | Loss: 0.6916\n",
      "Epoch: 002/008 | Batch 178/266 | Loss: 0.6896\n",
      "Epoch: 002/008 | Batch 179/266 | Loss: 0.6852\n",
      "Epoch: 002/008 | Batch 180/266 | Loss: 0.6906\n",
      "Epoch: 002/008 | Batch 181/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 182/266 | Loss: 0.6925\n",
      "Epoch: 002/008 | Batch 183/266 | Loss: 0.6868\n",
      "Epoch: 002/008 | Batch 184/266 | Loss: 0.6850\n",
      "Epoch: 002/008 | Batch 185/266 | Loss: 0.6872\n",
      "Epoch: 002/008 | Batch 186/266 | Loss: 0.6910\n",
      "Epoch: 002/008 | Batch 187/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 188/266 | Loss: 0.6897\n",
      "Epoch: 002/008 | Batch 189/266 | Loss: 0.6978\n",
      "Epoch: 002/008 | Batch 190/266 | Loss: 0.6899\n",
      "Epoch: 002/008 | Batch 191/266 | Loss: 0.6904\n",
      "Epoch: 002/008 | Batch 192/266 | Loss: 0.6928\n",
      "Epoch: 002/008 | Batch 193/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 194/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 195/266 | Loss: 0.6936\n",
      "Epoch: 002/008 | Batch 196/266 | Loss: 0.6886\n",
      "Epoch: 002/008 | Batch 197/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 198/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 199/266 | Loss: 0.6880\n",
      "Epoch: 002/008 | Batch 200/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 201/266 | Loss: 0.6925\n",
      "Epoch: 002/008 | Batch 202/266 | Loss: 0.6874\n",
      "Epoch: 002/008 | Batch 203/266 | Loss: 0.6889\n",
      "Epoch: 002/008 | Batch 204/266 | Loss: 0.6872\n",
      "Epoch: 002/008 | Batch 205/266 | Loss: 0.6888\n",
      "Epoch: 002/008 | Batch 206/266 | Loss: 0.6911\n",
      "Epoch: 002/008 | Batch 207/266 | Loss: 0.6858\n",
      "Epoch: 002/008 | Batch 208/266 | Loss: 0.6876\n",
      "Epoch: 002/008 | Batch 209/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 210/266 | Loss: 0.6904\n",
      "Epoch: 002/008 | Batch 211/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 212/266 | Loss: 0.6859\n",
      "Epoch: 002/008 | Batch 213/266 | Loss: 0.6901\n",
      "Epoch: 002/008 | Batch 214/266 | Loss: 0.6887\n",
      "Epoch: 002/008 | Batch 215/266 | Loss: 0.6884\n",
      "Epoch: 002/008 | Batch 216/266 | Loss: 0.6899\n",
      "Epoch: 002/008 | Batch 217/266 | Loss: 0.6834\n",
      "Epoch: 002/008 | Batch 218/266 | Loss: 0.6897\n",
      "Epoch: 002/008 | Batch 219/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 220/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 221/266 | Loss: 0.7126\n",
      "Epoch: 002/008 | Batch 222/266 | Loss: 0.6870\n",
      "Epoch: 002/008 | Batch 223/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 224/266 | Loss: 0.6869\n",
      "Epoch: 002/008 | Batch 225/266 | Loss: 0.6871\n",
      "Epoch: 002/008 | Batch 226/266 | Loss: 0.6850\n",
      "Epoch: 002/008 | Batch 227/266 | Loss: 0.6895\n",
      "Epoch: 002/008 | Batch 228/266 | Loss: 0.6875\n",
      "Epoch: 002/008 | Batch 229/266 | Loss: 0.6908\n",
      "Epoch: 002/008 | Batch 230/266 | Loss: 0.6927\n",
      "Epoch: 002/008 | Batch 231/266 | Loss: 0.6979\n",
      "Epoch: 002/008 | Batch 232/266 | Loss: 0.6885\n",
      "Epoch: 002/008 | Batch 233/266 | Loss: 0.6865\n",
      "Epoch: 002/008 | Batch 234/266 | Loss: 0.6889\n",
      "Epoch: 002/008 | Batch 235/266 | Loss: 0.6876\n",
      "Epoch: 002/008 | Batch 236/266 | Loss: 0.6894\n",
      "Epoch: 002/008 | Batch 237/266 | Loss: 0.6873\n",
      "Epoch: 002/008 | Batch 238/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 239/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 240/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 241/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 242/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 243/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 244/266 | Loss: 0.6883\n",
      "Epoch: 002/008 | Batch 245/266 | Loss: 0.6880\n",
      "Epoch: 002/008 | Batch 246/266 | Loss: 0.6881\n",
      "Epoch: 002/008 | Batch 247/266 | Loss: 0.6886\n",
      "Epoch: 002/008 | Batch 248/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 249/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 250/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 251/266 | Loss: 0.6946\n",
      "Epoch: 002/008 | Batch 252/266 | Loss: 0.7212\n",
      "Epoch: 002/008 | Batch 253/266 | Loss: 0.6876\n",
      "Epoch: 002/008 | Batch 254/266 | Loss: 0.6904\n",
      "Epoch: 002/008 | Batch 255/266 | Loss: 0.6884\n",
      "Epoch: 002/008 | Batch 256/266 | Loss: 0.6890\n",
      "Epoch: 002/008 | Batch 257/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 258/266 | Loss: 0.6945\n",
      "Epoch: 002/008 | Batch 259/266 | Loss: 0.6879\n",
      "Epoch: 002/008 | Batch 260/266 | Loss: 0.6877\n",
      "Epoch: 002/008 | Batch 261/266 | Loss: 0.6878\n",
      "Epoch: 002/008 | Batch 262/266 | Loss: 0.6897\n",
      "Epoch: 002/008 | Batch 263/266 | Loss: 0.6889\n",
      "Epoch: 002/008 | Batch 264/266 | Loss: 0.6923\n",
      "Epoch: 002/008 | Batch 265/266 | Loss: 0.6878\n",
      "training accuracy: 50.49%\n",
      "valid accuracy: 50.35%\n",
      "Time elapsed: 8.77 min\n",
      "Epoch: 003/008 | Batch 000/266 | Loss: 0.6879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/008 | Batch 001/266 | Loss: 0.6888\n",
      "Epoch: 003/008 | Batch 002/266 | Loss: 0.6869\n",
      "Epoch: 003/008 | Batch 003/266 | Loss: 0.6979\n",
      "Epoch: 003/008 | Batch 004/266 | Loss: 0.6883\n",
      "Epoch: 003/008 | Batch 005/266 | Loss: 0.6880\n",
      "Epoch: 003/008 | Batch 006/266 | Loss: 0.6860\n",
      "Epoch: 003/008 | Batch 007/266 | Loss: 0.6879\n",
      "Epoch: 003/008 | Batch 008/266 | Loss: 0.6893\n",
      "Epoch: 003/008 | Batch 009/266 | Loss: 0.6869\n",
      "Epoch: 003/008 | Batch 010/266 | Loss: 0.6925\n",
      "Epoch: 003/008 | Batch 011/266 | Loss: 0.6917\n",
      "Epoch: 003/008 | Batch 012/266 | Loss: 0.6866\n",
      "Epoch: 003/008 | Batch 013/266 | Loss: 0.6869\n",
      "Epoch: 003/008 | Batch 014/266 | Loss: 0.6887\n",
      "Epoch: 003/008 | Batch 015/266 | Loss: 0.6879\n",
      "Epoch: 003/008 | Batch 016/266 | Loss: 0.6877\n",
      "Epoch: 003/008 | Batch 017/266 | Loss: 0.6900\n",
      "Epoch: 003/008 | Batch 018/266 | Loss: 0.6873\n",
      "Epoch: 003/008 | Batch 019/266 | Loss: 0.6875\n",
      "Epoch: 003/008 | Batch 020/266 | Loss: 0.6870\n",
      "Epoch: 003/008 | Batch 021/266 | Loss: 0.6875\n",
      "Epoch: 003/008 | Batch 022/266 | Loss: 0.6897\n",
      "Epoch: 003/008 | Batch 023/266 | Loss: 0.6867\n",
      "Epoch: 003/008 | Batch 024/266 | Loss: 0.6859\n",
      "Epoch: 003/008 | Batch 025/266 | Loss: 0.6789\n",
      "Epoch: 003/008 | Batch 026/266 | Loss: 0.7054\n",
      "Epoch: 003/008 | Batch 027/266 | Loss: 0.6912\n",
      "Epoch: 003/008 | Batch 028/266 | Loss: 0.6906\n",
      "Epoch: 003/008 | Batch 029/266 | Loss: 0.6773\n",
      "Epoch: 003/008 | Batch 030/266 | Loss: 0.6814\n",
      "Epoch: 003/008 | Batch 031/266 | Loss: 0.6888\n",
      "Epoch: 003/008 | Batch 032/266 | Loss: 0.6817\n",
      "Epoch: 003/008 | Batch 033/266 | Loss: 0.6855\n",
      "Epoch: 003/008 | Batch 034/266 | Loss: 0.6762\n",
      "Epoch: 003/008 | Batch 035/266 | Loss: 0.6853\n",
      "Epoch: 003/008 | Batch 036/266 | Loss: 0.6648\n",
      "Epoch: 003/008 | Batch 037/266 | Loss: 0.6872\n",
      "Epoch: 003/008 | Batch 038/266 | Loss: 0.6594\n",
      "Epoch: 003/008 | Batch 039/266 | Loss: 0.6766\n",
      "Epoch: 003/008 | Batch 040/266 | Loss: 0.6868\n",
      "Epoch: 003/008 | Batch 041/266 | Loss: 0.6775\n",
      "Epoch: 003/008 | Batch 042/266 | Loss: 0.6678\n",
      "Epoch: 003/008 | Batch 043/266 | Loss: 0.6711\n",
      "Epoch: 003/008 | Batch 044/266 | Loss: 0.6706\n",
      "Epoch: 003/008 | Batch 045/266 | Loss: 0.6680\n",
      "Epoch: 003/008 | Batch 046/266 | Loss: 0.6888\n",
      "Epoch: 003/008 | Batch 047/266 | Loss: 0.6534\n",
      "Epoch: 003/008 | Batch 048/266 | Loss: 0.6649\n",
      "Epoch: 003/008 | Batch 049/266 | Loss: 0.6724\n",
      "Epoch: 003/008 | Batch 050/266 | Loss: 0.6585\n",
      "Epoch: 003/008 | Batch 051/266 | Loss: 0.6695\n",
      "Epoch: 003/008 | Batch 052/266 | Loss: 0.6526\n",
      "Epoch: 003/008 | Batch 053/266 | Loss: 0.7085\n",
      "Epoch: 003/008 | Batch 054/266 | Loss: 0.6456\n",
      "Epoch: 003/008 | Batch 055/266 | Loss: 0.6631\n",
      "Epoch: 003/008 | Batch 056/266 | Loss: 0.6449\n",
      "Epoch: 003/008 | Batch 057/266 | Loss: 0.7011\n",
      "Epoch: 003/008 | Batch 058/266 | Loss: 0.6393\n",
      "Epoch: 003/008 | Batch 059/266 | Loss: 0.6747\n",
      "Epoch: 003/008 | Batch 060/266 | Loss: 0.6631\n",
      "Epoch: 003/008 | Batch 061/266 | Loss: 0.6661\n",
      "Epoch: 003/008 | Batch 062/266 | Loss: 0.6428\n",
      "Epoch: 003/008 | Batch 063/266 | Loss: 0.6235\n",
      "Epoch: 003/008 | Batch 064/266 | Loss: 0.6734\n",
      "Epoch: 003/008 | Batch 065/266 | Loss: 0.6222\n",
      "Epoch: 003/008 | Batch 066/266 | Loss: 0.6767\n",
      "Epoch: 003/008 | Batch 067/266 | Loss: 0.6706\n",
      "Epoch: 003/008 | Batch 068/266 | Loss: 0.5968\n",
      "Epoch: 003/008 | Batch 069/266 | Loss: 0.6742\n",
      "Epoch: 003/008 | Batch 070/266 | Loss: 0.6543\n",
      "Epoch: 003/008 | Batch 071/266 | Loss: 0.7148\n",
      "Epoch: 003/008 | Batch 072/266 | Loss: 0.6229\n",
      "Epoch: 003/008 | Batch 073/266 | Loss: 0.5949\n",
      "Epoch: 003/008 | Batch 074/266 | Loss: 0.6426\n",
      "Epoch: 003/008 | Batch 075/266 | Loss: 0.6514\n",
      "Epoch: 003/008 | Batch 076/266 | Loss: 0.6526\n",
      "Epoch: 003/008 | Batch 077/266 | Loss: 0.6863\n",
      "Epoch: 003/008 | Batch 078/266 | Loss: 0.6064\n",
      "Epoch: 003/008 | Batch 079/266 | Loss: 0.6050\n",
      "Epoch: 003/008 | Batch 080/266 | Loss: 0.6031\n",
      "Epoch: 003/008 | Batch 081/266 | Loss: 0.5661\n",
      "Epoch: 003/008 | Batch 082/266 | Loss: 0.6333\n",
      "Epoch: 003/008 | Batch 083/266 | Loss: 0.5888\n",
      "Epoch: 003/008 | Batch 084/266 | Loss: 0.6153\n",
      "Epoch: 003/008 | Batch 085/266 | Loss: 0.6118\n",
      "Epoch: 003/008 | Batch 086/266 | Loss: 0.5904\n",
      "Epoch: 003/008 | Batch 087/266 | Loss: 0.6507\n",
      "Epoch: 003/008 | Batch 088/266 | Loss: 0.5884\n",
      "Epoch: 003/008 | Batch 089/266 | Loss: 0.6252\n",
      "Epoch: 003/008 | Batch 090/266 | Loss: 0.6538\n",
      "Epoch: 003/008 | Batch 091/266 | Loss: 0.5921\n",
      "Epoch: 003/008 | Batch 092/266 | Loss: 0.6114\n",
      "Epoch: 003/008 | Batch 093/266 | Loss: 0.5579\n",
      "Epoch: 003/008 | Batch 094/266 | Loss: 0.5511\n",
      "Epoch: 003/008 | Batch 095/266 | Loss: 0.6164\n",
      "Epoch: 003/008 | Batch 096/266 | Loss: 0.6201\n",
      "Epoch: 003/008 | Batch 097/266 | Loss: 0.5751\n",
      "Epoch: 003/008 | Batch 098/266 | Loss: 0.5787\n",
      "Epoch: 003/008 | Batch 099/266 | Loss: 0.5738\n",
      "Epoch: 003/008 | Batch 100/266 | Loss: 0.6312\n",
      "Epoch: 003/008 | Batch 101/266 | Loss: 0.5371\n",
      "Epoch: 003/008 | Batch 102/266 | Loss: 0.5872\n",
      "Epoch: 003/008 | Batch 103/266 | Loss: 0.5044\n",
      "Epoch: 003/008 | Batch 104/266 | Loss: 0.5872\n",
      "Epoch: 003/008 | Batch 105/266 | Loss: 0.6067\n",
      "Epoch: 003/008 | Batch 106/266 | Loss: 0.6312\n",
      "Epoch: 003/008 | Batch 107/266 | Loss: 0.5969\n",
      "Epoch: 003/008 | Batch 108/266 | Loss: 0.5742\n",
      "Epoch: 003/008 | Batch 109/266 | Loss: 0.5449\n",
      "Epoch: 003/008 | Batch 110/266 | Loss: 0.5367\n",
      "Epoch: 003/008 | Batch 111/266 | Loss: 0.6522\n",
      "Epoch: 003/008 | Batch 112/266 | Loss: 0.5440\n",
      "Epoch: 003/008 | Batch 113/266 | Loss: 0.6191\n",
      "Epoch: 003/008 | Batch 114/266 | Loss: 0.5460\n",
      "Epoch: 003/008 | Batch 115/266 | Loss: 0.5410\n",
      "Epoch: 003/008 | Batch 116/266 | Loss: 0.5343\n",
      "Epoch: 003/008 | Batch 117/266 | Loss: 0.5939\n",
      "Epoch: 003/008 | Batch 118/266 | Loss: 0.5463\n",
      "Epoch: 003/008 | Batch 119/266 | Loss: 0.5725\n",
      "Epoch: 003/008 | Batch 120/266 | Loss: 0.6311\n",
      "Epoch: 003/008 | Batch 121/266 | Loss: 0.5183\n",
      "Epoch: 003/008 | Batch 122/266 | Loss: 0.5775\n",
      "Epoch: 003/008 | Batch 123/266 | Loss: 0.5636\n",
      "Epoch: 003/008 | Batch 124/266 | Loss: 0.5171\n",
      "Epoch: 003/008 | Batch 125/266 | Loss: 0.5525\n",
      "Epoch: 003/008 | Batch 126/266 | Loss: 0.5248\n",
      "Epoch: 003/008 | Batch 127/266 | Loss: 0.5156\n",
      "Epoch: 003/008 | Batch 128/266 | Loss: 0.5817\n",
      "Epoch: 003/008 | Batch 129/266 | Loss: 0.5448\n",
      "Epoch: 003/008 | Batch 130/266 | Loss: 0.5131\n",
      "Epoch: 003/008 | Batch 131/266 | Loss: 0.4783\n",
      "Epoch: 003/008 | Batch 132/266 | Loss: 0.4925\n",
      "Epoch: 003/008 | Batch 133/266 | Loss: 0.4827\n",
      "Epoch: 003/008 | Batch 134/266 | Loss: 0.5474\n",
      "Epoch: 003/008 | Batch 135/266 | Loss: 0.5620\n",
      "Epoch: 003/008 | Batch 136/266 | Loss: 0.5554\n",
      "Epoch: 003/008 | Batch 137/266 | Loss: 0.5831\n",
      "Epoch: 003/008 | Batch 138/266 | Loss: 0.6014\n",
      "Epoch: 003/008 | Batch 139/266 | Loss: 0.6101\n",
      "Epoch: 003/008 | Batch 140/266 | Loss: 0.6233\n",
      "Epoch: 003/008 | Batch 141/266 | Loss: 0.5429\n",
      "Epoch: 003/008 | Batch 142/266 | Loss: 0.5434\n",
      "Epoch: 003/008 | Batch 143/266 | Loss: 0.5316\n",
      "Epoch: 003/008 | Batch 144/266 | Loss: 0.5617\n",
      "Epoch: 003/008 | Batch 145/266 | Loss: 0.5303\n",
      "Epoch: 003/008 | Batch 146/266 | Loss: 0.5661\n",
      "Epoch: 003/008 | Batch 147/266 | Loss: 0.5133\n",
      "Epoch: 003/008 | Batch 148/266 | Loss: 0.5425\n",
      "Epoch: 003/008 | Batch 149/266 | Loss: 0.5977\n",
      "Epoch: 003/008 | Batch 150/266 | Loss: 0.6070\n",
      "Epoch: 003/008 | Batch 151/266 | Loss: 0.4697\n",
      "Epoch: 003/008 | Batch 152/266 | Loss: 0.5222\n",
      "Epoch: 003/008 | Batch 153/266 | Loss: 0.5352\n",
      "Epoch: 003/008 | Batch 154/266 | Loss: 0.4808\n",
      "Epoch: 003/008 | Batch 155/266 | Loss: 0.4932\n",
      "Epoch: 003/008 | Batch 156/266 | Loss: 0.5737\n",
      "Epoch: 003/008 | Batch 157/266 | Loss: 0.5453\n",
      "Epoch: 003/008 | Batch 158/266 | Loss: 0.5961\n",
      "Epoch: 003/008 | Batch 159/266 | Loss: 0.4893\n",
      "Epoch: 003/008 | Batch 160/266 | Loss: 0.5243\n",
      "Epoch: 003/008 | Batch 161/266 | Loss: 0.4538\n",
      "Epoch: 003/008 | Batch 162/266 | Loss: 0.4798\n",
      "Epoch: 003/008 | Batch 163/266 | Loss: 0.4862\n",
      "Epoch: 003/008 | Batch 164/266 | Loss: 0.4802\n",
      "Epoch: 003/008 | Batch 165/266 | Loss: 0.4849\n",
      "Epoch: 003/008 | Batch 166/266 | Loss: 0.4917\n",
      "Epoch: 003/008 | Batch 167/266 | Loss: 0.4087\n",
      "Epoch: 003/008 | Batch 168/266 | Loss: 0.5305\n",
      "Epoch: 003/008 | Batch 169/266 | Loss: 0.4671\n",
      "Epoch: 003/008 | Batch 170/266 | Loss: 0.4799\n",
      "Epoch: 003/008 | Batch 171/266 | Loss: 0.4656\n",
      "Epoch: 003/008 | Batch 172/266 | Loss: 0.5338\n",
      "Epoch: 003/008 | Batch 173/266 | Loss: 0.5943\n",
      "Epoch: 003/008 | Batch 174/266 | Loss: 0.5337\n",
      "Epoch: 003/008 | Batch 175/266 | Loss: 0.5478\n",
      "Epoch: 003/008 | Batch 176/266 | Loss: 0.5362\n",
      "Epoch: 003/008 | Batch 177/266 | Loss: 0.3985\n",
      "Epoch: 003/008 | Batch 178/266 | Loss: 0.4665\n",
      "Epoch: 003/008 | Batch 179/266 | Loss: 0.4697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/008 | Batch 180/266 | Loss: 0.4722\n",
      "Epoch: 003/008 | Batch 181/266 | Loss: 0.5038\n",
      "Epoch: 003/008 | Batch 182/266 | Loss: 0.5696\n",
      "Epoch: 003/008 | Batch 183/266 | Loss: 0.4981\n",
      "Epoch: 003/008 | Batch 184/266 | Loss: 0.3751\n",
      "Epoch: 003/008 | Batch 185/266 | Loss: 0.4263\n",
      "Epoch: 003/008 | Batch 186/266 | Loss: 0.4425\n",
      "Epoch: 003/008 | Batch 187/266 | Loss: 0.4750\n",
      "Epoch: 003/008 | Batch 188/266 | Loss: 0.5335\n",
      "Epoch: 003/008 | Batch 189/266 | Loss: 0.4419\n",
      "Epoch: 003/008 | Batch 190/266 | Loss: 0.4706\n",
      "Epoch: 003/008 | Batch 191/266 | Loss: 0.4373\n",
      "Epoch: 003/008 | Batch 192/266 | Loss: 0.3908\n",
      "Epoch: 003/008 | Batch 193/266 | Loss: 0.4861\n",
      "Epoch: 003/008 | Batch 194/266 | Loss: 0.5015\n",
      "Epoch: 003/008 | Batch 195/266 | Loss: 0.3710\n",
      "Epoch: 003/008 | Batch 196/266 | Loss: 0.4185\n",
      "Epoch: 003/008 | Batch 197/266 | Loss: 0.4018\n",
      "Epoch: 003/008 | Batch 198/266 | Loss: 0.3985\n",
      "Epoch: 003/008 | Batch 199/266 | Loss: 0.4885\n",
      "Epoch: 003/008 | Batch 200/266 | Loss: 0.4255\n",
      "Epoch: 003/008 | Batch 201/266 | Loss: 0.4725\n",
      "Epoch: 003/008 | Batch 202/266 | Loss: 0.5907\n",
      "Epoch: 003/008 | Batch 203/266 | Loss: 0.4474\n",
      "Epoch: 003/008 | Batch 204/266 | Loss: 0.3899\n",
      "Epoch: 003/008 | Batch 205/266 | Loss: 0.4465\n",
      "Epoch: 003/008 | Batch 206/266 | Loss: 0.4406\n",
      "Epoch: 003/008 | Batch 207/266 | Loss: 0.4513\n",
      "Epoch: 003/008 | Batch 208/266 | Loss: 0.4549\n",
      "Epoch: 003/008 | Batch 209/266 | Loss: 0.4315\n",
      "Epoch: 003/008 | Batch 210/266 | Loss: 0.3892\n",
      "Epoch: 003/008 | Batch 211/266 | Loss: 0.4084\n",
      "Epoch: 003/008 | Batch 212/266 | Loss: 0.4385\n",
      "Epoch: 003/008 | Batch 213/266 | Loss: 0.4727\n",
      "Epoch: 003/008 | Batch 214/266 | Loss: 0.4537\n",
      "Epoch: 003/008 | Batch 215/266 | Loss: 0.3945\n",
      "Epoch: 003/008 | Batch 216/266 | Loss: 0.4302\n",
      "Epoch: 003/008 | Batch 217/266 | Loss: 0.3903\n",
      "Epoch: 003/008 | Batch 218/266 | Loss: 0.4271\n",
      "Epoch: 003/008 | Batch 219/266 | Loss: 0.4708\n",
      "Epoch: 003/008 | Batch 220/266 | Loss: 0.4491\n",
      "Epoch: 003/008 | Batch 221/266 | Loss: 0.4034\n",
      "Epoch: 003/008 | Batch 222/266 | Loss: 0.4050\n",
      "Epoch: 003/008 | Batch 223/266 | Loss: 0.4158\n",
      "Epoch: 003/008 | Batch 224/266 | Loss: 0.3706\n",
      "Epoch: 003/008 | Batch 225/266 | Loss: 0.4480\n",
      "Epoch: 003/008 | Batch 226/266 | Loss: 0.4371\n",
      "Epoch: 003/008 | Batch 227/266 | Loss: 0.3536\n",
      "Epoch: 003/008 | Batch 228/266 | Loss: 0.3810\n",
      "Epoch: 003/008 | Batch 229/266 | Loss: 0.3254\n",
      "Epoch: 003/008 | Batch 230/266 | Loss: 0.4423\n",
      "Epoch: 003/008 | Batch 231/266 | Loss: 0.3637\n",
      "Epoch: 003/008 | Batch 232/266 | Loss: 0.3684\n",
      "Epoch: 003/008 | Batch 233/266 | Loss: 0.4718\n",
      "Epoch: 003/008 | Batch 234/266 | Loss: 0.4963\n",
      "Epoch: 003/008 | Batch 235/266 | Loss: 0.4092\n",
      "Epoch: 003/008 | Batch 236/266 | Loss: 0.3749\n",
      "Epoch: 003/008 | Batch 237/266 | Loss: 0.3864\n",
      "Epoch: 003/008 | Batch 238/266 | Loss: 0.3998\n",
      "Epoch: 003/008 | Batch 239/266 | Loss: 0.4275\n",
      "Epoch: 003/008 | Batch 240/266 | Loss: 0.3810\n",
      "Epoch: 003/008 | Batch 241/266 | Loss: 0.4845\n",
      "Epoch: 003/008 | Batch 242/266 | Loss: 0.4339\n",
      "Epoch: 003/008 | Batch 243/266 | Loss: 0.3440\n",
      "Epoch: 003/008 | Batch 244/266 | Loss: 0.4019\n",
      "Epoch: 003/008 | Batch 245/266 | Loss: 0.3967\n",
      "Epoch: 003/008 | Batch 246/266 | Loss: 0.4772\n",
      "Epoch: 003/008 | Batch 247/266 | Loss: 0.3781\n",
      "Epoch: 003/008 | Batch 248/266 | Loss: 0.3086\n",
      "Epoch: 003/008 | Batch 249/266 | Loss: 0.3162\n",
      "Epoch: 003/008 | Batch 250/266 | Loss: 0.3804\n",
      "Epoch: 003/008 | Batch 251/266 | Loss: 0.3759\n",
      "Epoch: 003/008 | Batch 252/266 | Loss: 0.3572\n",
      "Epoch: 003/008 | Batch 253/266 | Loss: 0.3569\n",
      "Epoch: 003/008 | Batch 254/266 | Loss: 0.4616\n",
      "Epoch: 003/008 | Batch 255/266 | Loss: 0.4431\n",
      "Epoch: 003/008 | Batch 256/266 | Loss: 0.4291\n",
      "Epoch: 003/008 | Batch 257/266 | Loss: 0.2769\n",
      "Epoch: 003/008 | Batch 258/266 | Loss: 0.3893\n",
      "Epoch: 003/008 | Batch 259/266 | Loss: 0.3460\n",
      "Epoch: 003/008 | Batch 260/266 | Loss: 0.4535\n",
      "Epoch: 003/008 | Batch 261/266 | Loss: 0.3536\n",
      "Epoch: 003/008 | Batch 262/266 | Loss: 0.3385\n",
      "Epoch: 003/008 | Batch 263/266 | Loss: 0.3243\n",
      "Epoch: 003/008 | Batch 264/266 | Loss: 0.2835\n",
      "Epoch: 003/008 | Batch 265/266 | Loss: 0.3061\n",
      "training accuracy: 86.85%\n",
      "valid accuracy: 83.65%\n",
      "Time elapsed: 11.62 min\n",
      "Epoch: 004/008 | Batch 000/266 | Loss: 0.2750\n",
      "Epoch: 004/008 | Batch 001/266 | Loss: 0.3352\n",
      "Epoch: 004/008 | Batch 002/266 | Loss: 0.3702\n",
      "Epoch: 004/008 | Batch 003/266 | Loss: 0.3507\n",
      "Epoch: 004/008 | Batch 004/266 | Loss: 0.4381\n",
      "Epoch: 004/008 | Batch 005/266 | Loss: 0.3439\n",
      "Epoch: 004/008 | Batch 006/266 | Loss: 0.3464\n",
      "Epoch: 004/008 | Batch 007/266 | Loss: 0.3284\n",
      "Epoch: 004/008 | Batch 008/266 | Loss: 0.3217\n",
      "Epoch: 004/008 | Batch 009/266 | Loss: 0.3743\n",
      "Epoch: 004/008 | Batch 010/266 | Loss: 0.4043\n",
      "Epoch: 004/008 | Batch 011/266 | Loss: 0.2979\n",
      "Epoch: 004/008 | Batch 012/266 | Loss: 0.3865\n",
      "Epoch: 004/008 | Batch 013/266 | Loss: 0.2565\n",
      "Epoch: 004/008 | Batch 014/266 | Loss: 0.2773\n",
      "Epoch: 004/008 | Batch 015/266 | Loss: 0.3639\n",
      "Epoch: 004/008 | Batch 016/266 | Loss: 0.2619\n",
      "Epoch: 004/008 | Batch 017/266 | Loss: 0.3173\n",
      "Epoch: 004/008 | Batch 018/266 | Loss: 0.2675\n",
      "Epoch: 004/008 | Batch 019/266 | Loss: 0.4440\n",
      "Epoch: 004/008 | Batch 020/266 | Loss: 0.3354\n",
      "Epoch: 004/008 | Batch 021/266 | Loss: 0.3084\n",
      "Epoch: 004/008 | Batch 022/266 | Loss: 0.4289\n",
      "Epoch: 004/008 | Batch 023/266 | Loss: 0.4103\n",
      "Epoch: 004/008 | Batch 024/266 | Loss: 0.2999\n",
      "Epoch: 004/008 | Batch 025/266 | Loss: 0.3056\n",
      "Epoch: 004/008 | Batch 026/266 | Loss: 0.2741\n",
      "Epoch: 004/008 | Batch 027/266 | Loss: 0.3908\n",
      "Epoch: 004/008 | Batch 028/266 | Loss: 0.2633\n",
      "Epoch: 004/008 | Batch 029/266 | Loss: 0.3085\n",
      "Epoch: 004/008 | Batch 030/266 | Loss: 0.2670\n",
      "Epoch: 004/008 | Batch 031/266 | Loss: 0.2805\n",
      "Epoch: 004/008 | Batch 032/266 | Loss: 0.3940\n",
      "Epoch: 004/008 | Batch 033/266 | Loss: 0.2684\n",
      "Epoch: 004/008 | Batch 034/266 | Loss: 0.3017\n",
      "Epoch: 004/008 | Batch 035/266 | Loss: 0.2457\n",
      "Epoch: 004/008 | Batch 036/266 | Loss: 0.2504\n",
      "Epoch: 004/008 | Batch 037/266 | Loss: 0.3416\n",
      "Epoch: 004/008 | Batch 038/266 | Loss: 0.3012\n",
      "Epoch: 004/008 | Batch 039/266 | Loss: 0.3798\n",
      "Epoch: 004/008 | Batch 040/266 | Loss: 0.2654\n",
      "Epoch: 004/008 | Batch 041/266 | Loss: 0.2643\n",
      "Epoch: 004/008 | Batch 042/266 | Loss: 0.3142\n",
      "Epoch: 004/008 | Batch 043/266 | Loss: 0.2567\n",
      "Epoch: 004/008 | Batch 044/266 | Loss: 0.2932\n",
      "Epoch: 004/008 | Batch 045/266 | Loss: 0.2565\n",
      "Epoch: 004/008 | Batch 046/266 | Loss: 0.3554\n",
      "Epoch: 004/008 | Batch 047/266 | Loss: 0.2849\n",
      "Epoch: 004/008 | Batch 048/266 | Loss: 0.4107\n",
      "Epoch: 004/008 | Batch 049/266 | Loss: 0.2934\n",
      "Epoch: 004/008 | Batch 050/266 | Loss: 0.2929\n",
      "Epoch: 004/008 | Batch 051/266 | Loss: 0.4254\n",
      "Epoch: 004/008 | Batch 052/266 | Loss: 0.3187\n",
      "Epoch: 004/008 | Batch 053/266 | Loss: 0.3607\n",
      "Epoch: 004/008 | Batch 054/266 | Loss: 0.4095\n",
      "Epoch: 004/008 | Batch 055/266 | Loss: 0.3542\n",
      "Epoch: 004/008 | Batch 056/266 | Loss: 0.2382\n",
      "Epoch: 004/008 | Batch 057/266 | Loss: 0.3026\n",
      "Epoch: 004/008 | Batch 058/266 | Loss: 0.2901\n",
      "Epoch: 004/008 | Batch 059/266 | Loss: 0.2900\n",
      "Epoch: 004/008 | Batch 060/266 | Loss: 0.3224\n",
      "Epoch: 004/008 | Batch 061/266 | Loss: 0.3443\n",
      "Epoch: 004/008 | Batch 062/266 | Loss: 0.3893\n",
      "Epoch: 004/008 | Batch 063/266 | Loss: 0.2713\n",
      "Epoch: 004/008 | Batch 064/266 | Loss: 0.2323\n",
      "Epoch: 004/008 | Batch 065/266 | Loss: 0.3305\n",
      "Epoch: 004/008 | Batch 066/266 | Loss: 0.2749\n",
      "Epoch: 004/008 | Batch 067/266 | Loss: 0.2672\n",
      "Epoch: 004/008 | Batch 068/266 | Loss: 0.3450\n",
      "Epoch: 004/008 | Batch 069/266 | Loss: 0.3443\n",
      "Epoch: 004/008 | Batch 070/266 | Loss: 0.2959\n",
      "Epoch: 004/008 | Batch 071/266 | Loss: 0.2949\n",
      "Epoch: 004/008 | Batch 072/266 | Loss: 0.3152\n",
      "Epoch: 004/008 | Batch 073/266 | Loss: 0.2488\n",
      "Epoch: 004/008 | Batch 074/266 | Loss: 0.2713\n",
      "Epoch: 004/008 | Batch 075/266 | Loss: 0.2335\n",
      "Epoch: 004/008 | Batch 076/266 | Loss: 0.2987\n",
      "Epoch: 004/008 | Batch 077/266 | Loss: 0.2909\n",
      "Epoch: 004/008 | Batch 078/266 | Loss: 0.3540\n",
      "Epoch: 004/008 | Batch 079/266 | Loss: 0.2685\n",
      "Epoch: 004/008 | Batch 080/266 | Loss: 0.3136\n",
      "Epoch: 004/008 | Batch 081/266 | Loss: 0.3405\n",
      "Epoch: 004/008 | Batch 082/266 | Loss: 0.2772\n",
      "Epoch: 004/008 | Batch 083/266 | Loss: 0.4474\n",
      "Epoch: 004/008 | Batch 084/266 | Loss: 0.2874\n",
      "Epoch: 004/008 | Batch 085/266 | Loss: 0.3224\n",
      "Epoch: 004/008 | Batch 086/266 | Loss: 0.3166\n",
      "Epoch: 004/008 | Batch 087/266 | Loss: 0.3774\n",
      "Epoch: 004/008 | Batch 088/266 | Loss: 0.3429\n",
      "Epoch: 004/008 | Batch 089/266 | Loss: 0.3247\n",
      "Epoch: 004/008 | Batch 090/266 | Loss: 0.3110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004/008 | Batch 091/266 | Loss: 0.2533\n",
      "Epoch: 004/008 | Batch 092/266 | Loss: 0.3664\n",
      "Epoch: 004/008 | Batch 093/266 | Loss: 0.4436\n",
      "Epoch: 004/008 | Batch 094/266 | Loss: 0.2301\n",
      "Epoch: 004/008 | Batch 095/266 | Loss: 0.2704\n",
      "Epoch: 004/008 | Batch 096/266 | Loss: 0.3082\n",
      "Epoch: 004/008 | Batch 097/266 | Loss: 0.2957\n",
      "Epoch: 004/008 | Batch 098/266 | Loss: 0.3244\n",
      "Epoch: 004/008 | Batch 099/266 | Loss: 0.2517\n",
      "Epoch: 004/008 | Batch 100/266 | Loss: 0.3890\n",
      "Epoch: 004/008 | Batch 101/266 | Loss: 0.3052\n",
      "Epoch: 004/008 | Batch 102/266 | Loss: 0.2922\n",
      "Epoch: 004/008 | Batch 103/266 | Loss: 0.2992\n",
      "Epoch: 004/008 | Batch 104/266 | Loss: 0.2804\n",
      "Epoch: 004/008 | Batch 105/266 | Loss: 0.3411\n",
      "Epoch: 004/008 | Batch 106/266 | Loss: 0.2767\n",
      "Epoch: 004/008 | Batch 107/266 | Loss: 0.2416\n",
      "Epoch: 004/008 | Batch 108/266 | Loss: 0.2209\n",
      "Epoch: 004/008 | Batch 109/266 | Loss: 0.2727\n",
      "Epoch: 004/008 | Batch 110/266 | Loss: 0.4273\n",
      "Epoch: 004/008 | Batch 111/266 | Loss: 0.2158\n",
      "Epoch: 004/008 | Batch 112/266 | Loss: 0.3556\n",
      "Epoch: 004/008 | Batch 113/266 | Loss: 0.2936\n",
      "Epoch: 004/008 | Batch 114/266 | Loss: 0.1943\n",
      "Epoch: 004/008 | Batch 115/266 | Loss: 0.2367\n",
      "Epoch: 004/008 | Batch 116/266 | Loss: 0.2971\n",
      "Epoch: 004/008 | Batch 117/266 | Loss: 0.3039\n",
      "Epoch: 004/008 | Batch 118/266 | Loss: 0.3494\n",
      "Epoch: 004/008 | Batch 119/266 | Loss: 0.2371\n",
      "Epoch: 004/008 | Batch 120/266 | Loss: 0.2537\n",
      "Epoch: 004/008 | Batch 121/266 | Loss: 0.2470\n",
      "Epoch: 004/008 | Batch 122/266 | Loss: 0.2903\n",
      "Epoch: 004/008 | Batch 123/266 | Loss: 0.2796\n",
      "Epoch: 004/008 | Batch 124/266 | Loss: 0.3647\n",
      "Epoch: 004/008 | Batch 125/266 | Loss: 0.2320\n",
      "Epoch: 004/008 | Batch 126/266 | Loss: 0.3338\n",
      "Epoch: 004/008 | Batch 127/266 | Loss: 0.2818\n",
      "Epoch: 004/008 | Batch 128/266 | Loss: 0.3293\n",
      "Epoch: 004/008 | Batch 129/266 | Loss: 0.2885\n",
      "Epoch: 004/008 | Batch 130/266 | Loss: 0.2603\n",
      "Epoch: 004/008 | Batch 131/266 | Loss: 0.3915\n",
      "Epoch: 004/008 | Batch 132/266 | Loss: 0.3220\n",
      "Epoch: 004/008 | Batch 133/266 | Loss: 0.2891\n",
      "Epoch: 004/008 | Batch 134/266 | Loss: 0.2688\n",
      "Epoch: 004/008 | Batch 135/266 | Loss: 0.3377\n",
      "Epoch: 004/008 | Batch 136/266 | Loss: 0.3255\n",
      "Epoch: 004/008 | Batch 137/266 | Loss: 0.3785\n",
      "Epoch: 004/008 | Batch 138/266 | Loss: 0.2817\n",
      "Epoch: 004/008 | Batch 139/266 | Loss: 0.3174\n",
      "Epoch: 004/008 | Batch 140/266 | Loss: 0.2354\n",
      "Epoch: 004/008 | Batch 141/266 | Loss: 0.2489\n",
      "Epoch: 004/008 | Batch 142/266 | Loss: 0.2845\n",
      "Epoch: 004/008 | Batch 143/266 | Loss: 0.3130\n",
      "Epoch: 004/008 | Batch 144/266 | Loss: 0.3336\n",
      "Epoch: 004/008 | Batch 145/266 | Loss: 0.2743\n",
      "Epoch: 004/008 | Batch 146/266 | Loss: 0.3049\n",
      "Epoch: 004/008 | Batch 147/266 | Loss: 0.4164\n",
      "Epoch: 004/008 | Batch 148/266 | Loss: 0.2839\n",
      "Epoch: 004/008 | Batch 149/266 | Loss: 0.2350\n",
      "Epoch: 004/008 | Batch 150/266 | Loss: 0.4125\n",
      "Epoch: 004/008 | Batch 151/266 | Loss: 0.2508\n",
      "Epoch: 004/008 | Batch 152/266 | Loss: 0.2764\n",
      "Epoch: 004/008 | Batch 153/266 | Loss: 0.2417\n",
      "Epoch: 004/008 | Batch 154/266 | Loss: 0.2184\n",
      "Epoch: 004/008 | Batch 155/266 | Loss: 0.2630\n",
      "Epoch: 004/008 | Batch 156/266 | Loss: 0.2423\n",
      "Epoch: 004/008 | Batch 157/266 | Loss: 0.3756\n",
      "Epoch: 004/008 | Batch 158/266 | Loss: 0.3290\n",
      "Epoch: 004/008 | Batch 159/266 | Loss: 0.2434\n",
      "Epoch: 004/008 | Batch 160/266 | Loss: 0.3737\n",
      "Epoch: 004/008 | Batch 161/266 | Loss: 0.3596\n",
      "Epoch: 004/008 | Batch 162/266 | Loss: 0.3722\n",
      "Epoch: 004/008 | Batch 163/266 | Loss: 0.2408\n",
      "Epoch: 004/008 | Batch 164/266 | Loss: 0.2246\n",
      "Epoch: 004/008 | Batch 165/266 | Loss: 0.2738\n",
      "Epoch: 004/008 | Batch 166/266 | Loss: 0.1862\n",
      "Epoch: 004/008 | Batch 167/266 | Loss: 0.2418\n",
      "Epoch: 004/008 | Batch 168/266 | Loss: 0.2446\n",
      "Epoch: 004/008 | Batch 169/266 | Loss: 0.2430\n",
      "Epoch: 004/008 | Batch 170/266 | Loss: 0.2868\n",
      "Epoch: 004/008 | Batch 171/266 | Loss: 0.2909\n",
      "Epoch: 004/008 | Batch 172/266 | Loss: 0.2525\n",
      "Epoch: 004/008 | Batch 173/266 | Loss: 0.3093\n",
      "Epoch: 004/008 | Batch 174/266 | Loss: 0.3344\n",
      "Epoch: 004/008 | Batch 175/266 | Loss: 0.2712\n",
      "Epoch: 004/008 | Batch 176/266 | Loss: 0.2408\n",
      "Epoch: 004/008 | Batch 177/266 | Loss: 0.2911\n",
      "Epoch: 004/008 | Batch 178/266 | Loss: 0.2464\n",
      "Epoch: 004/008 | Batch 179/266 | Loss: 0.2132\n",
      "Epoch: 004/008 | Batch 180/266 | Loss: 0.4048\n",
      "Epoch: 004/008 | Batch 181/266 | Loss: 0.2320\n",
      "Epoch: 004/008 | Batch 182/266 | Loss: 0.3226\n",
      "Epoch: 004/008 | Batch 183/266 | Loss: 0.2884\n",
      "Epoch: 004/008 | Batch 184/266 | Loss: 0.2252\n",
      "Epoch: 004/008 | Batch 185/266 | Loss: 0.2807\n",
      "Epoch: 004/008 | Batch 186/266 | Loss: 0.2933\n",
      "Epoch: 004/008 | Batch 187/266 | Loss: 0.3108\n",
      "Epoch: 004/008 | Batch 188/266 | Loss: 0.3030\n",
      "Epoch: 004/008 | Batch 189/266 | Loss: 0.3130\n",
      "Epoch: 004/008 | Batch 190/266 | Loss: 0.2727\n",
      "Epoch: 004/008 | Batch 191/266 | Loss: 0.2780\n",
      "Epoch: 004/008 | Batch 192/266 | Loss: 0.2753\n",
      "Epoch: 004/008 | Batch 193/266 | Loss: 0.2689\n",
      "Epoch: 004/008 | Batch 194/266 | Loss: 0.2631\n",
      "Epoch: 004/008 | Batch 195/266 | Loss: 0.2616\n",
      "Epoch: 004/008 | Batch 196/266 | Loss: 0.2272\n",
      "Epoch: 004/008 | Batch 197/266 | Loss: 0.3447\n",
      "Epoch: 004/008 | Batch 198/266 | Loss: 0.2466\n",
      "Epoch: 004/008 | Batch 199/266 | Loss: 0.2440\n",
      "Epoch: 004/008 | Batch 200/266 | Loss: 0.2140\n",
      "Epoch: 004/008 | Batch 201/266 | Loss: 0.2555\n",
      "Epoch: 004/008 | Batch 202/266 | Loss: 0.3097\n",
      "Epoch: 004/008 | Batch 203/266 | Loss: 0.2380\n",
      "Epoch: 004/008 | Batch 204/266 | Loss: 0.2666\n",
      "Epoch: 004/008 | Batch 205/266 | Loss: 0.2374\n",
      "Epoch: 004/008 | Batch 206/266 | Loss: 0.2888\n",
      "Epoch: 004/008 | Batch 207/266 | Loss: 0.3276\n",
      "Epoch: 004/008 | Batch 208/266 | Loss: 0.3135\n",
      "Epoch: 004/008 | Batch 209/266 | Loss: 0.2536\n",
      "Epoch: 004/008 | Batch 210/266 | Loss: 0.3058\n",
      "Epoch: 004/008 | Batch 211/266 | Loss: 0.2137\n",
      "Epoch: 004/008 | Batch 212/266 | Loss: 0.3263\n",
      "Epoch: 004/008 | Batch 213/266 | Loss: 0.2902\n",
      "Epoch: 004/008 | Batch 214/266 | Loss: 0.2667\n",
      "Epoch: 004/008 | Batch 215/266 | Loss: 0.3342\n",
      "Epoch: 004/008 | Batch 216/266 | Loss: 0.2321\n",
      "Epoch: 004/008 | Batch 217/266 | Loss: 0.2332\n",
      "Epoch: 004/008 | Batch 218/266 | Loss: 0.2743\n",
      "Epoch: 004/008 | Batch 219/266 | Loss: 0.2734\n",
      "Epoch: 004/008 | Batch 220/266 | Loss: 0.2254\n",
      "Epoch: 004/008 | Batch 221/266 | Loss: 0.2050\n",
      "Epoch: 004/008 | Batch 222/266 | Loss: 0.2883\n",
      "Epoch: 004/008 | Batch 223/266 | Loss: 0.2332\n",
      "Epoch: 004/008 | Batch 224/266 | Loss: 0.3141\n",
      "Epoch: 004/008 | Batch 225/266 | Loss: 0.3229\n",
      "Epoch: 004/008 | Batch 226/266 | Loss: 0.2810\n",
      "Epoch: 004/008 | Batch 227/266 | Loss: 0.2839\n",
      "Epoch: 004/008 | Batch 228/266 | Loss: 0.2677\n",
      "Epoch: 004/008 | Batch 229/266 | Loss: 0.2276\n",
      "Epoch: 004/008 | Batch 230/266 | Loss: 0.3639\n",
      "Epoch: 004/008 | Batch 231/266 | Loss: 0.2404\n",
      "Epoch: 004/008 | Batch 232/266 | Loss: 0.2235\n",
      "Epoch: 004/008 | Batch 233/266 | Loss: 0.3570\n",
      "Epoch: 004/008 | Batch 234/266 | Loss: 0.2890\n",
      "Epoch: 004/008 | Batch 235/266 | Loss: 0.2691\n",
      "Epoch: 004/008 | Batch 236/266 | Loss: 0.2719\n",
      "Epoch: 004/008 | Batch 237/266 | Loss: 0.3248\n",
      "Epoch: 004/008 | Batch 238/266 | Loss: 0.3891\n",
      "Epoch: 004/008 | Batch 239/266 | Loss: 0.2522\n",
      "Epoch: 004/008 | Batch 240/266 | Loss: 0.2822\n",
      "Epoch: 004/008 | Batch 241/266 | Loss: 0.2737\n",
      "Epoch: 004/008 | Batch 242/266 | Loss: 0.2550\n",
      "Epoch: 004/008 | Batch 243/266 | Loss: 0.3578\n",
      "Epoch: 004/008 | Batch 244/266 | Loss: 0.2637\n",
      "Epoch: 004/008 | Batch 245/266 | Loss: 0.3585\n",
      "Epoch: 004/008 | Batch 246/266 | Loss: 0.2934\n",
      "Epoch: 004/008 | Batch 247/266 | Loss: 0.3977\n",
      "Epoch: 004/008 | Batch 248/266 | Loss: 0.3100\n",
      "Epoch: 004/008 | Batch 249/266 | Loss: 0.3653\n",
      "Epoch: 004/008 | Batch 250/266 | Loss: 0.2119\n",
      "Epoch: 004/008 | Batch 251/266 | Loss: 0.3396\n",
      "Epoch: 004/008 | Batch 252/266 | Loss: 0.2422\n",
      "Epoch: 004/008 | Batch 253/266 | Loss: 0.2343\n",
      "Epoch: 004/008 | Batch 254/266 | Loss: 0.3481\n",
      "Epoch: 004/008 | Batch 255/266 | Loss: 0.2324\n",
      "Epoch: 004/008 | Batch 256/266 | Loss: 0.2871\n",
      "Epoch: 004/008 | Batch 257/266 | Loss: 0.2936\n",
      "Epoch: 004/008 | Batch 258/266 | Loss: 0.2157\n",
      "Epoch: 004/008 | Batch 259/266 | Loss: 0.1784\n",
      "Epoch: 004/008 | Batch 260/266 | Loss: 0.2376\n",
      "Epoch: 004/008 | Batch 261/266 | Loss: 0.3907\n",
      "Epoch: 004/008 | Batch 262/266 | Loss: 0.2458\n",
      "Epoch: 004/008 | Batch 263/266 | Loss: 0.3214\n",
      "Epoch: 004/008 | Batch 264/266 | Loss: 0.2192\n",
      "Epoch: 004/008 | Batch 265/266 | Loss: 0.2874\n",
      "training accuracy: 92.79%\n",
      "valid accuracy: 87.30%\n",
      "Time elapsed: 14.30 min\n",
      "Epoch: 005/008 | Batch 000/266 | Loss: 0.1749\n",
      "Epoch: 005/008 | Batch 001/266 | Loss: 0.2776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005/008 | Batch 002/266 | Loss: 0.2021\n",
      "Epoch: 005/008 | Batch 003/266 | Loss: 0.2268\n",
      "Epoch: 005/008 | Batch 004/266 | Loss: 0.2543\n",
      "Epoch: 005/008 | Batch 005/266 | Loss: 0.3010\n",
      "Epoch: 005/008 | Batch 006/266 | Loss: 0.1730\n",
      "Epoch: 005/008 | Batch 007/266 | Loss: 0.1586\n",
      "Epoch: 005/008 | Batch 008/266 | Loss: 0.2176\n",
      "Epoch: 005/008 | Batch 009/266 | Loss: 0.1322\n",
      "Epoch: 005/008 | Batch 010/266 | Loss: 0.1924\n",
      "Epoch: 005/008 | Batch 011/266 | Loss: 0.2079\n",
      "Epoch: 005/008 | Batch 012/266 | Loss: 0.1457\n",
      "Epoch: 005/008 | Batch 013/266 | Loss: 0.1708\n",
      "Epoch: 005/008 | Batch 014/266 | Loss: 0.1876\n",
      "Epoch: 005/008 | Batch 015/266 | Loss: 0.2877\n",
      "Epoch: 005/008 | Batch 016/266 | Loss: 0.1435\n",
      "Epoch: 005/008 | Batch 017/266 | Loss: 0.2027\n",
      "Epoch: 005/008 | Batch 018/266 | Loss: 0.1547\n",
      "Epoch: 005/008 | Batch 019/266 | Loss: 0.2384\n",
      "Epoch: 005/008 | Batch 020/266 | Loss: 0.2256\n",
      "Epoch: 005/008 | Batch 021/266 | Loss: 0.1583\n",
      "Epoch: 005/008 | Batch 022/266 | Loss: 0.1323\n",
      "Epoch: 005/008 | Batch 023/266 | Loss: 0.2630\n",
      "Epoch: 005/008 | Batch 024/266 | Loss: 0.1124\n",
      "Epoch: 005/008 | Batch 025/266 | Loss: 0.2481\n",
      "Epoch: 005/008 | Batch 026/266 | Loss: 0.1689\n",
      "Epoch: 005/008 | Batch 027/266 | Loss: 0.2024\n",
      "Epoch: 005/008 | Batch 028/266 | Loss: 0.2403\n",
      "Epoch: 005/008 | Batch 029/266 | Loss: 0.2761\n",
      "Epoch: 005/008 | Batch 030/266 | Loss: 0.2470\n",
      "Epoch: 005/008 | Batch 031/266 | Loss: 0.2958\n",
      "Epoch: 005/008 | Batch 032/266 | Loss: 0.2518\n",
      "Epoch: 005/008 | Batch 033/266 | Loss: 0.1182\n",
      "Epoch: 005/008 | Batch 034/266 | Loss: 0.1035\n",
      "Epoch: 005/008 | Batch 035/266 | Loss: 0.1532\n",
      "Epoch: 005/008 | Batch 036/266 | Loss: 0.1677\n",
      "Epoch: 005/008 | Batch 037/266 | Loss: 0.1724\n",
      "Epoch: 005/008 | Batch 038/266 | Loss: 0.1939\n",
      "Epoch: 005/008 | Batch 039/266 | Loss: 0.1872\n",
      "Epoch: 005/008 | Batch 040/266 | Loss: 0.2708\n",
      "Epoch: 005/008 | Batch 041/266 | Loss: 0.2241\n",
      "Epoch: 005/008 | Batch 042/266 | Loss: 0.2190\n",
      "Epoch: 005/008 | Batch 043/266 | Loss: 0.3176\n",
      "Epoch: 005/008 | Batch 044/266 | Loss: 0.2022\n",
      "Epoch: 005/008 | Batch 045/266 | Loss: 0.2190\n",
      "Epoch: 005/008 | Batch 046/266 | Loss: 0.1522\n",
      "Epoch: 005/008 | Batch 047/266 | Loss: 0.1580\n",
      "Epoch: 005/008 | Batch 048/266 | Loss: 0.2196\n",
      "Epoch: 005/008 | Batch 049/266 | Loss: 0.2495\n",
      "Epoch: 005/008 | Batch 050/266 | Loss: 0.2167\n",
      "Epoch: 005/008 | Batch 051/266 | Loss: 0.1879\n",
      "Epoch: 005/008 | Batch 052/266 | Loss: 0.1974\n",
      "Epoch: 005/008 | Batch 053/266 | Loss: 0.2186\n",
      "Epoch: 005/008 | Batch 054/266 | Loss: 0.1422\n",
      "Epoch: 005/008 | Batch 055/266 | Loss: 0.2943\n",
      "Epoch: 005/008 | Batch 056/266 | Loss: 0.1411\n",
      "Epoch: 005/008 | Batch 057/266 | Loss: 0.1714\n",
      "Epoch: 005/008 | Batch 058/266 | Loss: 0.2709\n",
      "Epoch: 005/008 | Batch 059/266 | Loss: 0.1779\n",
      "Epoch: 005/008 | Batch 060/266 | Loss: 0.3255\n",
      "Epoch: 005/008 | Batch 061/266 | Loss: 0.1876\n",
      "Epoch: 005/008 | Batch 062/266 | Loss: 0.2460\n",
      "Epoch: 005/008 | Batch 063/266 | Loss: 0.2379\n",
      "Epoch: 005/008 | Batch 064/266 | Loss: 0.2389\n",
      "Epoch: 005/008 | Batch 065/266 | Loss: 0.1499\n",
      "Epoch: 005/008 | Batch 066/266 | Loss: 0.1877\n",
      "Epoch: 005/008 | Batch 067/266 | Loss: 0.1474\n",
      "Epoch: 005/008 | Batch 068/266 | Loss: 0.1673\n",
      "Epoch: 005/008 | Batch 069/266 | Loss: 0.1565\n",
      "Epoch: 005/008 | Batch 070/266 | Loss: 0.2976\n",
      "Epoch: 005/008 | Batch 071/266 | Loss: 0.1687\n",
      "Epoch: 005/008 | Batch 072/266 | Loss: 0.2822\n",
      "Epoch: 005/008 | Batch 073/266 | Loss: 0.1561\n",
      "Epoch: 005/008 | Batch 074/266 | Loss: 0.2644\n",
      "Epoch: 005/008 | Batch 075/266 | Loss: 0.1905\n",
      "Epoch: 005/008 | Batch 076/266 | Loss: 0.2639\n",
      "Epoch: 005/008 | Batch 077/266 | Loss: 0.1908\n",
      "Epoch: 005/008 | Batch 078/266 | Loss: 0.2291\n",
      "Epoch: 005/008 | Batch 079/266 | Loss: 0.1767\n",
      "Epoch: 005/008 | Batch 080/266 | Loss: 0.1953\n",
      "Epoch: 005/008 | Batch 081/266 | Loss: 0.2866\n",
      "Epoch: 005/008 | Batch 082/266 | Loss: 0.1958\n",
      "Epoch: 005/008 | Batch 083/266 | Loss: 0.1409\n",
      "Epoch: 005/008 | Batch 084/266 | Loss: 0.1795\n",
      "Epoch: 005/008 | Batch 085/266 | Loss: 0.1968\n",
      "Epoch: 005/008 | Batch 086/266 | Loss: 0.2335\n",
      "Epoch: 005/008 | Batch 087/266 | Loss: 0.2329\n",
      "Epoch: 005/008 | Batch 088/266 | Loss: 0.2097\n",
      "Epoch: 005/008 | Batch 089/266 | Loss: 0.1774\n",
      "Epoch: 005/008 | Batch 090/266 | Loss: 0.2098\n",
      "Epoch: 005/008 | Batch 091/266 | Loss: 0.1421\n",
      "Epoch: 005/008 | Batch 092/266 | Loss: 0.2268\n",
      "Epoch: 005/008 | Batch 093/266 | Loss: 0.1899\n",
      "Epoch: 005/008 | Batch 094/266 | Loss: 0.2166\n",
      "Epoch: 005/008 | Batch 095/266 | Loss: 0.2005\n",
      "Epoch: 005/008 | Batch 096/266 | Loss: 0.1495\n",
      "Epoch: 005/008 | Batch 097/266 | Loss: 0.2361\n",
      "Epoch: 005/008 | Batch 098/266 | Loss: 0.1906\n",
      "Epoch: 005/008 | Batch 099/266 | Loss: 0.2162\n",
      "Epoch: 005/008 | Batch 100/266 | Loss: 0.2187\n",
      "Epoch: 005/008 | Batch 101/266 | Loss: 0.1606\n",
      "Epoch: 005/008 | Batch 102/266 | Loss: 0.2300\n",
      "Epoch: 005/008 | Batch 103/266 | Loss: 0.2166\n",
      "Epoch: 005/008 | Batch 104/266 | Loss: 0.1258\n",
      "Epoch: 005/008 | Batch 105/266 | Loss: 0.1362\n",
      "Epoch: 005/008 | Batch 106/266 | Loss: 0.2120\n",
      "Epoch: 005/008 | Batch 107/266 | Loss: 0.2101\n",
      "Epoch: 005/008 | Batch 108/266 | Loss: 0.2051\n",
      "Epoch: 005/008 | Batch 109/266 | Loss: 0.1941\n",
      "Epoch: 005/008 | Batch 110/266 | Loss: 0.2106\n",
      "Epoch: 005/008 | Batch 111/266 | Loss: 0.2507\n",
      "Epoch: 005/008 | Batch 112/266 | Loss: 0.2576\n",
      "Epoch: 005/008 | Batch 113/266 | Loss: 0.1778\n",
      "Epoch: 005/008 | Batch 114/266 | Loss: 0.1856\n",
      "Epoch: 005/008 | Batch 115/266 | Loss: 0.2391\n",
      "Epoch: 005/008 | Batch 116/266 | Loss: 0.2025\n",
      "Epoch: 005/008 | Batch 117/266 | Loss: 0.2482\n",
      "Epoch: 005/008 | Batch 118/266 | Loss: 0.1526\n",
      "Epoch: 005/008 | Batch 119/266 | Loss: 0.1770\n",
      "Epoch: 005/008 | Batch 120/266 | Loss: 0.1963\n",
      "Epoch: 005/008 | Batch 121/266 | Loss: 0.2192\n",
      "Epoch: 005/008 | Batch 122/266 | Loss: 0.1093\n",
      "Epoch: 005/008 | Batch 123/266 | Loss: 0.0851\n",
      "Epoch: 005/008 | Batch 124/266 | Loss: 0.2316\n",
      "Epoch: 005/008 | Batch 125/266 | Loss: 0.2444\n",
      "Epoch: 005/008 | Batch 126/266 | Loss: 0.2768\n",
      "Epoch: 005/008 | Batch 127/266 | Loss: 0.2383\n",
      "Epoch: 005/008 | Batch 128/266 | Loss: 0.2678\n",
      "Epoch: 005/008 | Batch 129/266 | Loss: 0.1569\n",
      "Epoch: 005/008 | Batch 130/266 | Loss: 0.2069\n",
      "Epoch: 005/008 | Batch 131/266 | Loss: 0.1400\n",
      "Epoch: 005/008 | Batch 132/266 | Loss: 0.2040\n",
      "Epoch: 005/008 | Batch 133/266 | Loss: 0.2554\n",
      "Epoch: 005/008 | Batch 134/266 | Loss: 0.3800\n",
      "Epoch: 005/008 | Batch 135/266 | Loss: 0.1722\n",
      "Epoch: 005/008 | Batch 136/266 | Loss: 0.2622\n",
      "Epoch: 005/008 | Batch 137/266 | Loss: 0.3220\n",
      "Epoch: 005/008 | Batch 138/266 | Loss: 0.2688\n",
      "Epoch: 005/008 | Batch 139/266 | Loss: 0.1864\n",
      "Epoch: 005/008 | Batch 140/266 | Loss: 0.2782\n",
      "Epoch: 005/008 | Batch 141/266 | Loss: 0.1511\n",
      "Epoch: 005/008 | Batch 142/266 | Loss: 0.2497\n",
      "Epoch: 005/008 | Batch 143/266 | Loss: 0.1874\n",
      "Epoch: 005/008 | Batch 144/266 | Loss: 0.2123\n",
      "Epoch: 005/008 | Batch 145/266 | Loss: 0.1695\n",
      "Epoch: 005/008 | Batch 146/266 | Loss: 0.1807\n",
      "Epoch: 005/008 | Batch 147/266 | Loss: 0.2929\n",
      "Epoch: 005/008 | Batch 148/266 | Loss: 0.1147\n",
      "Epoch: 005/008 | Batch 149/266 | Loss: 0.2183\n",
      "Epoch: 005/008 | Batch 150/266 | Loss: 0.2340\n",
      "Epoch: 005/008 | Batch 151/266 | Loss: 0.1877\n",
      "Epoch: 005/008 | Batch 152/266 | Loss: 0.1818\n",
      "Epoch: 005/008 | Batch 153/266 | Loss: 0.1492\n",
      "Epoch: 005/008 | Batch 154/266 | Loss: 0.1356\n",
      "Epoch: 005/008 | Batch 155/266 | Loss: 0.1638\n",
      "Epoch: 005/008 | Batch 156/266 | Loss: 0.2133\n",
      "Epoch: 005/008 | Batch 157/266 | Loss: 0.2133\n",
      "Epoch: 005/008 | Batch 158/266 | Loss: 0.2306\n",
      "Epoch: 005/008 | Batch 159/266 | Loss: 0.1852\n",
      "Epoch: 005/008 | Batch 160/266 | Loss: 0.1280\n",
      "Epoch: 005/008 | Batch 161/266 | Loss: 0.1670\n",
      "Epoch: 005/008 | Batch 162/266 | Loss: 0.2343\n",
      "Epoch: 005/008 | Batch 163/266 | Loss: 0.3074\n",
      "Epoch: 005/008 | Batch 164/266 | Loss: 0.2782\n",
      "Epoch: 005/008 | Batch 165/266 | Loss: 0.2375\n",
      "Epoch: 005/008 | Batch 166/266 | Loss: 0.1766\n",
      "Epoch: 005/008 | Batch 167/266 | Loss: 0.3075\n",
      "Epoch: 005/008 | Batch 168/266 | Loss: 0.1938\n",
      "Epoch: 005/008 | Batch 169/266 | Loss: 0.2456\n",
      "Epoch: 005/008 | Batch 170/266 | Loss: 0.1384\n",
      "Epoch: 005/008 | Batch 171/266 | Loss: 0.1904\n",
      "Epoch: 005/008 | Batch 172/266 | Loss: 0.1191\n",
      "Epoch: 005/008 | Batch 173/266 | Loss: 0.2332\n",
      "Epoch: 005/008 | Batch 174/266 | Loss: 0.1822\n",
      "Epoch: 005/008 | Batch 175/266 | Loss: 0.1505\n",
      "Epoch: 005/008 | Batch 176/266 | Loss: 0.2885\n",
      "Epoch: 005/008 | Batch 177/266 | Loss: 0.1739\n",
      "Epoch: 005/008 | Batch 178/266 | Loss: 0.2026\n",
      "Epoch: 005/008 | Batch 179/266 | Loss: 0.1767\n",
      "Epoch: 005/008 | Batch 180/266 | Loss: 0.3091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005/008 | Batch 181/266 | Loss: 0.2872\n",
      "Epoch: 005/008 | Batch 182/266 | Loss: 0.1772\n",
      "Epoch: 005/008 | Batch 183/266 | Loss: 0.1159\n",
      "Epoch: 005/008 | Batch 184/266 | Loss: 0.1803\n",
      "Epoch: 005/008 | Batch 185/266 | Loss: 0.2297\n",
      "Epoch: 005/008 | Batch 186/266 | Loss: 0.2495\n",
      "Epoch: 005/008 | Batch 187/266 | Loss: 0.1687\n",
      "Epoch: 005/008 | Batch 188/266 | Loss: 0.1942\n",
      "Epoch: 005/008 | Batch 189/266 | Loss: 0.1569\n",
      "Epoch: 005/008 | Batch 190/266 | Loss: 0.2514\n",
      "Epoch: 005/008 | Batch 191/266 | Loss: 0.2546\n",
      "Epoch: 005/008 | Batch 192/266 | Loss: 0.2227\n",
      "Epoch: 005/008 | Batch 193/266 | Loss: 0.2774\n",
      "Epoch: 005/008 | Batch 194/266 | Loss: 0.2359\n",
      "Epoch: 005/008 | Batch 195/266 | Loss: 0.2058\n",
      "Epoch: 005/008 | Batch 196/266 | Loss: 0.2813\n",
      "Epoch: 005/008 | Batch 197/266 | Loss: 0.2633\n",
      "Epoch: 005/008 | Batch 198/266 | Loss: 0.2230\n",
      "Epoch: 005/008 | Batch 199/266 | Loss: 0.2015\n",
      "Epoch: 005/008 | Batch 200/266 | Loss: 0.1906\n",
      "Epoch: 005/008 | Batch 201/266 | Loss: 0.1732\n",
      "Epoch: 005/008 | Batch 202/266 | Loss: 0.1742\n",
      "Epoch: 005/008 | Batch 203/266 | Loss: 0.1832\n",
      "Epoch: 005/008 | Batch 204/266 | Loss: 0.2398\n",
      "Epoch: 005/008 | Batch 205/266 | Loss: 0.2340\n",
      "Epoch: 005/008 | Batch 206/266 | Loss: 0.2010\n",
      "Epoch: 005/008 | Batch 207/266 | Loss: 0.1405\n",
      "Epoch: 005/008 | Batch 208/266 | Loss: 0.2283\n",
      "Epoch: 005/008 | Batch 209/266 | Loss: 0.1768\n",
      "Epoch: 005/008 | Batch 210/266 | Loss: 0.1987\n",
      "Epoch: 005/008 | Batch 211/266 | Loss: 0.1070\n",
      "Epoch: 005/008 | Batch 212/266 | Loss: 0.1520\n",
      "Epoch: 005/008 | Batch 213/266 | Loss: 0.2144\n",
      "Epoch: 005/008 | Batch 214/266 | Loss: 0.2054\n",
      "Epoch: 005/008 | Batch 215/266 | Loss: 0.1685\n",
      "Epoch: 005/008 | Batch 216/266 | Loss: 0.1757\n",
      "Epoch: 005/008 | Batch 217/266 | Loss: 0.2830\n",
      "Epoch: 005/008 | Batch 218/266 | Loss: 0.2236\n",
      "Epoch: 005/008 | Batch 219/266 | Loss: 0.2907\n",
      "Epoch: 005/008 | Batch 220/266 | Loss: 0.2221\n",
      "Epoch: 005/008 | Batch 221/266 | Loss: 0.2734\n",
      "Epoch: 005/008 | Batch 222/266 | Loss: 0.1947\n",
      "Epoch: 005/008 | Batch 223/266 | Loss: 0.2004\n",
      "Epoch: 005/008 | Batch 224/266 | Loss: 0.2824\n",
      "Epoch: 005/008 | Batch 225/266 | Loss: 0.1624\n",
      "Epoch: 005/008 | Batch 226/266 | Loss: 0.1287\n",
      "Epoch: 005/008 | Batch 227/266 | Loss: 0.2192\n",
      "Epoch: 005/008 | Batch 228/266 | Loss: 0.2432\n",
      "Epoch: 005/008 | Batch 229/266 | Loss: 0.2599\n",
      "Epoch: 005/008 | Batch 230/266 | Loss: 0.3292\n",
      "Epoch: 005/008 | Batch 231/266 | Loss: 0.2363\n",
      "Epoch: 005/008 | Batch 232/266 | Loss: 0.2775\n",
      "Epoch: 005/008 | Batch 233/266 | Loss: 0.2533\n",
      "Epoch: 005/008 | Batch 234/266 | Loss: 0.1843\n",
      "Epoch: 005/008 | Batch 235/266 | Loss: 0.1335\n",
      "Epoch: 005/008 | Batch 236/266 | Loss: 0.1980\n",
      "Epoch: 005/008 | Batch 237/266 | Loss: 0.2023\n",
      "Epoch: 005/008 | Batch 238/266 | Loss: 0.2895\n",
      "Epoch: 005/008 | Batch 239/266 | Loss: 0.1893\n",
      "Epoch: 005/008 | Batch 240/266 | Loss: 0.2639\n",
      "Epoch: 005/008 | Batch 241/266 | Loss: 0.1853\n",
      "Epoch: 005/008 | Batch 242/266 | Loss: 0.2378\n",
      "Epoch: 005/008 | Batch 243/266 | Loss: 0.1407\n",
      "Epoch: 005/008 | Batch 244/266 | Loss: 0.1777\n",
      "Epoch: 005/008 | Batch 245/266 | Loss: 0.1918\n",
      "Epoch: 005/008 | Batch 246/266 | Loss: 0.2851\n",
      "Epoch: 005/008 | Batch 247/266 | Loss: 0.2308\n",
      "Epoch: 005/008 | Batch 248/266 | Loss: 0.1526\n",
      "Epoch: 005/008 | Batch 249/266 | Loss: 0.1933\n",
      "Epoch: 005/008 | Batch 250/266 | Loss: 0.1847\n",
      "Epoch: 005/008 | Batch 251/266 | Loss: 0.2628\n",
      "Epoch: 005/008 | Batch 252/266 | Loss: 0.1956\n",
      "Epoch: 005/008 | Batch 253/266 | Loss: 0.2285\n",
      "Epoch: 005/008 | Batch 254/266 | Loss: 0.1091\n",
      "Epoch: 005/008 | Batch 255/266 | Loss: 0.3083\n",
      "Epoch: 005/008 | Batch 256/266 | Loss: 0.1280\n",
      "Epoch: 005/008 | Batch 257/266 | Loss: 0.1362\n",
      "Epoch: 005/008 | Batch 258/266 | Loss: 0.1832\n",
      "Epoch: 005/008 | Batch 259/266 | Loss: 0.1962\n",
      "Epoch: 005/008 | Batch 260/266 | Loss: 0.2277\n",
      "Epoch: 005/008 | Batch 261/266 | Loss: 0.1766\n",
      "Epoch: 005/008 | Batch 262/266 | Loss: 0.2706\n",
      "Epoch: 005/008 | Batch 263/266 | Loss: 0.1017\n",
      "Epoch: 005/008 | Batch 264/266 | Loss: 0.1940\n",
      "Epoch: 005/008 | Batch 265/266 | Loss: 0.2191\n",
      "training accuracy: 95.70%\n",
      "valid accuracy: 87.97%\n",
      "Time elapsed: 16.98 min\n",
      "Epoch: 006/008 | Batch 000/266 | Loss: 0.1616\n",
      "Epoch: 006/008 | Batch 001/266 | Loss: 0.1620\n",
      "Epoch: 006/008 | Batch 002/266 | Loss: 0.0591\n",
      "Epoch: 006/008 | Batch 003/266 | Loss: 0.1187\n",
      "Epoch: 006/008 | Batch 004/266 | Loss: 0.1677\n",
      "Epoch: 006/008 | Batch 005/266 | Loss: 0.1298\n",
      "Epoch: 006/008 | Batch 006/266 | Loss: 0.1857\n",
      "Epoch: 006/008 | Batch 007/266 | Loss: 0.2075\n",
      "Epoch: 006/008 | Batch 008/266 | Loss: 0.2023\n",
      "Epoch: 006/008 | Batch 009/266 | Loss: 0.1204\n",
      "Epoch: 006/008 | Batch 010/266 | Loss: 0.1530\n",
      "Epoch: 006/008 | Batch 011/266 | Loss: 0.0794\n",
      "Epoch: 006/008 | Batch 012/266 | Loss: 0.1458\n",
      "Epoch: 006/008 | Batch 013/266 | Loss: 0.1531\n",
      "Epoch: 006/008 | Batch 014/266 | Loss: 0.1547\n",
      "Epoch: 006/008 | Batch 015/266 | Loss: 0.1524\n",
      "Epoch: 006/008 | Batch 016/266 | Loss: 0.1226\n",
      "Epoch: 006/008 | Batch 017/266 | Loss: 0.1592\n",
      "Epoch: 006/008 | Batch 018/266 | Loss: 0.1738\n",
      "Epoch: 006/008 | Batch 019/266 | Loss: 0.1647\n",
      "Epoch: 006/008 | Batch 020/266 | Loss: 0.1056\n",
      "Epoch: 006/008 | Batch 021/266 | Loss: 0.1188\n",
      "Epoch: 006/008 | Batch 022/266 | Loss: 0.1505\n",
      "Epoch: 006/008 | Batch 023/266 | Loss: 0.1262\n",
      "Epoch: 006/008 | Batch 024/266 | Loss: 0.1629\n",
      "Epoch: 006/008 | Batch 025/266 | Loss: 0.1446\n",
      "Epoch: 006/008 | Batch 026/266 | Loss: 0.0554\n",
      "Epoch: 006/008 | Batch 027/266 | Loss: 0.0988\n",
      "Epoch: 006/008 | Batch 028/266 | Loss: 0.1936\n",
      "Epoch: 006/008 | Batch 029/266 | Loss: 0.0934\n",
      "Epoch: 006/008 | Batch 030/266 | Loss: 0.0810\n",
      "Epoch: 006/008 | Batch 031/266 | Loss: 0.1159\n",
      "Epoch: 006/008 | Batch 032/266 | Loss: 0.1397\n",
      "Epoch: 006/008 | Batch 033/266 | Loss: 0.1672\n",
      "Epoch: 006/008 | Batch 034/266 | Loss: 0.1229\n",
      "Epoch: 006/008 | Batch 035/266 | Loss: 0.1679\n",
      "Epoch: 006/008 | Batch 036/266 | Loss: 0.1924\n",
      "Epoch: 006/008 | Batch 037/266 | Loss: 0.1767\n",
      "Epoch: 006/008 | Batch 038/266 | Loss: 0.1596\n",
      "Epoch: 006/008 | Batch 039/266 | Loss: 0.1134\n",
      "Epoch: 006/008 | Batch 040/266 | Loss: 0.1207\n",
      "Epoch: 006/008 | Batch 041/266 | Loss: 0.1286\n",
      "Epoch: 006/008 | Batch 042/266 | Loss: 0.1219\n",
      "Epoch: 006/008 | Batch 043/266 | Loss: 0.1481\n",
      "Epoch: 006/008 | Batch 044/266 | Loss: 0.1617\n",
      "Epoch: 006/008 | Batch 045/266 | Loss: 0.1306\n",
      "Epoch: 006/008 | Batch 046/266 | Loss: 0.1205\n",
      "Epoch: 006/008 | Batch 047/266 | Loss: 0.0943\n",
      "Epoch: 006/008 | Batch 048/266 | Loss: 0.1422\n",
      "Epoch: 006/008 | Batch 049/266 | Loss: 0.0909\n",
      "Epoch: 006/008 | Batch 050/266 | Loss: 0.1194\n",
      "Epoch: 006/008 | Batch 051/266 | Loss: 0.1815\n",
      "Epoch: 006/008 | Batch 052/266 | Loss: 0.0836\n",
      "Epoch: 006/008 | Batch 053/266 | Loss: 0.0997\n",
      "Epoch: 006/008 | Batch 054/266 | Loss: 0.1268\n",
      "Epoch: 006/008 | Batch 055/266 | Loss: 0.2220\n",
      "Epoch: 006/008 | Batch 056/266 | Loss: 0.0775\n",
      "Epoch: 006/008 | Batch 057/266 | Loss: 0.1621\n",
      "Epoch: 006/008 | Batch 058/266 | Loss: 0.0812\n",
      "Epoch: 006/008 | Batch 059/266 | Loss: 0.1307\n",
      "Epoch: 006/008 | Batch 060/266 | Loss: 0.0898\n",
      "Epoch: 006/008 | Batch 061/266 | Loss: 0.1587\n",
      "Epoch: 006/008 | Batch 062/266 | Loss: 0.1323\n",
      "Epoch: 006/008 | Batch 063/266 | Loss: 0.1475\n",
      "Epoch: 006/008 | Batch 064/266 | Loss: 0.1490\n",
      "Epoch: 006/008 | Batch 065/266 | Loss: 0.1170\n",
      "Epoch: 006/008 | Batch 066/266 | Loss: 0.1197\n",
      "Epoch: 006/008 | Batch 067/266 | Loss: 0.0905\n",
      "Epoch: 006/008 | Batch 068/266 | Loss: 0.2329\n",
      "Epoch: 006/008 | Batch 069/266 | Loss: 0.1422\n",
      "Epoch: 006/008 | Batch 070/266 | Loss: 0.1961\n",
      "Epoch: 006/008 | Batch 071/266 | Loss: 0.0978\n",
      "Epoch: 006/008 | Batch 072/266 | Loss: 0.1244\n",
      "Epoch: 006/008 | Batch 073/266 | Loss: 0.1493\n",
      "Epoch: 006/008 | Batch 074/266 | Loss: 0.1882\n",
      "Epoch: 006/008 | Batch 075/266 | Loss: 0.1137\n",
      "Epoch: 006/008 | Batch 076/266 | Loss: 0.1369\n",
      "Epoch: 006/008 | Batch 077/266 | Loss: 0.2152\n",
      "Epoch: 006/008 | Batch 078/266 | Loss: 0.1787\n",
      "Epoch: 006/008 | Batch 079/266 | Loss: 0.1449\n",
      "Epoch: 006/008 | Batch 080/266 | Loss: 0.1477\n",
      "Epoch: 006/008 | Batch 081/266 | Loss: 0.1570\n",
      "Epoch: 006/008 | Batch 082/266 | Loss: 0.2552\n",
      "Epoch: 006/008 | Batch 083/266 | Loss: 0.0878\n",
      "Epoch: 006/008 | Batch 084/266 | Loss: 0.1440\n",
      "Epoch: 006/008 | Batch 085/266 | Loss: 0.1600\n",
      "Epoch: 006/008 | Batch 086/266 | Loss: 0.1107\n",
      "Epoch: 006/008 | Batch 087/266 | Loss: 0.0988\n",
      "Epoch: 006/008 | Batch 088/266 | Loss: 0.1129\n",
      "Epoch: 006/008 | Batch 089/266 | Loss: 0.1224\n",
      "Epoch: 006/008 | Batch 090/266 | Loss: 0.1517\n",
      "Epoch: 006/008 | Batch 091/266 | Loss: 0.1313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006/008 | Batch 092/266 | Loss: 0.1889\n",
      "Epoch: 006/008 | Batch 093/266 | Loss: 0.1378\n",
      "Epoch: 006/008 | Batch 094/266 | Loss: 0.1296\n",
      "Epoch: 006/008 | Batch 095/266 | Loss: 0.1110\n",
      "Epoch: 006/008 | Batch 096/266 | Loss: 0.1823\n",
      "Epoch: 006/008 | Batch 097/266 | Loss: 0.1198\n",
      "Epoch: 006/008 | Batch 098/266 | Loss: 0.1585\n",
      "Epoch: 006/008 | Batch 099/266 | Loss: 0.1443\n",
      "Epoch: 006/008 | Batch 100/266 | Loss: 0.0780\n",
      "Epoch: 006/008 | Batch 101/266 | Loss: 0.1148\n",
      "Epoch: 006/008 | Batch 102/266 | Loss: 0.1645\n",
      "Epoch: 006/008 | Batch 103/266 | Loss: 0.1145\n",
      "Epoch: 006/008 | Batch 104/266 | Loss: 0.0934\n",
      "Epoch: 006/008 | Batch 105/266 | Loss: 0.1002\n",
      "Epoch: 006/008 | Batch 106/266 | Loss: 0.1959\n",
      "Epoch: 006/008 | Batch 107/266 | Loss: 0.1667\n",
      "Epoch: 006/008 | Batch 108/266 | Loss: 0.0738\n",
      "Epoch: 006/008 | Batch 109/266 | Loss: 0.1472\n",
      "Epoch: 006/008 | Batch 110/266 | Loss: 0.1215\n",
      "Epoch: 006/008 | Batch 111/266 | Loss: 0.0614\n",
      "Epoch: 006/008 | Batch 112/266 | Loss: 0.1199\n",
      "Epoch: 006/008 | Batch 113/266 | Loss: 0.0852\n",
      "Epoch: 006/008 | Batch 114/266 | Loss: 0.1388\n",
      "Epoch: 006/008 | Batch 115/266 | Loss: 0.1841\n",
      "Epoch: 006/008 | Batch 116/266 | Loss: 0.1764\n",
      "Epoch: 006/008 | Batch 117/266 | Loss: 0.1640\n",
      "Epoch: 006/008 | Batch 118/266 | Loss: 0.2086\n",
      "Epoch: 006/008 | Batch 119/266 | Loss: 0.1940\n",
      "Epoch: 006/008 | Batch 120/266 | Loss: 0.0862\n",
      "Epoch: 006/008 | Batch 121/266 | Loss: 0.1228\n",
      "Epoch: 006/008 | Batch 122/266 | Loss: 0.0562\n",
      "Epoch: 006/008 | Batch 123/266 | Loss: 0.1353\n",
      "Epoch: 006/008 | Batch 124/266 | Loss: 0.1513\n",
      "Epoch: 006/008 | Batch 125/266 | Loss: 0.1653\n",
      "Epoch: 006/008 | Batch 126/266 | Loss: 0.0533\n",
      "Epoch: 006/008 | Batch 127/266 | Loss: 0.1697\n",
      "Epoch: 006/008 | Batch 128/266 | Loss: 0.1392\n",
      "Epoch: 006/008 | Batch 129/266 | Loss: 0.1069\n",
      "Epoch: 006/008 | Batch 130/266 | Loss: 0.1102\n",
      "Epoch: 006/008 | Batch 131/266 | Loss: 0.1528\n",
      "Epoch: 006/008 | Batch 132/266 | Loss: 0.1485\n",
      "Epoch: 006/008 | Batch 133/266 | Loss: 0.1608\n",
      "Epoch: 006/008 | Batch 134/266 | Loss: 0.1387\n",
      "Epoch: 006/008 | Batch 135/266 | Loss: 0.2101\n",
      "Epoch: 006/008 | Batch 136/266 | Loss: 0.1249\n",
      "Epoch: 006/008 | Batch 137/266 | Loss: 0.1144\n",
      "Epoch: 006/008 | Batch 138/266 | Loss: 0.1796\n",
      "Epoch: 006/008 | Batch 139/266 | Loss: 0.1518\n",
      "Epoch: 006/008 | Batch 140/266 | Loss: 0.1097\n",
      "Epoch: 006/008 | Batch 141/266 | Loss: 0.2378\n",
      "Epoch: 006/008 | Batch 142/266 | Loss: 0.1904\n",
      "Epoch: 006/008 | Batch 143/266 | Loss: 0.2095\n",
      "Epoch: 006/008 | Batch 144/266 | Loss: 0.1117\n",
      "Epoch: 006/008 | Batch 145/266 | Loss: 0.2028\n",
      "Epoch: 006/008 | Batch 146/266 | Loss: 0.0975\n",
      "Epoch: 006/008 | Batch 147/266 | Loss: 0.1484\n",
      "Epoch: 006/008 | Batch 148/266 | Loss: 0.1403\n",
      "Epoch: 006/008 | Batch 149/266 | Loss: 0.1152\n",
      "Epoch: 006/008 | Batch 150/266 | Loss: 0.1905\n",
      "Epoch: 006/008 | Batch 151/266 | Loss: 0.1760\n",
      "Epoch: 006/008 | Batch 152/266 | Loss: 0.0947\n",
      "Epoch: 006/008 | Batch 153/266 | Loss: 0.0683\n",
      "Epoch: 006/008 | Batch 154/266 | Loss: 0.2328\n",
      "Epoch: 006/008 | Batch 155/266 | Loss: 0.1435\n",
      "Epoch: 006/008 | Batch 156/266 | Loss: 0.1521\n",
      "Epoch: 006/008 | Batch 157/266 | Loss: 0.1592\n",
      "Epoch: 006/008 | Batch 158/266 | Loss: 0.1320\n",
      "Epoch: 006/008 | Batch 159/266 | Loss: 0.2150\n",
      "Epoch: 006/008 | Batch 160/266 | Loss: 0.1300\n",
      "Epoch: 006/008 | Batch 161/266 | Loss: 0.1941\n",
      "Epoch: 006/008 | Batch 162/266 | Loss: 0.2890\n",
      "Epoch: 006/008 | Batch 163/266 | Loss: 0.0807\n",
      "Epoch: 006/008 | Batch 164/266 | Loss: 0.2169\n",
      "Epoch: 006/008 | Batch 165/266 | Loss: 0.1766\n",
      "Epoch: 006/008 | Batch 166/266 | Loss: 0.2376\n",
      "Epoch: 006/008 | Batch 167/266 | Loss: 0.1407\n",
      "Epoch: 006/008 | Batch 168/266 | Loss: 0.0822\n",
      "Epoch: 006/008 | Batch 169/266 | Loss: 0.2032\n",
      "Epoch: 006/008 | Batch 170/266 | Loss: 0.1277\n",
      "Epoch: 006/008 | Batch 171/266 | Loss: 0.1867\n",
      "Epoch: 006/008 | Batch 172/266 | Loss: 0.2572\n",
      "Epoch: 006/008 | Batch 173/266 | Loss: 0.1384\n",
      "Epoch: 006/008 | Batch 174/266 | Loss: 0.1570\n",
      "Epoch: 006/008 | Batch 175/266 | Loss: 0.1590\n",
      "Epoch: 006/008 | Batch 176/266 | Loss: 0.1681\n",
      "Epoch: 006/008 | Batch 177/266 | Loss: 0.0952\n",
      "Epoch: 006/008 | Batch 178/266 | Loss: 0.1022\n",
      "Epoch: 006/008 | Batch 179/266 | Loss: 0.0762\n",
      "Epoch: 006/008 | Batch 180/266 | Loss: 0.2637\n",
      "Epoch: 006/008 | Batch 181/266 | Loss: 0.1303\n",
      "Epoch: 006/008 | Batch 182/266 | Loss: 0.1801\n",
      "Epoch: 006/008 | Batch 183/266 | Loss: 0.0789\n",
      "Epoch: 006/008 | Batch 184/266 | Loss: 0.0964\n",
      "Epoch: 006/008 | Batch 185/266 | Loss: 0.1652\n",
      "Epoch: 006/008 | Batch 186/266 | Loss: 0.1913\n",
      "Epoch: 006/008 | Batch 187/266 | Loss: 0.1597\n",
      "Epoch: 006/008 | Batch 188/266 | Loss: 0.1104\n",
      "Epoch: 006/008 | Batch 189/266 | Loss: 0.0858\n",
      "Epoch: 006/008 | Batch 190/266 | Loss: 0.1515\n",
      "Epoch: 006/008 | Batch 191/266 | Loss: 0.1371\n",
      "Epoch: 006/008 | Batch 192/266 | Loss: 0.1738\n",
      "Epoch: 006/008 | Batch 193/266 | Loss: 0.1202\n",
      "Epoch: 006/008 | Batch 194/266 | Loss: 0.1646\n",
      "Epoch: 006/008 | Batch 195/266 | Loss: 0.1404\n",
      "Epoch: 006/008 | Batch 196/266 | Loss: 0.1204\n",
      "Epoch: 006/008 | Batch 197/266 | Loss: 0.2261\n",
      "Epoch: 006/008 | Batch 198/266 | Loss: 0.1444\n",
      "Epoch: 006/008 | Batch 199/266 | Loss: 0.0848\n",
      "Epoch: 006/008 | Batch 200/266 | Loss: 0.1220\n",
      "Epoch: 006/008 | Batch 201/266 | Loss: 0.1186\n",
      "Epoch: 006/008 | Batch 202/266 | Loss: 0.0734\n",
      "Epoch: 006/008 | Batch 203/266 | Loss: 0.1553\n",
      "Epoch: 006/008 | Batch 204/266 | Loss: 0.1840\n",
      "Epoch: 006/008 | Batch 205/266 | Loss: 0.1944\n",
      "Epoch: 006/008 | Batch 206/266 | Loss: 0.2374\n",
      "Epoch: 006/008 | Batch 207/266 | Loss: 0.1094\n",
      "Epoch: 006/008 | Batch 208/266 | Loss: 0.1259\n",
      "Epoch: 006/008 | Batch 209/266 | Loss: 0.0954\n",
      "Epoch: 006/008 | Batch 210/266 | Loss: 0.2415\n",
      "Epoch: 006/008 | Batch 211/266 | Loss: 0.1258\n",
      "Epoch: 006/008 | Batch 212/266 | Loss: 0.2311\n",
      "Epoch: 006/008 | Batch 213/266 | Loss: 0.1842\n",
      "Epoch: 006/008 | Batch 214/266 | Loss: 0.1242\n",
      "Epoch: 006/008 | Batch 215/266 | Loss: 0.2576\n",
      "Epoch: 006/008 | Batch 216/266 | Loss: 0.1569\n",
      "Epoch: 006/008 | Batch 217/266 | Loss: 0.0883\n",
      "Epoch: 006/008 | Batch 218/266 | Loss: 0.1775\n",
      "Epoch: 006/008 | Batch 219/266 | Loss: 0.1003\n",
      "Epoch: 006/008 | Batch 220/266 | Loss: 0.1672\n",
      "Epoch: 006/008 | Batch 221/266 | Loss: 0.1766\n",
      "Epoch: 006/008 | Batch 222/266 | Loss: 0.1127\n",
      "Epoch: 006/008 | Batch 223/266 | Loss: 0.1754\n",
      "Epoch: 006/008 | Batch 224/266 | Loss: 0.1916\n",
      "Epoch: 006/008 | Batch 225/266 | Loss: 0.1978\n",
      "Epoch: 006/008 | Batch 226/266 | Loss: 0.0633\n",
      "Epoch: 006/008 | Batch 227/266 | Loss: 0.2162\n",
      "Epoch: 006/008 | Batch 228/266 | Loss: 0.1611\n",
      "Epoch: 006/008 | Batch 229/266 | Loss: 0.1973\n",
      "Epoch: 006/008 | Batch 230/266 | Loss: 0.1677\n",
      "Epoch: 006/008 | Batch 231/266 | Loss: 0.1892\n",
      "Epoch: 006/008 | Batch 232/266 | Loss: 0.2244\n",
      "Epoch: 006/008 | Batch 233/266 | Loss: 0.1927\n",
      "Epoch: 006/008 | Batch 234/266 | Loss: 0.1736\n",
      "Epoch: 006/008 | Batch 235/266 | Loss: 0.1005\n",
      "Epoch: 006/008 | Batch 236/266 | Loss: 0.0698\n",
      "Epoch: 006/008 | Batch 237/266 | Loss: 0.0883\n",
      "Epoch: 006/008 | Batch 238/266 | Loss: 0.1496\n",
      "Epoch: 006/008 | Batch 239/266 | Loss: 0.1915\n",
      "Epoch: 006/008 | Batch 240/266 | Loss: 0.1416\n",
      "Epoch: 006/008 | Batch 241/266 | Loss: 0.1304\n",
      "Epoch: 006/008 | Batch 242/266 | Loss: 0.1654\n",
      "Epoch: 006/008 | Batch 243/266 | Loss: 0.1875\n",
      "Epoch: 006/008 | Batch 244/266 | Loss: 0.1474\n",
      "Epoch: 006/008 | Batch 245/266 | Loss: 0.1765\n",
      "Epoch: 006/008 | Batch 246/266 | Loss: 0.0967\n",
      "Epoch: 006/008 | Batch 247/266 | Loss: 0.2131\n",
      "Epoch: 006/008 | Batch 248/266 | Loss: 0.0785\n",
      "Epoch: 006/008 | Batch 249/266 | Loss: 0.1527\n",
      "Epoch: 006/008 | Batch 250/266 | Loss: 0.1235\n",
      "Epoch: 006/008 | Batch 251/266 | Loss: 0.1299\n",
      "Epoch: 006/008 | Batch 252/266 | Loss: 0.1708\n",
      "Epoch: 006/008 | Batch 253/266 | Loss: 0.2419\n",
      "Epoch: 006/008 | Batch 254/266 | Loss: 0.1612\n",
      "Epoch: 006/008 | Batch 255/266 | Loss: 0.1638\n",
      "Epoch: 006/008 | Batch 256/266 | Loss: 0.1685\n",
      "Epoch: 006/008 | Batch 257/266 | Loss: 0.1265\n",
      "Epoch: 006/008 | Batch 258/266 | Loss: 0.1782\n",
      "Epoch: 006/008 | Batch 259/266 | Loss: 0.1319\n",
      "Epoch: 006/008 | Batch 260/266 | Loss: 0.1613\n",
      "Epoch: 006/008 | Batch 261/266 | Loss: 0.2179\n",
      "Epoch: 006/008 | Batch 262/266 | Loss: 0.1391\n",
      "Epoch: 006/008 | Batch 263/266 | Loss: 0.1575\n",
      "Epoch: 006/008 | Batch 264/266 | Loss: 0.2239\n",
      "Epoch: 006/008 | Batch 265/266 | Loss: 0.1182\n",
      "training accuracy: 97.51%\n",
      "valid accuracy: 88.10%\n",
      "Time elapsed: 19.69 min\n",
      "Epoch: 007/008 | Batch 000/266 | Loss: 0.1445\n",
      "Epoch: 007/008 | Batch 001/266 | Loss: 0.0949\n",
      "Epoch: 007/008 | Batch 002/266 | Loss: 0.0995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007/008 | Batch 003/266 | Loss: 0.0639\n",
      "Epoch: 007/008 | Batch 004/266 | Loss: 0.1504\n",
      "Epoch: 007/008 | Batch 005/266 | Loss: 0.1172\n",
      "Epoch: 007/008 | Batch 006/266 | Loss: 0.0720\n",
      "Epoch: 007/008 | Batch 007/266 | Loss: 0.0579\n",
      "Epoch: 007/008 | Batch 008/266 | Loss: 0.0515\n",
      "Epoch: 007/008 | Batch 009/266 | Loss: 0.0838\n",
      "Epoch: 007/008 | Batch 010/266 | Loss: 0.1853\n",
      "Epoch: 007/008 | Batch 011/266 | Loss: 0.1345\n",
      "Epoch: 007/008 | Batch 012/266 | Loss: 0.0373\n",
      "Epoch: 007/008 | Batch 013/266 | Loss: 0.0568\n",
      "Epoch: 007/008 | Batch 014/266 | Loss: 0.0487\n",
      "Epoch: 007/008 | Batch 015/266 | Loss: 0.1226\n",
      "Epoch: 007/008 | Batch 016/266 | Loss: 0.0895\n",
      "Epoch: 007/008 | Batch 017/266 | Loss: 0.0995\n",
      "Epoch: 007/008 | Batch 018/266 | Loss: 0.0646\n",
      "Epoch: 007/008 | Batch 019/266 | Loss: 0.0764\n",
      "Epoch: 007/008 | Batch 020/266 | Loss: 0.2157\n",
      "Epoch: 007/008 | Batch 021/266 | Loss: 0.0981\n",
      "Epoch: 007/008 | Batch 022/266 | Loss: 0.0389\n",
      "Epoch: 007/008 | Batch 023/266 | Loss: 0.1553\n",
      "Epoch: 007/008 | Batch 024/266 | Loss: 0.1743\n",
      "Epoch: 007/008 | Batch 025/266 | Loss: 0.0694\n",
      "Epoch: 007/008 | Batch 026/266 | Loss: 0.1135\n",
      "Epoch: 007/008 | Batch 027/266 | Loss: 0.0464\n",
      "Epoch: 007/008 | Batch 028/266 | Loss: 0.0775\n",
      "Epoch: 007/008 | Batch 029/266 | Loss: 0.1091\n",
      "Epoch: 007/008 | Batch 030/266 | Loss: 0.0575\n",
      "Epoch: 007/008 | Batch 031/266 | Loss: 0.1301\n",
      "Epoch: 007/008 | Batch 032/266 | Loss: 0.0846\n",
      "Epoch: 007/008 | Batch 033/266 | Loss: 0.0765\n",
      "Epoch: 007/008 | Batch 034/266 | Loss: 0.0790\n",
      "Epoch: 007/008 | Batch 035/266 | Loss: 0.1195\n",
      "Epoch: 007/008 | Batch 036/266 | Loss: 0.0741\n",
      "Epoch: 007/008 | Batch 037/266 | Loss: 0.1596\n",
      "Epoch: 007/008 | Batch 038/266 | Loss: 0.1629\n",
      "Epoch: 007/008 | Batch 039/266 | Loss: 0.0784\n",
      "Epoch: 007/008 | Batch 040/266 | Loss: 0.0601\n",
      "Epoch: 007/008 | Batch 041/266 | Loss: 0.1595\n",
      "Epoch: 007/008 | Batch 042/266 | Loss: 0.0750\n",
      "Epoch: 007/008 | Batch 043/266 | Loss: 0.1030\n",
      "Epoch: 007/008 | Batch 044/266 | Loss: 0.1656\n",
      "Epoch: 007/008 | Batch 045/266 | Loss: 0.1304\n",
      "Epoch: 007/008 | Batch 046/266 | Loss: 0.1531\n",
      "Epoch: 007/008 | Batch 047/266 | Loss: 0.0474\n",
      "Epoch: 007/008 | Batch 048/266 | Loss: 0.0475\n",
      "Epoch: 007/008 | Batch 049/266 | Loss: 0.0491\n",
      "Epoch: 007/008 | Batch 050/266 | Loss: 0.1025\n",
      "Epoch: 007/008 | Batch 051/266 | Loss: 0.2047\n",
      "Epoch: 007/008 | Batch 052/266 | Loss: 0.0717\n",
      "Epoch: 007/008 | Batch 053/266 | Loss: 0.0416\n",
      "Epoch: 007/008 | Batch 054/266 | Loss: 0.2307\n",
      "Epoch: 007/008 | Batch 055/266 | Loss: 0.0375\n",
      "Epoch: 007/008 | Batch 056/266 | Loss: 0.0568\n",
      "Epoch: 007/008 | Batch 057/266 | Loss: 0.0742\n",
      "Epoch: 007/008 | Batch 058/266 | Loss: 0.0942\n",
      "Epoch: 007/008 | Batch 059/266 | Loss: 0.0990\n",
      "Epoch: 007/008 | Batch 060/266 | Loss: 0.0599\n",
      "Epoch: 007/008 | Batch 061/266 | Loss: 0.1198\n",
      "Epoch: 007/008 | Batch 062/266 | Loss: 0.1293\n",
      "Epoch: 007/008 | Batch 063/266 | Loss: 0.0789\n",
      "Epoch: 007/008 | Batch 064/266 | Loss: 0.1216\n",
      "Epoch: 007/008 | Batch 065/266 | Loss: 0.1478\n",
      "Epoch: 007/008 | Batch 066/266 | Loss: 0.1332\n",
      "Epoch: 007/008 | Batch 067/266 | Loss: 0.1235\n",
      "Epoch: 007/008 | Batch 068/266 | Loss: 0.0290\n",
      "Epoch: 007/008 | Batch 069/266 | Loss: 0.1268\n",
      "Epoch: 007/008 | Batch 070/266 | Loss: 0.0401\n",
      "Epoch: 007/008 | Batch 071/266 | Loss: 0.0990\n",
      "Epoch: 007/008 | Batch 072/266 | Loss: 0.0633\n",
      "Epoch: 007/008 | Batch 073/266 | Loss: 0.1832\n",
      "Epoch: 007/008 | Batch 074/266 | Loss: 0.1300\n",
      "Epoch: 007/008 | Batch 075/266 | Loss: 0.0549\n",
      "Epoch: 007/008 | Batch 076/266 | Loss: 0.0892\n",
      "Epoch: 007/008 | Batch 077/266 | Loss: 0.1738\n",
      "Epoch: 007/008 | Batch 078/266 | Loss: 0.0874\n",
      "Epoch: 007/008 | Batch 079/266 | Loss: 0.1141\n",
      "Epoch: 007/008 | Batch 080/266 | Loss: 0.0746\n",
      "Epoch: 007/008 | Batch 081/266 | Loss: 0.1437\n",
      "Epoch: 007/008 | Batch 082/266 | Loss: 0.0300\n",
      "Epoch: 007/008 | Batch 083/266 | Loss: 0.0627\n",
      "Epoch: 007/008 | Batch 084/266 | Loss: 0.0720\n",
      "Epoch: 007/008 | Batch 085/266 | Loss: 0.2275\n",
      "Epoch: 007/008 | Batch 086/266 | Loss: 0.0265\n",
      "Epoch: 007/008 | Batch 087/266 | Loss: 0.1551\n",
      "Epoch: 007/008 | Batch 088/266 | Loss: 0.0929\n",
      "Epoch: 007/008 | Batch 089/266 | Loss: 0.0710\n",
      "Epoch: 007/008 | Batch 090/266 | Loss: 0.0881\n",
      "Epoch: 007/008 | Batch 091/266 | Loss: 0.0567\n",
      "Epoch: 007/008 | Batch 092/266 | Loss: 0.0805\n",
      "Epoch: 007/008 | Batch 093/266 | Loss: 0.1541\n",
      "Epoch: 007/008 | Batch 094/266 | Loss: 0.1262\n",
      "Epoch: 007/008 | Batch 095/266 | Loss: 0.0539\n",
      "Epoch: 007/008 | Batch 096/266 | Loss: 0.0819\n",
      "Epoch: 007/008 | Batch 097/266 | Loss: 0.1397\n",
      "Epoch: 007/008 | Batch 098/266 | Loss: 0.0482\n",
      "Epoch: 007/008 | Batch 099/266 | Loss: 0.0302\n",
      "Epoch: 007/008 | Batch 100/266 | Loss: 0.0443\n",
      "Epoch: 007/008 | Batch 101/266 | Loss: 0.1621\n",
      "Epoch: 007/008 | Batch 102/266 | Loss: 0.1243\n",
      "Epoch: 007/008 | Batch 103/266 | Loss: 0.0600\n",
      "Epoch: 007/008 | Batch 104/266 | Loss: 0.1078\n",
      "Epoch: 007/008 | Batch 105/266 | Loss: 0.1071\n",
      "Epoch: 007/008 | Batch 106/266 | Loss: 0.1051\n",
      "Epoch: 007/008 | Batch 107/266 | Loss: 0.1255\n",
      "Epoch: 007/008 | Batch 108/266 | Loss: 0.0779\n",
      "Epoch: 007/008 | Batch 109/266 | Loss: 0.0787\n",
      "Epoch: 007/008 | Batch 110/266 | Loss: 0.2181\n",
      "Epoch: 007/008 | Batch 111/266 | Loss: 0.1408\n",
      "Epoch: 007/008 | Batch 112/266 | Loss: 0.1029\n",
      "Epoch: 007/008 | Batch 113/266 | Loss: 0.1210\n",
      "Epoch: 007/008 | Batch 114/266 | Loss: 0.1579\n",
      "Epoch: 007/008 | Batch 115/266 | Loss: 0.0248\n",
      "Epoch: 007/008 | Batch 116/266 | Loss: 0.0832\n",
      "Epoch: 007/008 | Batch 117/266 | Loss: 0.1244\n",
      "Epoch: 007/008 | Batch 118/266 | Loss: 0.0730\n",
      "Epoch: 007/008 | Batch 119/266 | Loss: 0.0949\n",
      "Epoch: 007/008 | Batch 120/266 | Loss: 0.1056\n",
      "Epoch: 007/008 | Batch 121/266 | Loss: 0.0544\n",
      "Epoch: 007/008 | Batch 122/266 | Loss: 0.1337\n",
      "Epoch: 007/008 | Batch 123/266 | Loss: 0.1183\n",
      "Epoch: 007/008 | Batch 124/266 | Loss: 0.0840\n",
      "Epoch: 007/008 | Batch 125/266 | Loss: 0.0716\n",
      "Epoch: 007/008 | Batch 126/266 | Loss: 0.0448\n",
      "Epoch: 007/008 | Batch 127/266 | Loss: 0.1572\n",
      "Epoch: 007/008 | Batch 128/266 | Loss: 0.1558\n",
      "Epoch: 007/008 | Batch 129/266 | Loss: 0.0461\n",
      "Epoch: 007/008 | Batch 130/266 | Loss: 0.0853\n",
      "Epoch: 007/008 | Batch 131/266 | Loss: 0.1066\n",
      "Epoch: 007/008 | Batch 132/266 | Loss: 0.0490\n",
      "Epoch: 007/008 | Batch 133/266 | Loss: 0.1823\n",
      "Epoch: 007/008 | Batch 134/266 | Loss: 0.1983\n",
      "Epoch: 007/008 | Batch 135/266 | Loss: 0.1221\n",
      "Epoch: 007/008 | Batch 136/266 | Loss: 0.1083\n",
      "Epoch: 007/008 | Batch 137/266 | Loss: 0.0878\n",
      "Epoch: 007/008 | Batch 138/266 | Loss: 0.1146\n",
      "Epoch: 007/008 | Batch 139/266 | Loss: 0.1446\n",
      "Epoch: 007/008 | Batch 140/266 | Loss: 0.0890\n",
      "Epoch: 007/008 | Batch 141/266 | Loss: 0.0736\n",
      "Epoch: 007/008 | Batch 142/266 | Loss: 0.1316\n",
      "Epoch: 007/008 | Batch 143/266 | Loss: 0.0247\n",
      "Epoch: 007/008 | Batch 144/266 | Loss: 0.1824\n",
      "Epoch: 007/008 | Batch 145/266 | Loss: 0.1130\n",
      "Epoch: 007/008 | Batch 146/266 | Loss: 0.0514\n",
      "Epoch: 007/008 | Batch 147/266 | Loss: 0.0685\n",
      "Epoch: 007/008 | Batch 148/266 | Loss: 0.0881\n",
      "Epoch: 007/008 | Batch 149/266 | Loss: 0.0843\n",
      "Epoch: 007/008 | Batch 150/266 | Loss: 0.1328\n",
      "Epoch: 007/008 | Batch 151/266 | Loss: 0.1648\n",
      "Epoch: 007/008 | Batch 152/266 | Loss: 0.1253\n",
      "Epoch: 007/008 | Batch 153/266 | Loss: 0.1448\n",
      "Epoch: 007/008 | Batch 154/266 | Loss: 0.0850\n",
      "Epoch: 007/008 | Batch 155/266 | Loss: 0.0478\n",
      "Epoch: 007/008 | Batch 156/266 | Loss: 0.1285\n",
      "Epoch: 007/008 | Batch 157/266 | Loss: 0.1320\n",
      "Epoch: 007/008 | Batch 158/266 | Loss: 0.0892\n",
      "Epoch: 007/008 | Batch 159/266 | Loss: 0.0480\n",
      "Epoch: 007/008 | Batch 160/266 | Loss: 0.1736\n",
      "Epoch: 007/008 | Batch 161/266 | Loss: 0.1097\n",
      "Epoch: 007/008 | Batch 162/266 | Loss: 0.1447\n",
      "Epoch: 007/008 | Batch 163/266 | Loss: 0.0838\n",
      "Epoch: 007/008 | Batch 164/266 | Loss: 0.1310\n",
      "Epoch: 007/008 | Batch 165/266 | Loss: 0.1504\n",
      "Epoch: 007/008 | Batch 166/266 | Loss: 0.0491\n",
      "Epoch: 007/008 | Batch 167/266 | Loss: 0.0430\n",
      "Epoch: 007/008 | Batch 168/266 | Loss: 0.0699\n",
      "Epoch: 007/008 | Batch 169/266 | Loss: 0.0476\n",
      "Epoch: 007/008 | Batch 170/266 | Loss: 0.0685\n",
      "Epoch: 007/008 | Batch 171/266 | Loss: 0.0639\n",
      "Epoch: 007/008 | Batch 172/266 | Loss: 0.0730\n",
      "Epoch: 007/008 | Batch 173/266 | Loss: 0.0771\n",
      "Epoch: 007/008 | Batch 174/266 | Loss: 0.1302\n",
      "Epoch: 007/008 | Batch 175/266 | Loss: 0.0318\n",
      "Epoch: 007/008 | Batch 176/266 | Loss: 0.0750\n",
      "Epoch: 007/008 | Batch 177/266 | Loss: 0.0695\n",
      "Epoch: 007/008 | Batch 178/266 | Loss: 0.1352\n",
      "Epoch: 007/008 | Batch 179/266 | Loss: 0.0655\n",
      "Epoch: 007/008 | Batch 180/266 | Loss: 0.1058\n",
      "Epoch: 007/008 | Batch 181/266 | Loss: 0.0679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007/008 | Batch 182/266 | Loss: 0.0636\n",
      "Epoch: 007/008 | Batch 183/266 | Loss: 0.1222\n",
      "Epoch: 007/008 | Batch 184/266 | Loss: 0.0462\n",
      "Epoch: 007/008 | Batch 185/266 | Loss: 0.0586\n",
      "Epoch: 007/008 | Batch 186/266 | Loss: 0.0672\n",
      "Epoch: 007/008 | Batch 187/266 | Loss: 0.0596\n",
      "Epoch: 007/008 | Batch 188/266 | Loss: 0.1320\n",
      "Epoch: 007/008 | Batch 189/266 | Loss: 0.1222\n",
      "Epoch: 007/008 | Batch 190/266 | Loss: 0.1490\n",
      "Epoch: 007/008 | Batch 191/266 | Loss: 0.0819\n",
      "Epoch: 007/008 | Batch 192/266 | Loss: 0.1482\n",
      "Epoch: 007/008 | Batch 193/266 | Loss: 0.0828\n",
      "Epoch: 007/008 | Batch 194/266 | Loss: 0.0356\n",
      "Epoch: 007/008 | Batch 195/266 | Loss: 0.1933\n",
      "Epoch: 007/008 | Batch 196/266 | Loss: 0.0940\n",
      "Epoch: 007/008 | Batch 197/266 | Loss: 0.0460\n",
      "Epoch: 007/008 | Batch 198/266 | Loss: 0.2468\n",
      "Epoch: 007/008 | Batch 199/266 | Loss: 0.1178\n",
      "Epoch: 007/008 | Batch 200/266 | Loss: 0.1225\n",
      "Epoch: 007/008 | Batch 201/266 | Loss: 0.1082\n",
      "Epoch: 007/008 | Batch 202/266 | Loss: 0.0853\n",
      "Epoch: 007/008 | Batch 203/266 | Loss: 0.2817\n",
      "Epoch: 007/008 | Batch 204/266 | Loss: 0.2136\n",
      "Epoch: 007/008 | Batch 205/266 | Loss: 0.0837\n",
      "Epoch: 007/008 | Batch 206/266 | Loss: 0.0606\n",
      "Epoch: 007/008 | Batch 207/266 | Loss: 0.0350\n",
      "Epoch: 007/008 | Batch 208/266 | Loss: 0.1296\n",
      "Epoch: 007/008 | Batch 209/266 | Loss: 0.1149\n",
      "Epoch: 007/008 | Batch 210/266 | Loss: 0.0981\n",
      "Epoch: 007/008 | Batch 211/266 | Loss: 0.0743\n",
      "Epoch: 007/008 | Batch 212/266 | Loss: 0.1299\n",
      "Epoch: 007/008 | Batch 213/266 | Loss: 0.1342\n",
      "Epoch: 007/008 | Batch 214/266 | Loss: 0.1533\n",
      "Epoch: 007/008 | Batch 215/266 | Loss: 0.0906\n",
      "Epoch: 007/008 | Batch 216/266 | Loss: 0.0774\n",
      "Epoch: 007/008 | Batch 217/266 | Loss: 0.0820\n",
      "Epoch: 007/008 | Batch 218/266 | Loss: 0.1298\n",
      "Epoch: 007/008 | Batch 219/266 | Loss: 0.0977\n",
      "Epoch: 007/008 | Batch 220/266 | Loss: 0.0893\n",
      "Epoch: 007/008 | Batch 221/266 | Loss: 0.1293\n",
      "Epoch: 007/008 | Batch 222/266 | Loss: 0.0310\n",
      "Epoch: 007/008 | Batch 223/266 | Loss: 0.1352\n",
      "Epoch: 007/008 | Batch 224/266 | Loss: 0.1034\n",
      "Epoch: 007/008 | Batch 225/266 | Loss: 0.0996\n",
      "Epoch: 007/008 | Batch 226/266 | Loss: 0.1455\n",
      "Epoch: 007/008 | Batch 227/266 | Loss: 0.1053\n",
      "Epoch: 007/008 | Batch 228/266 | Loss: 0.1592\n",
      "Epoch: 007/008 | Batch 229/266 | Loss: 0.0417\n",
      "Epoch: 007/008 | Batch 230/266 | Loss: 0.1087\n",
      "Epoch: 007/008 | Batch 231/266 | Loss: 0.0736\n",
      "Epoch: 007/008 | Batch 232/266 | Loss: 0.1018\n",
      "Epoch: 007/008 | Batch 233/266 | Loss: 0.0553\n",
      "Epoch: 007/008 | Batch 234/266 | Loss: 0.1184\n",
      "Epoch: 007/008 | Batch 235/266 | Loss: 0.1051\n",
      "Epoch: 007/008 | Batch 236/266 | Loss: 0.1754\n",
      "Epoch: 007/008 | Batch 237/266 | Loss: 0.1189\n",
      "Epoch: 007/008 | Batch 238/266 | Loss: 0.1174\n",
      "Epoch: 007/008 | Batch 239/266 | Loss: 0.0889\n",
      "Epoch: 007/008 | Batch 240/266 | Loss: 0.0850\n",
      "Epoch: 007/008 | Batch 241/266 | Loss: 0.1925\n",
      "Epoch: 007/008 | Batch 242/266 | Loss: 0.1081\n",
      "Epoch: 007/008 | Batch 243/266 | Loss: 0.0579\n",
      "Epoch: 007/008 | Batch 244/266 | Loss: 0.1000\n",
      "Epoch: 007/008 | Batch 245/266 | Loss: 0.1544\n",
      "Epoch: 007/008 | Batch 246/266 | Loss: 0.0750\n",
      "Epoch: 007/008 | Batch 247/266 | Loss: 0.0675\n",
      "Epoch: 007/008 | Batch 248/266 | Loss: 0.1760\n",
      "Epoch: 007/008 | Batch 249/266 | Loss: 0.1319\n",
      "Epoch: 007/008 | Batch 250/266 | Loss: 0.0509\n",
      "Epoch: 007/008 | Batch 251/266 | Loss: 0.0805\n",
      "Epoch: 007/008 | Batch 252/266 | Loss: 0.1799\n",
      "Epoch: 007/008 | Batch 253/266 | Loss: 0.0948\n",
      "Epoch: 007/008 | Batch 254/266 | Loss: 0.1130\n",
      "Epoch: 007/008 | Batch 255/266 | Loss: 0.0665\n",
      "Epoch: 007/008 | Batch 256/266 | Loss: 0.0445\n",
      "Epoch: 007/008 | Batch 257/266 | Loss: 0.1122\n",
      "Epoch: 007/008 | Batch 258/266 | Loss: 0.1306\n",
      "Epoch: 007/008 | Batch 259/266 | Loss: 0.1278\n",
      "Epoch: 007/008 | Batch 260/266 | Loss: 0.1923\n",
      "Epoch: 007/008 | Batch 261/266 | Loss: 0.0624\n",
      "Epoch: 007/008 | Batch 262/266 | Loss: 0.1246\n",
      "Epoch: 007/008 | Batch 263/266 | Loss: 0.0969\n",
      "Epoch: 007/008 | Batch 264/266 | Loss: 0.1998\n",
      "Epoch: 007/008 | Batch 265/266 | Loss: 0.1426\n",
      "training accuracy: 98.46%\n",
      "valid accuracy: 88.28%\n",
      "Time elapsed: 22.43 min\n",
      "Epoch: 008/008 | Batch 000/266 | Loss: 0.0519\n",
      "Epoch: 008/008 | Batch 001/266 | Loss: 0.0672\n",
      "Epoch: 008/008 | Batch 002/266 | Loss: 0.0217\n",
      "Epoch: 008/008 | Batch 003/266 | Loss: 0.0708\n",
      "Epoch: 008/008 | Batch 004/266 | Loss: 0.0860\n",
      "Epoch: 008/008 | Batch 005/266 | Loss: 0.0574\n",
      "Epoch: 008/008 | Batch 006/266 | Loss: 0.1152\n",
      "Epoch: 008/008 | Batch 007/266 | Loss: 0.0803\n",
      "Epoch: 008/008 | Batch 008/266 | Loss: 0.0755\n",
      "Epoch: 008/008 | Batch 009/266 | Loss: 0.0569\n",
      "Epoch: 008/008 | Batch 010/266 | Loss: 0.0612\n",
      "Epoch: 008/008 | Batch 011/266 | Loss: 0.0713\n",
      "Epoch: 008/008 | Batch 012/266 | Loss: 0.1129\n",
      "Epoch: 008/008 | Batch 013/266 | Loss: 0.0796\n",
      "Epoch: 008/008 | Batch 014/266 | Loss: 0.0310\n",
      "Epoch: 008/008 | Batch 015/266 | Loss: 0.0252\n",
      "Epoch: 008/008 | Batch 016/266 | Loss: 0.0675\n",
      "Epoch: 008/008 | Batch 017/266 | Loss: 0.0893\n",
      "Epoch: 008/008 | Batch 018/266 | Loss: 0.0572\n",
      "Epoch: 008/008 | Batch 019/266 | Loss: 0.0603\n",
      "Epoch: 008/008 | Batch 020/266 | Loss: 0.0181\n",
      "Epoch: 008/008 | Batch 021/266 | Loss: 0.0585\n",
      "Epoch: 008/008 | Batch 022/266 | Loss: 0.1241\n",
      "Epoch: 008/008 | Batch 023/266 | Loss: 0.0377\n",
      "Epoch: 008/008 | Batch 024/266 | Loss: 0.0902\n",
      "Epoch: 008/008 | Batch 025/266 | Loss: 0.0875\n",
      "Epoch: 008/008 | Batch 026/266 | Loss: 0.0696\n",
      "Epoch: 008/008 | Batch 027/266 | Loss: 0.0198\n",
      "Epoch: 008/008 | Batch 028/266 | Loss: 0.0874\n",
      "Epoch: 008/008 | Batch 029/266 | Loss: 0.0866\n",
      "Epoch: 008/008 | Batch 030/266 | Loss: 0.0450\n",
      "Epoch: 008/008 | Batch 031/266 | Loss: 0.1507\n",
      "Epoch: 008/008 | Batch 032/266 | Loss: 0.0648\n",
      "Epoch: 008/008 | Batch 033/266 | Loss: 0.1071\n",
      "Epoch: 008/008 | Batch 034/266 | Loss: 0.0208\n",
      "Epoch: 008/008 | Batch 035/266 | Loss: 0.1146\n",
      "Epoch: 008/008 | Batch 036/266 | Loss: 0.0914\n",
      "Epoch: 008/008 | Batch 037/266 | Loss: 0.0648\n",
      "Epoch: 008/008 | Batch 038/266 | Loss: 0.0312\n",
      "Epoch: 008/008 | Batch 039/266 | Loss: 0.0776\n",
      "Epoch: 008/008 | Batch 040/266 | Loss: 0.0212\n",
      "Epoch: 008/008 | Batch 041/266 | Loss: 0.0133\n",
      "Epoch: 008/008 | Batch 042/266 | Loss: 0.1281\n",
      "Epoch: 008/008 | Batch 043/266 | Loss: 0.0227\n",
      "Epoch: 008/008 | Batch 044/266 | Loss: 0.0586\n",
      "Epoch: 008/008 | Batch 045/266 | Loss: 0.0690\n",
      "Epoch: 008/008 | Batch 046/266 | Loss: 0.0588\n",
      "Epoch: 008/008 | Batch 047/266 | Loss: 0.0843\n",
      "Epoch: 008/008 | Batch 048/266 | Loss: 0.0418\n",
      "Epoch: 008/008 | Batch 049/266 | Loss: 0.0482\n",
      "Epoch: 008/008 | Batch 050/266 | Loss: 0.0472\n",
      "Epoch: 008/008 | Batch 051/266 | Loss: 0.0931\n",
      "Epoch: 008/008 | Batch 052/266 | Loss: 0.0336\n",
      "Epoch: 008/008 | Batch 053/266 | Loss: 0.0374\n",
      "Epoch: 008/008 | Batch 054/266 | Loss: 0.0243\n",
      "Epoch: 008/008 | Batch 055/266 | Loss: 0.0862\n",
      "Epoch: 008/008 | Batch 056/266 | Loss: 0.1023\n",
      "Epoch: 008/008 | Batch 057/266 | Loss: 0.0717\n",
      "Epoch: 008/008 | Batch 058/266 | Loss: 0.0222\n",
      "Epoch: 008/008 | Batch 059/266 | Loss: 0.0377\n",
      "Epoch: 008/008 | Batch 060/266 | Loss: 0.0912\n",
      "Epoch: 008/008 | Batch 061/266 | Loss: 0.1156\n",
      "Epoch: 008/008 | Batch 062/266 | Loss: 0.1982\n",
      "Epoch: 008/008 | Batch 063/266 | Loss: 0.0167\n",
      "Epoch: 008/008 | Batch 064/266 | Loss: 0.0494\n",
      "Epoch: 008/008 | Batch 065/266 | Loss: 0.0542\n",
      "Epoch: 008/008 | Batch 066/266 | Loss: 0.0508\n",
      "Epoch: 008/008 | Batch 067/266 | Loss: 0.0906\n",
      "Epoch: 008/008 | Batch 068/266 | Loss: 0.0647\n",
      "Epoch: 008/008 | Batch 069/266 | Loss: 0.0158\n",
      "Epoch: 008/008 | Batch 070/266 | Loss: 0.1126\n",
      "Epoch: 008/008 | Batch 071/266 | Loss: 0.0657\n",
      "Epoch: 008/008 | Batch 072/266 | Loss: 0.0949\n",
      "Epoch: 008/008 | Batch 073/266 | Loss: 0.0916\n",
      "Epoch: 008/008 | Batch 074/266 | Loss: 0.0981\n",
      "Epoch: 008/008 | Batch 075/266 | Loss: 0.1215\n",
      "Epoch: 008/008 | Batch 076/266 | Loss: 0.0644\n",
      "Epoch: 008/008 | Batch 077/266 | Loss: 0.0183\n",
      "Epoch: 008/008 | Batch 078/266 | Loss: 0.0429\n",
      "Epoch: 008/008 | Batch 079/266 | Loss: 0.0993\n",
      "Epoch: 008/008 | Batch 080/266 | Loss: 0.0655\n",
      "Epoch: 008/008 | Batch 081/266 | Loss: 0.0404\n",
      "Epoch: 008/008 | Batch 082/266 | Loss: 0.0677\n",
      "Epoch: 008/008 | Batch 083/266 | Loss: 0.0616\n",
      "Epoch: 008/008 | Batch 084/266 | Loss: 0.0748\n",
      "Epoch: 008/008 | Batch 085/266 | Loss: 0.0737\n",
      "Epoch: 008/008 | Batch 086/266 | Loss: 0.0286\n",
      "Epoch: 008/008 | Batch 087/266 | Loss: 0.0569\n",
      "Epoch: 008/008 | Batch 088/266 | Loss: 0.1041\n",
      "Epoch: 008/008 | Batch 089/266 | Loss: 0.0955\n",
      "Epoch: 008/008 | Batch 090/266 | Loss: 0.0507\n",
      "Epoch: 008/008 | Batch 091/266 | Loss: 0.0235\n",
      "Epoch: 008/008 | Batch 092/266 | Loss: 0.0135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008/008 | Batch 093/266 | Loss: 0.0883\n",
      "Epoch: 008/008 | Batch 094/266 | Loss: 0.1161\n",
      "Epoch: 008/008 | Batch 095/266 | Loss: 0.1395\n",
      "Epoch: 008/008 | Batch 096/266 | Loss: 0.0896\n",
      "Epoch: 008/008 | Batch 097/266 | Loss: 0.1375\n",
      "Epoch: 008/008 | Batch 098/266 | Loss: 0.0451\n",
      "Epoch: 008/008 | Batch 099/266 | Loss: 0.0089\n",
      "Epoch: 008/008 | Batch 100/266 | Loss: 0.0714\n",
      "Epoch: 008/008 | Batch 101/266 | Loss: 0.0900\n",
      "Epoch: 008/008 | Batch 102/266 | Loss: 0.0326\n",
      "Epoch: 008/008 | Batch 103/266 | Loss: 0.0148\n",
      "Epoch: 008/008 | Batch 104/266 | Loss: 0.0988\n",
      "Epoch: 008/008 | Batch 105/266 | Loss: 0.0458\n",
      "Epoch: 008/008 | Batch 106/266 | Loss: 0.0727\n",
      "Epoch: 008/008 | Batch 107/266 | Loss: 0.0267\n",
      "Epoch: 008/008 | Batch 108/266 | Loss: 0.0901\n",
      "Epoch: 008/008 | Batch 109/266 | Loss: 0.0452\n",
      "Epoch: 008/008 | Batch 110/266 | Loss: 0.0513\n",
      "Epoch: 008/008 | Batch 111/266 | Loss: 0.0802\n",
      "Epoch: 008/008 | Batch 112/266 | Loss: 0.0765\n",
      "Epoch: 008/008 | Batch 113/266 | Loss: 0.0576\n",
      "Epoch: 008/008 | Batch 114/266 | Loss: 0.0298\n",
      "Epoch: 008/008 | Batch 115/266 | Loss: 0.0370\n",
      "Epoch: 008/008 | Batch 116/266 | Loss: 0.0452\n",
      "Epoch: 008/008 | Batch 117/266 | Loss: 0.0763\n",
      "Epoch: 008/008 | Batch 118/266 | Loss: 0.0689\n",
      "Epoch: 008/008 | Batch 119/266 | Loss: 0.1093\n",
      "Epoch: 008/008 | Batch 120/266 | Loss: 0.0569\n",
      "Epoch: 008/008 | Batch 121/266 | Loss: 0.0556\n",
      "Epoch: 008/008 | Batch 122/266 | Loss: 0.0941\n",
      "Epoch: 008/008 | Batch 123/266 | Loss: 0.1418\n",
      "Epoch: 008/008 | Batch 124/266 | Loss: 0.1084\n",
      "Epoch: 008/008 | Batch 125/266 | Loss: 0.0556\n",
      "Epoch: 008/008 | Batch 126/266 | Loss: 0.0426\n",
      "Epoch: 008/008 | Batch 127/266 | Loss: 0.1053\n",
      "Epoch: 008/008 | Batch 128/266 | Loss: 0.0389\n",
      "Epoch: 008/008 | Batch 129/266 | Loss: 0.0575\n",
      "Epoch: 008/008 | Batch 130/266 | Loss: 0.0825\n",
      "Epoch: 008/008 | Batch 131/266 | Loss: 0.1698\n",
      "Epoch: 008/008 | Batch 132/266 | Loss: 0.0168\n",
      "Epoch: 008/008 | Batch 133/266 | Loss: 0.0198\n",
      "Epoch: 008/008 | Batch 134/266 | Loss: 0.0510\n",
      "Epoch: 008/008 | Batch 135/266 | Loss: 0.0429\n",
      "Epoch: 008/008 | Batch 136/266 | Loss: 0.1056\n",
      "Epoch: 008/008 | Batch 137/266 | Loss: 0.1141\n",
      "Epoch: 008/008 | Batch 138/266 | Loss: 0.1562\n",
      "Epoch: 008/008 | Batch 139/266 | Loss: 0.0832\n",
      "Epoch: 008/008 | Batch 140/266 | Loss: 0.0680\n",
      "Epoch: 008/008 | Batch 141/266 | Loss: 0.1413\n",
      "Epoch: 008/008 | Batch 142/266 | Loss: 0.0585\n",
      "Epoch: 008/008 | Batch 143/266 | Loss: 0.0344\n",
      "Epoch: 008/008 | Batch 144/266 | Loss: 0.0554\n",
      "Epoch: 008/008 | Batch 145/266 | Loss: 0.1031\n",
      "Epoch: 008/008 | Batch 146/266 | Loss: 0.1163\n",
      "Epoch: 008/008 | Batch 147/266 | Loss: 0.0537\n",
      "Epoch: 008/008 | Batch 148/266 | Loss: 0.0959\n",
      "Epoch: 008/008 | Batch 149/266 | Loss: 0.0759\n",
      "Epoch: 008/008 | Batch 150/266 | Loss: 0.0588\n",
      "Epoch: 008/008 | Batch 151/266 | Loss: 0.0484\n",
      "Epoch: 008/008 | Batch 152/266 | Loss: 0.0673\n",
      "Epoch: 008/008 | Batch 153/266 | Loss: 0.1160\n",
      "Epoch: 008/008 | Batch 154/266 | Loss: 0.0945\n",
      "Epoch: 008/008 | Batch 155/266 | Loss: 0.0376\n",
      "Epoch: 008/008 | Batch 156/266 | Loss: 0.0511\n",
      "Epoch: 008/008 | Batch 157/266 | Loss: 0.0860\n",
      "Epoch: 008/008 | Batch 158/266 | Loss: 0.0529\n",
      "Epoch: 008/008 | Batch 159/266 | Loss: 0.0769\n",
      "Epoch: 008/008 | Batch 160/266 | Loss: 0.1322\n",
      "Epoch: 008/008 | Batch 161/266 | Loss: 0.1301\n",
      "Epoch: 008/008 | Batch 162/266 | Loss: 0.0924\n",
      "Epoch: 008/008 | Batch 163/266 | Loss: 0.1685\n",
      "Epoch: 008/008 | Batch 164/266 | Loss: 0.0721\n",
      "Epoch: 008/008 | Batch 165/266 | Loss: 0.0746\n",
      "Epoch: 008/008 | Batch 166/266 | Loss: 0.0364\n",
      "Epoch: 008/008 | Batch 167/266 | Loss: 0.1288\n",
      "Epoch: 008/008 | Batch 168/266 | Loss: 0.0510\n",
      "Epoch: 008/008 | Batch 169/266 | Loss: 0.0624\n",
      "Epoch: 008/008 | Batch 170/266 | Loss: 0.0447\n",
      "Epoch: 008/008 | Batch 171/266 | Loss: 0.0250\n",
      "Epoch: 008/008 | Batch 172/266 | Loss: 0.0712\n",
      "Epoch: 008/008 | Batch 173/266 | Loss: 0.0704\n",
      "Epoch: 008/008 | Batch 174/266 | Loss: 0.1082\n",
      "Epoch: 008/008 | Batch 175/266 | Loss: 0.1543\n",
      "Epoch: 008/008 | Batch 176/266 | Loss: 0.0620\n",
      "Epoch: 008/008 | Batch 177/266 | Loss: 0.1024\n",
      "Epoch: 008/008 | Batch 178/266 | Loss: 0.0606\n",
      "Epoch: 008/008 | Batch 179/266 | Loss: 0.0761\n",
      "Epoch: 008/008 | Batch 180/266 | Loss: 0.0781\n",
      "Epoch: 008/008 | Batch 181/266 | Loss: 0.0670\n",
      "Epoch: 008/008 | Batch 182/266 | Loss: 0.0625\n",
      "Epoch: 008/008 | Batch 183/266 | Loss: 0.0496\n",
      "Epoch: 008/008 | Batch 184/266 | Loss: 0.0210\n",
      "Epoch: 008/008 | Batch 185/266 | Loss: 0.0900\n",
      "Epoch: 008/008 | Batch 186/266 | Loss: 0.0317\n",
      "Epoch: 008/008 | Batch 187/266 | Loss: 0.0810\n",
      "Epoch: 008/008 | Batch 188/266 | Loss: 0.1053\n",
      "Epoch: 008/008 | Batch 189/266 | Loss: 0.0638\n",
      "Epoch: 008/008 | Batch 190/266 | Loss: 0.0292\n",
      "Epoch: 008/008 | Batch 191/266 | Loss: 0.0764\n",
      "Epoch: 008/008 | Batch 192/266 | Loss: 0.0514\n",
      "Epoch: 008/008 | Batch 193/266 | Loss: 0.0252\n",
      "Epoch: 008/008 | Batch 194/266 | Loss: 0.1432\n",
      "Epoch: 008/008 | Batch 195/266 | Loss: 0.0386\n",
      "Epoch: 008/008 | Batch 196/266 | Loss: 0.0592\n",
      "Epoch: 008/008 | Batch 197/266 | Loss: 0.0584\n",
      "Epoch: 008/008 | Batch 198/266 | Loss: 0.1001\n",
      "Epoch: 008/008 | Batch 199/266 | Loss: 0.1687\n",
      "Epoch: 008/008 | Batch 200/266 | Loss: 0.0656\n",
      "Epoch: 008/008 | Batch 201/266 | Loss: 0.1991\n",
      "Epoch: 008/008 | Batch 202/266 | Loss: 0.1326\n",
      "Epoch: 008/008 | Batch 203/266 | Loss: 0.1062\n",
      "Epoch: 008/008 | Batch 204/266 | Loss: 0.0600\n",
      "Epoch: 008/008 | Batch 205/266 | Loss: 0.0814\n",
      "Epoch: 008/008 | Batch 206/266 | Loss: 0.0325\n",
      "Epoch: 008/008 | Batch 207/266 | Loss: 0.1510\n",
      "Epoch: 008/008 | Batch 208/266 | Loss: 0.0188\n",
      "Epoch: 008/008 | Batch 209/266 | Loss: 0.0331\n",
      "Epoch: 008/008 | Batch 210/266 | Loss: 0.0183\n",
      "Epoch: 008/008 | Batch 211/266 | Loss: 0.0568\n",
      "Epoch: 008/008 | Batch 212/266 | Loss: 0.0371\n",
      "Epoch: 008/008 | Batch 213/266 | Loss: 0.0428\n",
      "Epoch: 008/008 | Batch 214/266 | Loss: 0.0326\n",
      "Epoch: 008/008 | Batch 215/266 | Loss: 0.1213\n",
      "Epoch: 008/008 | Batch 216/266 | Loss: 0.0378\n",
      "Epoch: 008/008 | Batch 217/266 | Loss: 0.0572\n",
      "Epoch: 008/008 | Batch 218/266 | Loss: 0.0655\n",
      "Epoch: 008/008 | Batch 219/266 | Loss: 0.0754\n",
      "Epoch: 008/008 | Batch 220/266 | Loss: 0.0754\n",
      "Epoch: 008/008 | Batch 221/266 | Loss: 0.0747\n",
      "Epoch: 008/008 | Batch 222/266 | Loss: 0.0572\n",
      "Epoch: 008/008 | Batch 223/266 | Loss: 0.0167\n",
      "Epoch: 008/008 | Batch 224/266 | Loss: 0.0430\n",
      "Epoch: 008/008 | Batch 225/266 | Loss: 0.1906\n",
      "Epoch: 008/008 | Batch 226/266 | Loss: 0.0339\n",
      "Epoch: 008/008 | Batch 227/266 | Loss: 0.1429\n",
      "Epoch: 008/008 | Batch 228/266 | Loss: 0.0907\n",
      "Epoch: 008/008 | Batch 229/266 | Loss: 0.1236\n",
      "Epoch: 008/008 | Batch 230/266 | Loss: 0.0580\n",
      "Epoch: 008/008 | Batch 231/266 | Loss: 0.1363\n",
      "Epoch: 008/008 | Batch 232/266 | Loss: 0.1390\n",
      "Epoch: 008/008 | Batch 233/266 | Loss: 0.1112\n",
      "Epoch: 008/008 | Batch 234/266 | Loss: 0.0813\n",
      "Epoch: 008/008 | Batch 235/266 | Loss: 0.1605\n",
      "Epoch: 008/008 | Batch 236/266 | Loss: 0.0951\n",
      "Epoch: 008/008 | Batch 237/266 | Loss: 0.0373\n",
      "Epoch: 008/008 | Batch 238/266 | Loss: 0.0239\n",
      "Epoch: 008/008 | Batch 239/266 | Loss: 0.0795\n",
      "Epoch: 008/008 | Batch 240/266 | Loss: 0.0300\n",
      "Epoch: 008/008 | Batch 241/266 | Loss: 0.0809\n",
      "Epoch: 008/008 | Batch 242/266 | Loss: 0.0358\n",
      "Epoch: 008/008 | Batch 243/266 | Loss: 0.0637\n",
      "Epoch: 008/008 | Batch 244/266 | Loss: 0.0720\n",
      "Epoch: 008/008 | Batch 245/266 | Loss: 0.0381\n",
      "Epoch: 008/008 | Batch 246/266 | Loss: 0.0373\n",
      "Epoch: 008/008 | Batch 247/266 | Loss: 0.1135\n",
      "Epoch: 008/008 | Batch 248/266 | Loss: 0.1054\n",
      "Epoch: 008/008 | Batch 249/266 | Loss: 0.0740\n",
      "Epoch: 008/008 | Batch 250/266 | Loss: 0.0353\n",
      "Epoch: 008/008 | Batch 251/266 | Loss: 0.1545\n",
      "Epoch: 008/008 | Batch 252/266 | Loss: 0.1034\n",
      "Epoch: 008/008 | Batch 253/266 | Loss: 0.0452\n",
      "Epoch: 008/008 | Batch 254/266 | Loss: 0.0765\n",
      "Epoch: 008/008 | Batch 255/266 | Loss: 0.1994\n",
      "Epoch: 008/008 | Batch 256/266 | Loss: 0.0896\n",
      "Epoch: 008/008 | Batch 257/266 | Loss: 0.1065\n",
      "Epoch: 008/008 | Batch 258/266 | Loss: 0.0933\n",
      "Epoch: 008/008 | Batch 259/266 | Loss: 0.1391\n",
      "Epoch: 008/008 | Batch 260/266 | Loss: 0.0330\n",
      "Epoch: 008/008 | Batch 261/266 | Loss: 0.0398\n",
      "Epoch: 008/008 | Batch 262/266 | Loss: 0.0920\n",
      "Epoch: 008/008 | Batch 263/266 | Loss: 0.0812\n",
      "Epoch: 008/008 | Batch 264/266 | Loss: 0.0859\n",
      "Epoch: 008/008 | Batch 265/266 | Loss: 0.0351\n",
      "training accuracy: 98.94%\n",
      "valid accuracy: 87.52%\n",
      "Time elapsed: 25.11 min\n",
      "Total Training Time: 25.11 min\n",
      "Test accuracy: 86.94%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "NUM_EPOCHS = 8\n",
    "BATCH_SIZE = 24\n",
    "EMBEDDING_DIM = 16        #128\n",
    "HIDDEN_DIM =  24         #256\n",
    "LEARNING_RATE = 0.02\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text = batch_data.TEXT_COLUMN_NAME.to(DEVICE)\n",
    "        labels = batch_data.LABEL_COLUMN_NAME.to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "               f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "               f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4d1be",
   "metadata": {},
   "source": [
    "## Below I print out the final weights of the trained RNN Model before saving them to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1735e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embedding.weight', tensor([[-0.4851, -0.2355,  0.0630,  ...,  1.3268,  0.0636,  0.6657],\n",
      "        [-0.9825, -1.8656, -1.0225,  ...,  1.0825, -0.7484, -3.3752],\n",
      "        [-1.4598,  0.1361, -3.5899,  ...,  0.6832,  0.1474, -1.1503],\n",
      "        ...,\n",
      "        [ 0.1470,  0.8990, -0.5071,  ..., -0.6089, -0.7529,  0.5489],\n",
      "        [-0.7083, -0.8101,  0.9433,  ...,  1.0532, -0.9461, -0.5724],\n",
      "        [ 0.0263, -1.8620,  1.7259,  ...,  0.4474, -0.9539, -0.4021]])), ('rnn.weight_ih_l0', tensor([[-0.1460, -0.5099, -0.3953,  ...,  0.2937, -0.0484,  0.7155],\n",
      "        [ 0.7581,  0.3694, -0.4216,  ..., -0.5087,  0.0948,  1.0980],\n",
      "        [-0.8754, -0.6928,  0.4072,  ...,  0.4331,  0.5352, -0.1056],\n",
      "        ...,\n",
      "        [-0.2470,  0.9151,  0.5359,  ...,  0.0547,  0.9964,  0.4931],\n",
      "        [-1.0646,  0.4829,  0.1846,  ...,  0.5054, -0.1292,  0.6290],\n",
      "        [ 1.1518, -0.8127, -0.1810,  ...,  0.0412, -0.4375,  0.7093]])), ('rnn.weight_hh_l0', tensor([[-0.2916, -0.3677, -0.2623,  ...,  0.1071,  0.6704,  0.0505],\n",
      "        [-0.4826, -0.2316,  0.8813,  ..., -1.0930,  0.2059, -0.1094],\n",
      "        [-0.0530,  0.1164,  0.0445,  ..., -0.6086,  0.1073,  0.9725],\n",
      "        ...,\n",
      "        [ 0.7809, -0.8569,  0.0172,  ..., -0.1810, -0.5095,  0.4794],\n",
      "        [ 0.6087, -0.5869, -0.0956,  ...,  0.7197,  0.4778,  0.0714],\n",
      "        [ 0.0538, -0.1297, -0.2206,  ..., -0.2009, -0.4386,  0.0169]])), ('rnn.bias_ih_l0', tensor([-0.0365,  0.2111,  0.3669, -0.3320, -0.1557, -0.6568, -0.8361, -0.4487,\n",
      "        -0.4208,  0.0930, -0.4736, -0.8252, -0.6165, -0.6456, -1.0112, -0.2836,\n",
      "        -0.4310,  0.0982, -0.3574, -0.2333, -0.1272, -0.1764, -0.6185, -0.5905,\n",
      "         0.5678, -0.6371,  0.2786, -0.3068,  0.4855,  0.0638,  0.9609, -0.3817,\n",
      "        -0.5857,  0.9318,  0.2568, -0.2018, -0.3864, -0.6427,  1.9108,  0.4736,\n",
      "         0.1337,  0.2295,  0.2765,  1.0534, -0.6175, -0.0739,  0.6730, -0.0838,\n",
      "         0.4625,  0.2489, -0.2536, -0.0083,  0.1282, -0.3788, -0.1696, -0.2815,\n",
      "        -0.0828, -0.2475,  0.1431,  0.1457, -0.0382, -0.1499, -0.1272,  0.2893,\n",
      "         0.0382, -0.1857, -0.2965,  0.1372, -0.2365,  0.2727,  0.2633,  0.3316,\n",
      "        -0.1939, -0.2257,  0.2200, -0.2088, -0.2288, -0.7962,  0.3514,  0.1181,\n",
      "        -0.4104, -0.4025, -0.2512, -0.9678, -0.5518, -0.8797,  0.5082,  0.3772,\n",
      "        -0.4338, -0.0596,  0.0228,  0.1263,  0.1466, -0.7262,  0.2300, -0.3648])), ('rnn.bias_hh_l0', tensor([ 0.0945,  0.0284,  0.3206, -0.3257, -0.2757, -0.6363, -0.9240, -0.2021,\n",
      "        -0.7206, -0.1061, -0.3604, -1.1949, -0.5707, -0.8669, -1.1716, -0.4885,\n",
      "        -0.2627,  0.3268, -0.0716, -0.2504, -0.1150, -0.1755, -0.5214, -0.6590,\n",
      "         0.4991, -0.5936,  0.1261, -0.0383,  0.4786,  0.0649,  0.9123, -0.5193,\n",
      "        -0.5600,  0.7122,  0.1615, -0.4028, -0.3784, -0.3570,  2.0371,  0.5305,\n",
      "        -0.1343,  0.5349,  0.3079,  0.7024, -0.6047,  0.1838,  0.7938, -0.2332,\n",
      "         0.2545,  0.3253, -0.2165, -0.0515, -0.1058, -0.6261,  0.0024,  0.0495,\n",
      "        -0.1926, -0.4006,  0.1640,  0.1884,  0.0725, -0.3412, -0.1345,  0.4345,\n",
      "        -0.0059, -0.0032, -0.1332,  0.0908,  0.0656,  0.2651,  0.2985,  0.3696,\n",
      "        -0.4595, -0.2043,  0.3818, -0.3836, -0.0661, -0.6455,  0.3525, -0.0036,\n",
      "        -0.5206, -0.2098, -0.1731, -0.7922, -0.4905, -0.7958,  0.3559,  0.4140,\n",
      "        -0.2365, -0.0332,  0.1643, -0.0044,  0.2324, -0.7682,  0.1658, -0.5941])), ('fc.weight', tensor([[-0.1696,  0.4487,  0.7187,  0.0223,  0.1178,  0.1792, -0.5687,  0.4116,\n",
      "         -0.0849, -0.2087, -0.3168, -0.2224,  0.0327, -0.0123, -2.3642,  1.0498,\n",
      "         -0.1115,  0.9084, -0.0125,  0.3004, -0.5976, -0.1928, -0.2089, -0.1956],\n",
      "        [ 0.0760, -0.4280, -0.7845,  0.0814, -0.0940, -0.2003,  0.2673, -0.2561,\n",
      "          0.0940,  0.5401,  0.2063,  0.0179,  0.0142, -0.3095,  2.0092, -0.9374,\n",
      "         -0.0738, -1.0330, -0.1230, -0.4555,  0.5809, -0.0963,  0.1985,  0.0329]])), ('fc.bias', tensor([0.0070, 0.1142]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d05e2a",
   "metadata": {},
   "source": [
    "## I learned that below is the way to save a model (I am just saving it to my current directory under the name bestRNNweights).  I can then load these weights into a model with the same dimensions in my other jupyter notebook and I won't have to retrain the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14497e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./bestRNNweights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecd81a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(20002, 16)\n",
      "  (rnn): LSTM(16, 24)\n",
      "  (fc): Linear(in_features=24, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023639a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
