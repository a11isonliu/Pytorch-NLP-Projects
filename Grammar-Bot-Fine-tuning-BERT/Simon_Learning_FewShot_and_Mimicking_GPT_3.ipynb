{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH-k7-VcCgaF"
   },
   "source": [
    "# Investigating Few Shot Learning and mimicking GPT-3\n",
    "In this notebook, we began by doing an overarching investigation into zero-shot, one-shot, and few-shot learning. This includes a conceptual explaination of the material after watching and following a few videos and tutorial provided by OpenAI and other large ML organizations. As you may expect, we quickly stumbled accross GPT-3 (also mentioned as a cool product in class) and we were of course fascinated by its capabilities. Unfortunately, we couldn't find any online interface to try out the OpenAI product, so we began the actual coding in this notebook mimicking some of GPT-3's capabilities with other, similar yet smaller models that will run on our computers. This included using GPTneo and flair TARSC. As far as code, this notebook isn't extensively long, but it contains material and relevant worldly products that have to do with the topics we learned in this biweekly interval in class, and contains our origional markdown explainations and commenting. \n",
    "\n",
    "## Few Shot Learning Explainations\n",
    "Below is a conglomeration of our knowledge that we have aquired about few-shot learning after watching videos and tutorials. Training what you read below as the \"prior worldly knowledge\" takes a lot of computational time and data, and mostly is not available in tutorials. Instead there are pleanty of model packages people typically just import for this, so we felt the need to have some thurough explainations before showing code that relies heavily on just importing someone else's pretrained knowledge data. \n",
    "### Intro\n",
    "\"Few-shot learning\" is an umbrella term for zero-shot, one-shot, or N-shot learning models. The overall function of these models is to train a dataset even when there is limited available datapoints to train on. After watching some of the tutorial videos online, a good analogy we thought of is few-shot learning is extremely close to how the average human being's brain can operate. We can see only one picture of a person and then recognize if a \"testing\" picture is the same person or not. This is because we have seen so many other people in our lifetime, that we are able to use our prior knowledge from the world to improve our learning abilities. With previous methods we have worked on in this course, we would need to train our model on many pictures of the same person for it to start successfully recognizing their face. \n",
    "\n",
    "For the application of natural language processing, the same concepts follow. A typicall RNN or other models require extensive training datapoints in order to interpret words and language context of a sequence. With few-shot learning, we can simply import \"prior worldly knowledge\" to our system, and allow it to figure out what is going on in our \"test\" data dequence without much training data. N-shot, one-shot, and zero-shot represent the exact amount of datapoints we are allowing our few-shot learning model to have before using testing data. In application few-shot learning, models can be trained in rare, under-mearsured topics. It also reduces the amount of data processing required to build a model\n",
    "\n",
    "### Approaches\n",
    "The three main approaches are similarity, learning, and data.\n",
    "#### Similarity\n",
    "Few-shot learning models that are based on similarity learn pattterns in their \"prior worldly knowledge\" that can then be used to separate categories of data even if the test data is completely unseen before. To learn these patterns, the model must be able to compare between multiple classification networks in order to have the distinguishing be high level at \"similar\" or \"not similar\" instead of specific like --for example-- \"dog or cat\". For a very simplified nlp example, by comparing a network that distinguishes a sequence between \"loving\" and \"not-loving\" and between \"funny\" and \"not funny\", it will be able to use what it has learned about similarities between sequences to then distinguish between \"positive\" or \"negative\" review comments after only a few training datapoints on a positive or negative review (we just made this example up). In another notebook we have turned in this week, we perform a siamese network implementation which is training this \"prior worldy knowledge\" on two networks at once.\n",
    "\n",
    "#### Learning\n",
    "In performing few shot learning based on learning, the \"prior worldly knowledge\" consists of information of constraints such as hyperparameters and rules. If the dataset has trained already to hyperparameters of previous similar models, it doesn't take many training data points for it to identify how these hyperparameters apply to the input sequence and follow the necessary operations. LSTMs and MAML are both examles of using this type of approach. \n",
    "\n",
    "#### Data\n",
    "Models based on prior knowledge of the dataset at hand can also be categorized as few show learning as well. Often times, this includes pretrained knowledge that are organized in \"families\" of data classes on the internet. Then importing the knowledge from families of datasets that are the same or extremely similar to the ones you are training on, will reduce the necessary training data for you to have available. Common methods of model based few-shot learning include pen-strike and analogies by Facebook AI. \n",
    "\n",
    "Some key sources to learn all of this included: \n",
    "- https://analyticsindiamag.com/an-introductory-guide-to-few-shot-learning-for-beginners/\n",
    "- https://www.youtube.com/watch?v=VqPmrYFvKf8\n",
    "- https://www.analyticsvidhya.com/blog/2021/05/an-introduction-to-few-shot-learning/\n",
    "- https://jaketae.github.io/study/gpt/\n",
    "\n",
    "## Trying out GPTneo \n",
    "Out of curiousity of GPT-3, below we have implemented GPTneo, which is a smaller version of GPT-3 available for download from PyTorch. It is mostly fun to mess around with but also inspires some cool aspirations and project extensions as it truely does demonstrate the power of few-shot learning in language models. This also goes along with what we previously mentioned that implementing these few shot learning applications can require very few lines of code because it is designed to rely on someone elses priorly trained work that is available on the internet. After this we make an implementation that is a little bit more involved.\n",
    "\n",
    "A good source for this implentation was this youtube video:\n",
    "- https://www.youtube.com/watch?v=6MI0f6YjJIk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucD5YHDxCgaN",
    "outputId": "37c18860-8ff8-4f5a-c95d-64a2dde9585f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "\u001b[1;33mwarning\u001b[0m: \u001b[1mmissing-index-doctype\u001b[0m\n",
      "\n",
      "\u001b[33m×\u001b[0m The package index page being used does not have a proper HTML doctype declaration.\n",
      "\u001b[33m╰─>\u001b[0m Problematic URL: \u001b[4;94mhttps://download.pytorch.org/whl/torch_stable.html\u001b[0m\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the page at the URL mentioned above.\n",
      "\u001b[1;36mhint\u001b[0m: You might need to reach out to the owner of that package index, to get this fixed. See \u001b[4;94mhttps://github.com/pypa/pip/issues/10825\u001b[0m for context.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.1+cu111 (from versions: 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.1+cu111\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (4.16.2)\n",
      "Requirement already satisfied: requests in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.11.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: sacremoses in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: filelock in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: joblib in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: click in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Following tutorial on nessesary tools to implement GPTneo here\n",
    "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "8ba715d4d9ad42d78982622dd3a032ab",
      "7497949cfaad48b4a66e52636f2d3ddd",
      "40221087fd4741f5ace7631ce4725d8b",
      "ca463577f13d4e259a0e4b8ffb5f3327",
      "cf5197d3b7154143af5fd174e0814348",
      "65d621cbec86476babee97e4f4bbecb3",
      "e09eaca374b64cd6b7018e2fe8fb6941",
      "b7a11d7d74d842c89047520d302c25a6",
      "74e24eafd18a4fa7921ccb9d2b48353a",
      "bfddba8647b8491482c7f351d94e2d1f",
      "be7cdf5aeb394ff1839890ce9b35ac1f",
      "d416b6228796488b9c5784a64401829b",
      "aff0cab087b9425380d8955d7a9eb285",
      "4c2e025514aa4b4eba9c66eb239593c4",
      "55707e91344e4785b1bcee504ba42ec6",
      "d8e88ee874c040d08bc9acf6d9953508",
      "e5f099bcec174aceadaa4d2e46de30b0",
      "66a005ecd7a54bca840be28905dd6c04",
      "617ba8ad75bd4b93839dc966bad77888",
      "4a7391dd6fdc441c91377b4b60d83db1",
      "a6c89f41451f406d9e37d46cb5889f6a",
      "dde6d3accd304312ad41190e33373ee0"
     ]
    },
    "id": "IPZU0d02CgaP",
    "outputId": "02de9ad4-fe2b-4882-c2aa-8aeca3a3a27f"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B') #import \"prior knowledge\" from GPTneo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vkxZc74vCgaP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 12th president of the US was born in Hawaii, but moved to Washington DC, where his parents ran a restaurant. However, Trump has always had an extremely high opinion of himself.\n",
      "\n",
      "The real estate mogul has been married since 1981 and\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The 12th president of the US was\" # Prompt a random phrase that we thought of\n",
    "res = generator(prompt, max_length=50, do_sample=True, temperature=0.9) # generates additional text to our prompt up to 50 characters\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjVykcZkCgaQ"
   },
   "source": [
    "We tried many fun prompts in this model. If you enter \"import pandas as pd\", the model outputs python code that could follow. We also put in a problem for our numerics homework and the model outputted latex code of mathematics that was relevant and more or less followed logically. The GPT-3 has plenty of more knowledge that it has trained on than GPTneo, so the results would be much better if we had the resources to use it.\n",
    "\n",
    "## Performing zero-shot learning using \"flair\" models for sequence classification \n",
    "Since we were very interested, but weren't extremely satisfied with just using the GPTneo implementation above, we found \"flair\" which is a pytorch based package that performs zero-shot learning in a slightly more involved way. It has families of pretrained \"TARS\" models for different languages (below we use English so we can enjoy our results). After loading in the \"previous worldy knowledge\" we define classifications to fit a prompted sequence into. Then when comparing the prompted sequence to our TARS worldly knowledge, we can classify the sequence. All without expending our own resources to train a model with large amounts of data!\n",
    "\n",
    "A good tutorial we found on this is here:\n",
    "- https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_10_TRAINING_ZERO_SHOT_MODEL.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O-keaT4aCgaQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: flair in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (0.10)\n",
      "Requirement already satisfied: wikipedia-api in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (0.5.4)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (1.2.13)\n",
      "Requirement already satisfied: ftfy in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (6.1.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (0.1.95)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (0.23.1)\n",
      "Requirement already satisfied: conllu>=4.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (4.4.1)\n",
      "Requirement already satisfied: lxml in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (4.5.2)\n",
      "Requirement already satisfied: gdown==3.12.2 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (3.12.2)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (4.16.2)\n",
      "Requirement already satisfied: more-itertools~=8.8.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (8.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied: mpld3==0.3 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (4.6.5)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (4.1.2)\n",
      "Requirement already satisfied: janome in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (0.4.1)\n",
      "Requirement already satisfied: tabulate in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (0.8.9)\n",
      "Requirement already satisfied: langdetect in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (3.3.1)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (4.47.0)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (1.10.1)\n",
      "Requirement already satisfied: regex in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (2020.6.8)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (1.7.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from flair) (0.4.0)\n",
      "Requirement already satisfied: six in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from gdown==3.12.2->flair) (1.15.0)\n",
      "Requirement already satisfied: filelock in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from gdown==3.12.2->flair) (3.0.12)\n",
      "Requirement already satisfied: requests[socks] in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from gdown==3.12.2->flair) (2.27.1)\n",
      "Requirement already satisfied: numpy in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from bpemb>=0.3.2->flair) (1.19.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from gensim>=3.4.0->flair) (1.5.0)\n",
      "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
      "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (7.2.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (2021.5.30)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from torch!=1.8,>=1.5.0->flair) (4.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers>=4.0.0->flair) (5.3.1)\n",
      "Requirement already satisfied: sacremoses in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers>=4.0.0->flair) (0.0.47)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers>=4.0.0->flair) (0.11.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from transformers>=4.0.0->flair) (21.3)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->gdown==3.12.2->flair) (1.25.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->gdown==3.12.2->flair) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->gdown==3.12.2->flair) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\n",
      "Requirement already satisfied: click in /Users/simon/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/simon/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k6p4gaEoCgaR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-23 03:32:17,420 loading file /Users/simon/.flair/models/tars-base-v8.pt\n",
      "Sentence: \"I am so glad you liked it !\"   [− Tokens: 8  − Sentence-Labels: {'happy-sad': [happy (0.8667)]}]\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TARSClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "tars = TARSClassifier.load('tars-base') #Loading prior worldy knowledge\n",
    "\n",
    "classes = [\"happy\", \"sad\"] #Define classifications\n",
    "\n",
    "sentence = Sentence(\"I am so glad you liked it!\") #input sentence sequence\n",
    "\n",
    "tars.predict_zero_shot(sentence, classes) #run zsl on the TARS model for classification\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt3Ejw8ACgaS"
   },
   "source": [
    "Next, instead of loading the \"base\" family from TARS for English, we uploaded the Named Entity Recognition (ner) family. This prior knowledge allows our model to identify named identities in our inputted sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_30_zCT1CgaS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-23 03:32:23,343 loading file /Users/simon/.flair/models/tars-ner.pt\n",
      "The Humboldt <B-University> University <I-University> of <I-University> Berlin <E-University> is situated near the Spree <S-River> in Berlin <S-City> , Germany <S-Country>\n",
      "Bayern <B-Soccer Team> Munich <E-Soccer Team> played against Real <B-Soccer Team> Madrid <E-Soccer Team>\n",
      "I flew with an Airbus <B-Vehicle> A380 <E-Vehicle> to Peru <S-City> to pick up my Porsche <B-Vehicle> Cayenne <E-Vehicle>\n",
      "Game <B-TV Show> of <I-TV Show> Thrones <E-TV Show> is my favorite series\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TARSTagger\n",
    "from flair.data import Sentence\n",
    "\n",
    "tars = TARSTagger.load('tars-ner') #Loading prior worldy knowledge\n",
    "\n",
    "labels = [\"Soccer Team\", \"University\", \"Vehicle\", \"River\", \"City\", \"Country\", \"Person\", \"Movie\", \"TV Show\"] #Define classifications\n",
    "\n",
    "sentences = [ \n",
    "    Sentence(\"The Humboldt University of Berlin is situated near the Spree in Berlin, Germany\"),\n",
    "    Sentence(\"Bayern Munich played against Real Madrid\"),\n",
    "    Sentence(\"I flew with an Airbus A380 to Peru to pick up my Porsche Cayenne\"),\n",
    "    Sentence(\"Game of Thrones is my favorite series\"),\n",
    "] #input sentence sequences\n",
    "\n",
    "tars.add_and_switch_to_new_task('task 1', labels, label_type='ner') #run zsl on the TARS model for classification\n",
    "\n",
    "for sentence in sentences:\n",
    "    tars.predict(sentence)\n",
    "    print(sentence.to_tagged_string(\"ner\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLsxW99oCgaT"
   },
   "source": [
    "As we can see this is working very well! This reminds us of when you \"have heard of a sports team\" but know nothing about it and don't remember where you heard it from in the first place. When someone says \"Real Madrid\" in the context of the sentence, you somehow know --from zero other context except for prior worldly knowledge-- that they are talking about soccer. \n",
    "\n",
    "In this next snippets we get a little bit more involved with implementing a more difficult classification that will require training from our model additionally. Of course, we still will only need to train with a few data points! In this case, we will take in the \"prior worldly knowledge\" from TARS, while also defining an unrelated dictionary of emotions from corpus for classification. Thus, our model has a very good idea of how English and natural language works, but needs to be very briefly trained on what \"emotions\" are before it can perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjguMJuNCgaT",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-23 03:32:47,783 loading file resources/taggers/trec/best-model.pt\n",
      "2022-02-23 03:32:53,158 Reading data from /Users/simon/.flair/datasets/go_emotions\n",
      "2022-02-23 03:32:53,159 Train: /Users/simon/.flair/datasets/go_emotions/train.txt\n",
      "2022-02-23 03:32:53,160 Dev: /Users/simon/.flair/datasets/go_emotions/dev.txt\n",
      "2022-02-23 03:32:53,160 Test: /Users/simon/.flair/datasets/go_emotions/test.txt\n",
      "2022-02-23 03:32:54,084 Initialized corpus /Users/simon/.flair/datasets/go_emotions (label type name is 'emotion')\n",
      "2022-02-23 03:32:54,085 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43410/43410 [00:12<00:00, 3470.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-23 03:33:40,240 Corpus contains the labels: emotion (#43410)\n",
      "2022-02-23 03:33:40,241 Created (for label 'emotion') Dictionary with 29 tags: <unk>, NEUTRAL, ANGER, FEAR, ANNOYANCE, SURPRISE, GRATITUDE, DESIRE, OPTIMISM, ADMIRATION, CONFUSION, AMUSEMENT, APPROVAL, CARING, EMBARRASSMENT, REALIZATION, DISAPPOINTMENT, GRIEF, SADNESS, CURIOSITY, JOY, LOVE, EXCITEMENT, DISAPPROVAL, REMORSE, DISGUST, RELIEF, PRIDE, NERVOUSNESS\n",
      "2022-02-23 03:33:40,241 Task `GO_EMOTIONS` already exists in TARS model. Switching to it.\n",
      "2022-02-23 03:33:40,244 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-23 03:33:40,246 Model: \"TARSClassifier(\n",
      "  (tars_model): TextClassifier(\n",
      "    (loss_function): CrossEntropyLoss()\n",
      "    (document_embeddings): TransformerDocumentEmbeddings(\n",
      "      (model): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-23 03:33:40,247 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-23 03:33:40,247 Corpus: \"Corpus: 43410 train + 5426 dev + 5427 test sentences\"\n",
      "2022-02-23 03:33:40,248 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-23 03:33:40,249 Parameters:\n",
      "2022-02-23 03:33:40,249  - learning_rate: \"0.02\"\n",
      "2022-02-23 03:33:40,250  - mini_batch_size: \"16\"\n",
      "2022-02-23 03:33:40,250  - patience: \"3\"\n",
      "2022-02-23 03:33:40,251  - anneal_factor: \"0.5\"\n",
      "2022-02-23 03:33:40,251  - max_epochs: \"10\"\n",
      "2022-02-23 03:33:40,251  - shuffle: \"True\"\n",
      "2022-02-23 03:33:40,252  - train_with_dev: \"False\"\n",
      "2022-02-23 03:33:40,252  - batch_growth_annealing: \"False\"\n",
      "2022-02-23 03:33:40,253 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-23 03:33:40,254 Model training base path: \"resources/taggers/go_emotions\"\n",
      "2022-02-23 03:33:40,254 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-23 03:33:40,255 Device: cpu\n",
      "2022-02-23 03:33:40,255 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-23 03:33:40,256 Embeddings storage mode: cpu\n",
      "2022-02-23 03:33:40,260 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-23 04:23:32,851 epoch 1 - iter 271/2714 - loss 0.01426208 - samples/sec: 1.46 - lr: 0.020000\n",
      "2022-02-23 05:13:47,877 epoch 1 - iter 542/2714 - loss 0.01390668 - samples/sec: 1.44 - lr: 0.020000\n",
      "2022-02-23 06:04:33,292 epoch 1 - iter 813/2714 - loss 0.01415939 - samples/sec: 1.42 - lr: 0.020000\n",
      "2022-02-23 06:57:58,626 epoch 1 - iter 1084/2714 - loss 0.01404646 - samples/sec: 1.35 - lr: 0.020000\n",
      "2022-02-23 07:51:22,612 epoch 1 - iter 1355/2714 - loss 0.01407711 - samples/sec: 1.35 - lr: 0.020000\n",
      "2022-02-23 08:44:43,219 epoch 1 - iter 1626/2714 - loss 0.01407313 - samples/sec: 1.35 - lr: 0.020000\n",
      "2022-02-23 09:39:03,155 epoch 1 - iter 1897/2714 - loss 0.01411025 - samples/sec: 1.33 - lr: 0.020000\n",
      "2022-02-23 10:32:33,857 epoch 1 - iter 2168/2714 - loss 0.01406061 - samples/sec: 1.35 - lr: 0.020000\n",
      "2022-02-23 11:24:14,160 epoch 1 - iter 2439/2714 - loss 0.01400876 - samples/sec: 1.40 - lr: 0.020000\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import GO_EMOTIONS\n",
    "from flair.models import TARSClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "tars = TARSClassifier.load('resources/taggers/trec/best-model.pt') #Loading prior worldy knowledge\n",
    "\n",
    "# 2. load a new flair corpus e.g., GO_EMOTIONS\n",
    "new_corpus = GO_EMOTIONS()\n",
    "label_type = \"emotion\"\n",
    "label_dict = new_corpus.make_label_dictionary(label_type=label_type)\n",
    "\n",
    "tars.add_and_switch_to_new_task(\"GO_EMOTIONS\",\n",
    "                                label_dictionary=label_dict,\n",
    "                                label_type=label_type) #compare to TARS \"worldly knowledge\" model prior to training to \"emotions\"\n",
    "\n",
    "trainer = ModelTrainer(tars, new_corpus) #train\n",
    "trainer.train(base_path='resources/taggers/go_emotions', # path to store the model artifacts\n",
    "              learning_rate=0.02, \n",
    "              mini_batch_size=16,\n",
    "              mini_batch_chunk_size=4, \n",
    "              max_epochs=10, \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g6gByfaECgaU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing tasks are: {'AGNews', 'DBPedia', 'IMDB', 'SST', 'TREC_6', 'NEWS_CATEGORY', 'Amazon', 'Yelp', 'GO_EMOTIONS'}\n",
      "Sentence: \"I absolutely love this !\"   [− Tokens: 5  − Sentence-Labels: {\"label\": [LOVE (0.9708)]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tars = TARSClassifier.load('tars-base') #Loading prior worldy knowledge\n",
    "\n",
    "existing_tasks = tars.list_existing_tasks()\n",
    "print(f\"Existing tasks are: {existing_tasks}\") #Check out what datasets it was trained on\n",
    "\n",
    "\n",
    "tars.switch_to_task(\"GO_EMOTIONS\") #Apply our emotions training\n",
    "\n",
    "sentence = Sentence(\"I absolutely love this!\")\n",
    "tars.predict(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reran this notebook (illadvised), so the training above didn't finish, but luckily we are still able to see the output from the previous run above. It looks extremely accurate! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Learning_FewShot_and_Mimicking_GPT-3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "40221087fd4741f5ace7631ce4725d8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e09eaca374b64cd6b7018e2fe8fb6941",
      "placeholder": "​",
      "style": "IPY_MODEL_65d621cbec86476babee97e4f4bbecb3",
      "value": "Downloading: 100%"
     }
    },
    "4a7391dd6fdc441c91377b4b60d83db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c2e025514aa4b4eba9c66eb239593c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66a005ecd7a54bca840be28905dd6c04",
      "placeholder": "​",
      "style": "IPY_MODEL_e5f099bcec174aceadaa4d2e46de30b0",
      "value": "Downloading: 100%"
     }
    },
    "55707e91344e4785b1bcee504ba42ec6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a7391dd6fdc441c91377b4b60d83db1",
      "max": 10672498931,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_617ba8ad75bd4b93839dc966bad77888",
      "value": 10672498931
     }
    },
    "617ba8ad75bd4b93839dc966bad77888": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65d621cbec86476babee97e4f4bbecb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66a005ecd7a54bca840be28905dd6c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7497949cfaad48b4a66e52636f2d3ddd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74e24eafd18a4fa7921ccb9d2b48353a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ba715d4d9ad42d78982622dd3a032ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40221087fd4741f5ace7631ce4725d8b",
       "IPY_MODEL_ca463577f13d4e259a0e4b8ffb5f3327",
       "IPY_MODEL_cf5197d3b7154143af5fd174e0814348"
      ],
      "layout": "IPY_MODEL_7497949cfaad48b4a66e52636f2d3ddd"
     }
    },
    "a6c89f41451f406d9e37d46cb5889f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aff0cab087b9425380d8955d7a9eb285": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7a11d7d74d842c89047520d302c25a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "be7cdf5aeb394ff1839890ce9b35ac1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfddba8647b8491482c7f351d94e2d1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca463577f13d4e259a0e4b8ffb5f3327": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74e24eafd18a4fa7921ccb9d2b48353a",
      "max": 1455,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b7a11d7d74d842c89047520d302c25a6",
      "value": 1455
     }
    },
    "cf5197d3b7154143af5fd174e0814348": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be7cdf5aeb394ff1839890ce9b35ac1f",
      "placeholder": "​",
      "style": "IPY_MODEL_bfddba8647b8491482c7f351d94e2d1f",
      "value": " 1.42k/1.42k [00:00&lt;00:00, 37.2kB/s]"
     }
    },
    "d416b6228796488b9c5784a64401829b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c2e025514aa4b4eba9c66eb239593c4",
       "IPY_MODEL_55707e91344e4785b1bcee504ba42ec6",
       "IPY_MODEL_d8e88ee874c040d08bc9acf6d9953508"
      ],
      "layout": "IPY_MODEL_aff0cab087b9425380d8955d7a9eb285"
     }
    },
    "d8e88ee874c040d08bc9acf6d9953508": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dde6d3accd304312ad41190e33373ee0",
      "placeholder": "​",
      "style": "IPY_MODEL_a6c89f41451f406d9e37d46cb5889f6a",
      "value": " 9.94G/9.94G [06:13&lt;00:00, 36.0MB/s]"
     }
    },
    "dde6d3accd304312ad41190e33373ee0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e09eaca374b64cd6b7018e2fe8fb6941": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5f099bcec174aceadaa4d2e46de30b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
